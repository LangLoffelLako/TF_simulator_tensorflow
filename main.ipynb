{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import functools\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility and Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=log.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "log_enabled = True\n",
    "execute_helper = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dec(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            if log_enabled:\n",
    "                start_time = time()\n",
    "                log.info('{} started'.format(func.__name__))\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "        finally:\n",
    "            if log_enabled:\n",
    "                duration = time() - start_time\n",
    "                log.info('{} finished'.format(func.__name__))\n",
    "    return wrapper\n",
    "\n",
    "def run_helper(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if execute_helper:\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            return\n",
    "    return wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and model elements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Load the tokenizer from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'story_corpus_tokenizer'\n",
    "\n",
    "tokenizer = tf.saved_model.load(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Dataset\n",
    "Load the txt dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'datasets\\\\corpus.txt'\n",
    "\n",
    "@log_dec\n",
    "def load_dataset(dataset_text_file):\n",
    "    return tf.data.TextLineDataset(filenames=dataset_text_file)\n",
    "\n",
    "dataset = load_dataset(dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the length of the different data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for example in dataset.batch(1024):\n",
    "    tokens = tokenizer.tokenize(example)\n",
    "    lengths.append(tokens.row_lengths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lengths = np.concatenate(lengths)\n",
    "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
    "plt.ylim(plt.ylim())\n",
    "max_length = max(all_lengths)\n",
    "med_length = np.mean(all_lengths)\n",
    "plt.plot([max_length, max_length], plt.ylim())\n",
    "plt.title(f'Maximum tokens per example: {max_length}');\n",
    "plt.plot([med_length, med_length], plt.ylim())\n",
    "plt.title(f'Average tokens per example: {med_length}');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data batching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a stub for now, we first have to define the components in order to get more information how a batch has to look in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 128\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def prepare_batch(data_entry):\n",
    "    tokens = tokenizer.tokenize(data_entry)\n",
    "    trim_to_max = tokens[:, :MAX_TOKENS]\n",
    "    trim_to_max_and_one = tokens[:, :(MAX_TOKENS+1)]\n",
    "\n",
    "    encoder_input = trim_to_max.to_tensor()\n",
    "\n",
    "    decoder_input = trim_to_max_and_one[:, :-1].to_tensor()\n",
    "    decoder_output = trim_to_max_and_one[:, 1:].to_tensor()\n",
    "\n",
    "    return (encoder_input, decoder_input), decoder_output\n",
    "\n",
    "def make_batches(dataset):\n",
    "    return dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).map(prepare_batch, tf.data.AUTOTUNE).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_dataset = make_batches(dataset)\n",
    "for (enc_in, dec_in), dec_out in batched_dataset:\n",
    "    break\n",
    "\n",
    "print(dec_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Container classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 encoder: tf.keras.layers.Layer, \n",
    "                 decoder: tf.keras.layers.Layer, \n",
    "                 enc_embed: tf.keras.layers.Layer, \n",
    "                 dec_embed: tf.keras.layers.Layer, \n",
    "                 generator: tf.keras.layers.Layer,\n",
    "                 pad_mask=None,\n",
    "                 subseq_mask=None, \n",
    "                 trainable=True, \n",
    "                 name=None, \n",
    "                 dtype=None, \n",
    "                 dynamic=False, \n",
    "                 **kwargs):\n",
    "        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n",
    "\n",
    "        # modules\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.enc_embed = enc_embed\n",
    "        self.dec_embed = dec_embed\n",
    "        self.generator = generator\n",
    "\n",
    "        # masking\n",
    "        self.pad_mask = pad_mask\n",
    "        self.subseq_mask = subseq_mask\n",
    "\n",
    "    def encode(self, inputs, pad_mask=None):\n",
    "        if pad_mask is None:\n",
    "            pad_mask = self.pad_mask \n",
    "\n",
    "        return self.encoder(\n",
    "            self.enc_embed(inputs), \n",
    "            pad_mask)\n",
    "    \n",
    "    def decode(self, inputs, enc_input, pad_mask=None, subseq_mask=None):\n",
    "        if pad_mask is None:\n",
    "            pad_mask = self.pad_mask \n",
    "        if subseq_mask is None:\n",
    "            subseq_mask = self.subseq_mask \n",
    "\n",
    "        return self.decoder(\n",
    "            self.dec_embed(inputs), \n",
    "            inputs, \n",
    "            pad_mask, \n",
    "            subseq_mask)\n",
    "\n",
    "    def call(self, enc_input, dec_input, pad_mask=None, subseq_mask=None, *args, **kwargs):\n",
    "        if pad_mask is None:\n",
    "            pad_mask = self.pad_mask \n",
    "        if subseq_mask is None:\n",
    "            subseq_mask = self.subseq_mask \n",
    "\n",
    "        return self.decode(dec_input, \n",
    "                           self.encode(enc_input, pad_mask), \n",
    "                           pad_mask, \n",
    "                           subseq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]   # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)               # (1, depth)\n",
    "    angle_rads  = positions * angle_rates           # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1\n",
    "        )\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding explanation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show how positional encoding looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "\n",
    "# Check the shape.\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n",
    "p = pos_encoding[1000]\n",
    "dots = tf.einsum('pd,d -> p', pos_encoding, p)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(dots)\n",
    "plt.ylim([0,1])\n",
    "plt.plot([950, 950, float('nan'), 1050, 1050],\n",
    "         [0,1,float('nan'),0,1], color='k', label='Zoom')\n",
    "plt.legend()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(dots)\n",
    "plt.xlim([950, 1050])\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positional_encoding\n",
    "        x *=tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional embedding example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example how a vector is encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = PositionalEmbedding(vocab_size=tokenizer.get_vocab_size(), d_model=512)\n",
    "dec_in_embed = embed(dec_in)\n",
    "print(dec_in_embed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \n",
    "    d_k = query.size(-1)\n",
    "    scores = tf.matmul(query, key.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, trainable=True, name=None, dtype=None, dynamic=False, **kwargs):\n",
    "        super().__init__(trainable, name, dtype, dynamic, **kwargs)\n",
    "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_simu_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
