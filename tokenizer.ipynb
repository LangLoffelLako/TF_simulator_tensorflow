{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import functools\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=log.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "log_enabled = True\n",
    "execute_helper = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dec(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            if log_enabled:\n",
    "                start_time = time()\n",
    "                log.info('{} started'.format(func.__name__))\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "        finally:\n",
    "            if log_enabled:\n",
    "                duration = time() - start_time\n",
    "                log.info('{} finished'.format(func.__name__))\n",
    "    return wrapper\n",
    "\n",
    "def run_helper(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if execute_helper:\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            return\n",
    "    return wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from tensorflow as described in [https://www.tensorflow.org/text/tutorials/text_generation](https://www.tensorflow.org/text/tutorials/text_generation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_helper\n",
    "def showcase_dataset():\n",
    "    text = open('datasets\\\\corpus.txt', 'rb').read().decode(encoding='utf-8')\n",
    "    print(f'Length ot text: {len(text)} characters')\n",
    "    print(text[:250])\n",
    "    vocab = sorted(set(text))\n",
    "    print(f'{len(vocab)} unique characters')\n",
    "\n",
    "# uncomment to run\n",
    "# execute_helper = True\n",
    "showcase_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load textfile as dataset and create vocab for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 14:04:48 INFO     create_vocab_from_textdata started\n",
      "2023-05-04 14:04:48 INFO     load_dataset started\n",
      "2023-05-04 14:04:48 INFO     load_dataset finished\n",
      "2023-05-04 14:04:48 INFO     create_vocab started\n",
      "2023-05-04 14:11:10 INFO     create_vocab finished\n",
      "2023-05-04 14:11:10 INFO     create_vocab_from_textdata finished\n",
      "2023-05-04 14:11:10 INFO     write_vocab_file started\n",
      "2023-05-04 14:11:10 INFO     write_vocab_file finished\n"
     ]
    }
   ],
   "source": [
    "@log_dec\n",
    "def load_dataset(dataset_text_file):\n",
    "    return tf.data.TextLineDataset(filenames=dataset_text_file)\n",
    "\n",
    "@log_dec\n",
    "def create_vocab(dataset):\n",
    "    bert_vocab_args=dict(\n",
    "        vocab_size = 8000,\n",
    "        reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"],\n",
    "        bert_tokenizer_params = dict(lower_case=True),\n",
    "        learn_params = {},\n",
    "    )\n",
    "\n",
    "    story_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "        dataset.batch(1000).prefetch(2),\n",
    "        **bert_vocab_args\n",
    "    )\n",
    "    return story_vocab\n",
    "\n",
    "@run_helper\n",
    "@log_dec\n",
    "def create_vocab_from_textdata(text_file='datasets\\\\corpus.txt'):\n",
    "    dataset = load_dataset(text_file)\n",
    "    vocab = create_vocab(dataset)\n",
    "    return vocab\n",
    "\n",
    "@run_helper\n",
    "@log_dec\n",
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as file:\n",
    "        for token in vocab:\n",
    "            print(token, file=file)\n",
    "\n",
    "write_vocab_file('datasets\\\\vocab.txt', create_vocab_from_textdata())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tokenizer from vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_dec\n",
    "def create_tokenizer():\n",
    "    bert_tokenizer_params = dict(lower_case=True)\n",
    "    story_tokenizer = tf_text.BertTokenizer('datasets\\\\vocab.txt', **bert_tokenizer_params)\n",
    "    return story_tokenizer\n",
    "\n",
    "@run_helper\n",
    "@log_dec\n",
    "def test_tokenizer(tokenizer):\n",
    "    dataset = load_dataset('datasets\\\\corpus.txt')\n",
    "    dataset_short = dataset.take(2)\n",
    "    token_batch = list(map(lambda x: tokenizer.tokenize(x).merge_dims(-2, -1), dataset_short))\n",
    "    return token_batch\n",
    "\n",
    "tokenizer = create_tokenizer()\n",
    "batch = test_tokenizer(tokenizer)\n",
    "txt_tokens = tf.gather(create_vocab_from_textdata(), test_tokenizer())\n",
    "tf.strings.reduce_join(txt_tokens, separator=' ', axis=-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arrange(length)[:, np.newaxis]   # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)               # (1, depth)\n",
    "    angle_rads  = positions * angle_rates           # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1\n",
    "        )\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x *=tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_simu_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
