{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import functools\n",
    "from time import time\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=log.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "log_enabled = True\n",
    "run_helper = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dec(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            if log_enabled:\n",
    "                start_time = time()\n",
    "                log.info('{} started'.format(func.__name__))\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "        finally:\n",
    "            if log_enabled:\n",
    "                duration = time() - start_time\n",
    "                log.info('{} finished'.format(func.__name__))\n",
    "    return wrapper\n",
    "\n",
    "def run_helper(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if run_helper:\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            return\n",
    "    return wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer pipeline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from tensorflow as described in [https://www.tensorflow.org/text/tutorials/text_generation](https://www.tensorflow.org/text/tutorials/text_generation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'datasets\\\\corpus.txt'\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "vocab_path = 'datasets\\\\vocab.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocab from dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `load_dataset(dataset_text_file)`:\n",
    "\n",
    "    This function loads a text dataset from a file.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    `dataset_text_file`: The path of the text file to be loaded.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It uses the `TextLineDataset` function from TensorFlow's `tf.data` module to load the dataset from the specified file. Each line of the file becomes an element of the dataset.\n",
    "    - It returns the loaded dataset.\n",
    "\n",
    "2. `create_vocab(dataset)`:\n",
    "\n",
    "    This function creates a vocabulary from a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    `dataset`: The dataset from which to create the vocabulary.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It defines the parameters for the vocabulary, including the size of the vocabulary, reserved tokens, and BERT tokenizer parameters.\n",
    "    - It uses the `bert_vocab_from_dataset` function from TensorFlow's `bert_vocab` module to create the vocabulary from the dataset.\n",
    "    - It returns the created vocabulary.\n",
    "\n",
    "3. `create_vocab_from_textdata(text_file=dataset_path)`:\n",
    "\n",
    "    This function creates a vocabulary from a text file.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    `text_file`: The path of the text file from which to create the vocabulary. Default is `dataset_path`.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It loads the dataset from the specified text file using the `load_dataset()` function.\n",
    "    - It creates the vocabulary from the loaded dataset using the `create_vocab()` function.\n",
    "    - It returns the created vocabulary.\n",
    "\n",
    "4. `write_vocab_file(filepath, vocab)`:\n",
    "\n",
    "    This function writes a vocabulary to a file.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    `filepath`: The path of the file to which to write the vocabulary.\n",
    "    \n",
    "    `vocab`: The vocabulary to write.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It opens the specified file in write mode.\n",
    "    - It writes each token of the vocabulary to the file on a new line."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_dec\n",
    "def load_dataset(dataset_text_file):\n",
    "    return tf.data.TextLineDataset(filenames=dataset_text_file)\n",
    "\n",
    "@log_dec\n",
    "def create_vocab(dataset):\n",
    "    bert_vocab_args=dict(\n",
    "        vocab_size = 8000,\n",
    "        reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"],\n",
    "        bert_tokenizer_params = dict(lower_case=True),\n",
    "        learn_params = {},\n",
    "    )\n",
    "\n",
    "    story_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "        dataset.batch(1000).prefetch(2),\n",
    "        **bert_vocab_args\n",
    "    )\n",
    "    return story_vocab\n",
    "\n",
    "@run_helper\n",
    "@log_dec\n",
    "def create_vocab_from_textdata(text_file=dataset_path):\n",
    "    dataset = load_dataset(text_file)\n",
    "    vocab = create_vocab(dataset)\n",
    "    return vocab\n",
    "\n",
    "@run_helper\n",
    "@log_dec\n",
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as file:\n",
    "        for token in vocab:\n",
    "            print(token, file=file)\n",
    "\n",
    "#write_vocab_file('datasets\\\\vocab.txt', create_vocab_from_textdata())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tokenizer class from vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `add_start_end(ragged)`:\n",
    "\n",
    "    This function adds start and end tokens to a ragged tensor. The ragged parameter is a ragged tensor where each slice along the first dimension represents a tokenized string (for example, a sentence or a line of text), and the length of each slice varies depending on the number of tokens in the string. The function adds start and end tokens to each of these strings.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    `ragged`: The ragged tensor to which to add start and end tokens.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It finds the indices of the start and end tokens in the reserved tokens list.\n",
    "    - It creates tensors of start and end tokens with the same number of elements as the number of sequences in the ragged tensor.\n",
    "    - It concatenates the start tokens, the original ragged tensor, and the end tokens along the sequence axis.\n",
    "    - It returns the new ragged tensor with added start and end tokens.\n",
    "\n",
    "2. `cleanup_text(reserved_tokens, token_txt)`:\n",
    "\n",
    "    This function cleans up a tokenized text by removing certain reserved tokens.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    `reserved_tokens`: The list of reserved tokens from which to remove certain tokens.\n",
    "\n",
    "    `token_txt`: The tokenized text to clean up.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It creates a list of bad tokens by removing the \"[UNK]\" token from the reserved tokens list.\n",
    "    - It creates a regular expression pattern from the bad tokens list.\n",
    "    - It finds the cells in the tokenized text that match the bad tokens pattern.\n",
    "    - It removes the bad cells from the tokenized text using a boolean mask.\n",
    "    - It joins the cleaned tokenized text back into a single string with a space as the separator.\n",
    "    - It returns the cleaned text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_dec\n",
    "def add_start_end(ragged):\n",
    "    START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "    END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count, 1], START)\n",
    "    ends = tf.fill([count, 1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "@log_dec\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    bad_tokens = list(filter(lambda token: token != \"[UNK]\", reserved_tokens))\n",
    "    bad_tokens_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_tokens_re)\n",
    "    ragged_result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    result = tf.strings.reduce_join(ragged_result, separator=' ', axis=-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `StoryTokenizer`:\n",
    "\n",
    "    This class defines a custom tokenizer for story data using the BERT tokenizer. The class is a subclass of the `tf.Module` class in TensorFlow, which is a base class for building reusable and shareable machine learning modules.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    - `tokenizer`: A `tf_text.BertTokenizer` object, which is a tokenizer specifically designed to preprocess text for BERT models. It handles tasks like lowercasing and Unicode normalization in addition to tokenization.\n",
    "\n",
    "    - `_reserved_tokens`: A list of special tokens reserved for specific uses like padding, marking the start or end of sequences, etc.\n",
    "\n",
    "    - `_vocab_path`: A `tf.saved_model.Asset` object, which tracks the path to the vocabulary file and ensures that it is included when the module is exported to a SavedModel.\n",
    "\n",
    "    - `vocab`: A `tf.Variable` containing the vocabulary read from the vocabulary file.\n",
    "\n",
    "    Methods:\n",
    "\n",
    "    - `__init__(reserved_tokens, vocab_path)`: Initializes the tokenizer and the vocabulary from a given vocabulary file. It also defines concrete functions for the other methods. Concrete functions are TensorFlow graph functions that can be called directly and support serialization and SavedModels.\n",
    "\n",
    "    - `tokenize(strings)`: Tokenizes a batch of strings. This involves splitting the strings into words, subwords, or other meaningful units using the BERT tokenizer. It then adds start and end tokens to each tokenized string.\n",
    "\n",
    "    - `detokenize(tokenized)`: Converts a batch of tokenized text back into human-readable strings. This involves replacing token ids with the corresponding tokens from the vocabulary and joining them into strings. It also cleans up the text by removing certain reserved tokens.\n",
    "\n",
    "    - `lookup(token_ids)`: Converts a batch of token ids into the corresponding tokens from the vocabulary.\n",
    "\n",
    "    - `get_vocab_size()`: Returns the size of the vocabulary.\n",
    "\n",
    "    - `get_vocab_path()`: Returns the path of the vocabulary file.\n",
    "\n",
    "    - `get_reserved_tokens()`: Returns the list of reserved tokens.\n",
    "\n",
    "    Note: All methods of this class are decorated with `@tf.function`, meaning that they are compiled into TensorFlow graph functions for better performance. This is especially useful when the methods involve TensorFlow operations, as it allows the methods to be run in graph mode for speed, and it also enables them to be serialized and exported as part of a SavedModel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tf_text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:\n",
    "\n",
    "        # tokenize signature for a batch of strings\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "        \n",
    "        # detokenize and lookup signature for:\n",
    "        # * Tensor with shape [tokens] and [batch, tokens]\n",
    "        # * RaggedTensor with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        \n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        \n",
    "\n",
    "        # get_* methods take no argument\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        encoded = self.tokenizer.tokenize(strings)\n",
    "        merged_enc = encoded.merge_dims(-2, -1)\n",
    "        merg_enc_start_end = add_start_end(merged_enc)\n",
    "        return merg_enc_start_end\n",
    "    \n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "    \n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "    \n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "    \n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "    \n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StoryTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Luis\\Dateien\\Beschaeftigungen\\WiMi_Lingen\\Implementation\\TF_simulator_tensorflow\\tokenizer.ipynb Zelle 25\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Luis/Dateien/Beschaeftigungen/WiMi_Lingen/Implementation/TF_simulator_tensorflow/tokenizer.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m StoryTokenizer(reserved_tokens, vocab_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Luis/Dateien/Beschaeftigungen/WiMi_Lingen/Implementation/TF_simulator_tensorflow/tokenizer.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m@log_dec\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Luis/Dateien/Beschaeftigungen/WiMi_Lingen/Implementation/TF_simulator_tensorflow/tokenizer.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_tokenizer\u001b[39m(tokenizer):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Luis/Dateien/Beschaeftigungen/WiMi_Lingen/Implementation/TF_simulator_tensorflow/tokenizer.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mcorpus.txt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StoryTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "@log_dec\n",
    "def test_tokenizer(tokenizer):\n",
    "    dataset = load_dataset('datasets\\\\corpus.txt')\n",
    "    dataset_short = dataset.take(2)\n",
    "    token_batch = list(map(lambda x: tokenizer.tokenize(x), dataset_short))\n",
    "    text = list(map(lambda x: tokenizer.detokenize(x), token_batch))\n",
    "    return token_batch, text\n",
    "\n",
    "test_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'story_corpus_tokenizer'\n",
    "tf.saved_model.save(tokenizer, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'story_corpus_tokenizer'\n",
    "reload_story_tokenizer = tf.saved_model.load(model_name)\n",
    "reload_story_tokenizer.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = reload_story_tokenizer.tokenize(['Hello TensorFlow!'])\n",
    "tokens.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = reload_story_tokenizer.lookup(tokens)\n",
    "text_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_simu_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
