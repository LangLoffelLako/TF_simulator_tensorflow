{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab5bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:21.815663900Z",
     "start_time": "2024-01-16T09:57:16.327504900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logging und Decorators\n",
    "import logging as log\n",
    "\n",
    "# Tensorflow Module\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Visualisierung und Eingabe\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact_manual, interactive, interact, VBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Backend Module\n",
    "from interactive_inference_backend import ModelLoader, StoryTokenizer, WordComplete, VisualWrapper, positional_encoding\n",
    "from interactive_inference_backend import reserved_tokens, vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff4da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.461354500Z",
     "start_time": "2024-01-16T09:57:23.186207800Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ModelLoader(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                            d_model=512,\n",
    "                            n_stacks=2,\n",
    "                            h_att=4,\n",
    "                            load_model=True,\n",
    "                            model_load_path=\"model_N2_h4_d512_t20230831-134344\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc133b",
   "metadata": {},
   "source": [
    "# Interaktive Erklärung der Transformerarchitektur\n",
    "\n",
    "## Inhaltsverzeichnis\n",
    "- [Einleitung](#einleitung)\n",
    "    - [Disclaimer](#disclaimer)\n",
    "    - [Ziel des Artikels](#ziel-dieses-interaktiven-artikels)\n",
    "- [Grundlagen](#basics)\n",
    "    - [Warum Transformer?](#transformer_simple)\n",
    "    - [Textverarbeitung: Wie wird Text für den Transformer vorbereitet?](#text_prep_simple)\n",
    "        - [Tokenization: Wie ein Satz in sinnvolle Einheiten zerlegt wird](#Tokenization_simple)\n",
    "        - [Embedding: Wie Wörter als Zahlen dargestellt werden](#embedding_simple)\n",
    "        - [Positional Encoding: Warum die Reihenfolge der Wörter wichtig ist](#positional-encoding_simple)\n",
    "    - [Attention: Wie erkennt der Transformer wichtige Wörter?](#attention_simple)\n",
    "        - [Warum ist Attention der Schlüssel zu modernen KI-Modellen?](#why_attention)\n",
    "        - [Self-Attention einfach erklärt: Wie Worte sich gegenseitig beeinflussen](#self_attention_simple)\n",
    "    - [Masking: Warum und wie werden bestimmte Wörter versteckt?](#masking_simple)\n",
    "    - [Trainingsmethoden: So lernt ein Transformer Schritt für Schritt](#training_methods)\n",
    "    - [Zusammenfassung: Wie alle Bausteine zusammenkommen](#summary_simple)\n",
    "- [Transformer im Detail](#trainingsmethoden)\n",
    "    - [Architekturübersicht](#architekturübersicht)\n",
    "        - [Encoder-Decoder](#encoder-decoder)\n",
    "        - [Architekturvarianten](#architekturvarianten)\n",
    "        - [Architekturblöcke](#architekturblöcke)\n",
    "    - [Input](#input)\n",
    "        - [Tokenization](#Tokenization)\n",
    "        - [Byte-Pair Encoding](#byte-pair-encoding)\n",
    "        - [Embedding](#embedding)\n",
    "        - [Positional Encoding](#positional-encoding)\n",
    "        - [Trainingsmethoden](#trainingsmethoden)\n",
    "            - [Dropout](#dropout)\n",
    "            - [Normalisierung](#normalization)\n",
    "            - [Residual Connection](#residual-connection)\n",
    "    - [Layers](#layers)\n",
    "        - [Attention](#attention)\n",
    "            - [Vorteile von Transformern](#vorteile-von-transformern)\n",
    "            - [Attention als Funktion](#attention-function)\n",
    "            - [Skalierung mit $\\sqrt{d_{model}}$](#skalierung-mit-sqrd_k)\n",
    "            - [Multi-Headed Attention](#multi-headed-attention)\n",
    "        - [Masking](#masking)\n",
    "            - [Padding Masking](#padding-masking)\n",
    "            - [Subsequent Masking](#subsequent-masking)\n",
    "        - [Attention-Mechanismen](#attention-mechanismen)\n",
    "            - [Self-Attention](#self-attention)\n",
    "            - [Cross-Attention](#cross-attention)\n",
    "            - [Masked Attention](#masked-attention)\n",
    "- [Simulation](#simulation)\n",
    "- [Bibliographie](#bibliographie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94fa9c-ba0a-4b73-92ed-2f9d2965a7ac",
   "metadata": {},
   "source": [
    "# Interaktive Erklärung der Transformerarchitektur\n",
    "\n",
    "## Inhaltsverzeichnis\n",
    "- [Einleitung](#einleitung)\n",
    "    - [Disclaimer](#disclaimer)\n",
    "    - [Ziel des Artikels](#ziel-dieses-interaktiven-artikels)\n",
    "    - [Transformer: Motivation & Kernkomponenten](#kurzübersicht-transformer)\n",
    "    - [Architekturübersicht](#architekturübersicht)\n",
    "        - [Encoder-Decoder](#encoder-decoder)\n",
    "        - [Architekturvarianten](#architekturvarianten)\n",
    "        - [Architekturblöcke](#architekturblöcke)\n",
    "- [Input](#input)\n",
    "    - [Tokenization](#Tokenization)\n",
    "    - [Byte-Pair Encoding](#byte-pair-encoding)\n",
    "    - [Embedding](#embedding)\n",
    "    - [Positional Encoding](#positional-encoding)\n",
    "- [Trainingsmethoden](#trainingsmethoden)\n",
    "    - [Dropout](#dropout)\n",
    "    - [Normalisierung](#normalization)\n",
    "    - [Residual Connection](#residual-connection)\n",
    "- [Layers](#layers)\n",
    "    - [Attention](#attention)\n",
    "        - [Vorteile von Transformern](#vorteile-von-transformern)\n",
    "        - [Attention als Funktion](#attention-function)\n",
    "        - [Skalierung mit $\\sqrt{d_{model}}$](#skalierung-mit-sqrd_k)\n",
    "        - [Multi-Headed Attention](#multi-headed-attention)\n",
    "    - [Masking](#masking)\n",
    "        - [Padding Masking](#padding-masking)\n",
    "        - [Subsequent Masking](#subsequent-masking)\n",
    "    - [Attention-Mechanismen](#attention-mechanismen)\n",
    "        - [Self-Attention](#self-attention)\n",
    "        - [Cross-Attention](#cross-attention)\n",
    "        - [Masked Attention](#masked-attention)\n",
    "- [Simulation](#simulation)\n",
    "- [Bibliographie](#bibliographie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634588dd",
   "metadata": {},
   "source": [
    "## <a id=\"einleitung\"></a>Einleitung\n",
    "\n",
    "### <a id=\"disclaimer\"></a>Disclaimer\n",
    "\n",
    "Wir haben diesen Artikel für eine deutschsprachige Leserschaft geschrieben. Da viele Fachbegriffe oft kein deutsches Equivalent besitzen, haben wir uns entschieden, die Fachsprache aus dem Gebiet des Machine Learning möglichst einheitlich in Englisch zu halten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a793e0",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"ziel-dieses-interaktiven-artikels\"></a>Ziel des Artikels \n",
    "\n",
    "Das Ziel dieses Artikels besteht darin, die Transformerarchitektur aus Vaswani et al. [1] durch die Simulation ihrer Verarbeitungsschritte und Komponenten zu erläutern. Diese Simulation erlaubt es, interaktiv die Auswirkungen verschiedener Eingaben auf die Verarbeitungsschritte zu untersuchen und so ein schrittweises Verständnis der Gesamtverarbeitung zu entwickeln. Während sich wissenschaftliche Literatur wie der ursprüngliche Artikel [1] und darauf aufbauende wissenschaftliche Arbeiten [7, 8], Erklärartikel oder -videos [9, 10] sich oft auf einzelne Komponenten wie z.B. Attention-Blöcke konzentrieren, werden andere Elemente, die technischen Feinheiten der Architektur, wie z.B. \"<a href=\"#dropout\">Dropout</a>\" [11], \"<a href=\"#residual-connection\">Residual Connections</a>\" [12], \"<a href=\"#byte-pair-encoding\">Byte-Pair Encoding</a>\" [13], Embedding oder des \"Log-Softmax Algorithmus\" [5] oft nicht oder nur rudimentär erklärt. Genau diese Begriffe möchten wir erklären und darüber hinaus durch unsere Simulation erfahrbar machen. Sollten sie als Manager oder als Entwickler in Betracht sein eine Transformerarchitektur für eine Machine Learning Anwendung in Betracht zu ziehen, dann möchten wir, dass sie am Ende des Artikels verstehen, wie die Technologie funktioniert und wie sie sie implementieren könnten oder jemand anderes sie implementiert hat.\n",
    "Zudem soll über die Simulation das Zusammenspiel der Elemente der Transformer Architektur verdeutlicht werden, was wesentlich ist für ihr Verständnis. Wir wünschen viel Spaß beim Lesen und Simulieren.\n",
    "\n",
    "#### Voraussetzungen\n",
    "\n",
    "Trotz unseres Ziels die Transformer möglichst umfänglich zu erklären setzen wir einige Informationen ihrerseits voraus. Sie müssen keinerlei Vorwissen über die Funktionsweise von Transformern mitbringen, allerdings sollten sie dazu in der Lage sein die mathematische Theorie, sowie die Logik, die hinter der Entwicklung von Machine Learning Algorithmen zu verstehen. Spezifisch setzen wir die Begriffe und Ideen der lineare Algebra und der Wahrscheinlichkeitstheorie, die dem Machine Learning zugrundeliegen voraus. Alternativ empfehlen wir sich die Grundlagen beider Theorien im Standardwerk des Deep Learning anzueignen [5]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d44de",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"kurzübersicht-transformer\"></a>Transformer: Motivation & Kernkomponenten\n",
    "\n",
    "Transformer-Modelle sind eine von Vaswani et al. [1] vorgeschlagene Architektur für das Modellieren sequenzieller Daten, also Daten, die räumlich oder zeitlich geordnet sind, wie z.B. Zeitreihen, Text- und Bilddaten. Im Gegensatz zu zuvor genutzten Architekturen wie Recurrent Neural Networks (RNN) [2, 3] oder Convolutional Neural Networks (CNN) [4] ermöglichen Transformer das parallele Verarbeiten sequentieller Daten. Dies ermöglicht, bei Verfügbarkeit entsprechend parallel arbeitender leistungsfähiger Hardware (wie insbesondere \"GPUs\" - Graphical Processing Units mit i.d.R. einigen tausend parallelen Recheneinheiten), oftmals eine signifikante Reduktion der Trainingszeiten [1, 6]. Typischerweise werden diesen Modellen Aufgaben gestellt, bei denen es darum geht, das nächste Element einer Sequenz vorherzusagen, wie beispielsweise den nächsten Datenpunkt einer Zeitreihe, das nächste Wort in einem Textabschnitt oder das nächste Bild in einem Video. Zur Veranschaulichung konzentrieren wir uns im Folgenden auf die Verarbeitung von Textdaten.\n",
    "\n",
    "Kernbestandteil von Transformermodellen bilden sog. \"Attention\"-Blöcke. Diese Blöcke steuern welchen Teilen der Eingabe das Modell seine \"Aufmerksamkeit\" bei der Datenverarbeitung widmet. Das bedeutet, dass nicht jedes Wort in einer Wortfolge dieselbe Bedeutung für die Prognose des nächsten Worts hat. Genauer gesagt verarbeitet das Modell nicht unbedingt ganze Wörter, sondern sog. \"Token\". Dabei handelt es sich in der Regel um häufig autretende Buchstabenkombinationen, die ganze Wörter, Wortteile oder zusammenhängende Wörter, in den zu lernenden Texten sein können. Attention-Blöcke sind mathematisch betrachtet trainierbare Matrizen, die die Eingabe (z.B. ein Vektor, der die Frage an einen Chatbot kodiert) auf die gewünschte Ausgabe (wiederum ein Vektor, der die Antwort auf diese Frage kodiert) projizieren. Idealerweise projiziert das Transformer-Modell dabei die Eingabe genau so, dass dabei eine möglichst passende Antwort generiert wird. Dies ähnelt dem Lesen eines Textes, um eine ganz bestimmte Frage zu beantworten. Genau wie Menschen filtert das Transformer-Modell genau die Informationen aus, die eine hohe Relevanz zur Beantwortung der Frage besitzen.\n",
    "\n",
    "Während des Trainings von Texten verwenden Transformer-Architekturen eine sog. \"Maske\", um einen natürlichen Lesefluss zu imitieren und gleichzeitig zu verhindern, dass das Modell Einblick in zukünftige Informationen erhält, die es prognostizieren soll. Mithilfe der Maske stützt sich die Prognose nur auf bereits bekannten Daten der Sequenz, das heißt, auf die zuvor gelesenen Wörter. Nehmen wir den Satz \"Der Himmel ist klar und blau\" als Beispiel: Während des Trainings werden die Wörter \"Der Himmel ist\" dem Modell präsentiert und es soll das Wort \"klar\" lernen. Das zu lernende Wort \"klar\" und all nachfolgenden Wörter wie \"und blau\" werden maskiert, da sie in dem natürlichen Lesefluss erst nach \"Der Himmel ist\" gelesen werden würden. Diese Technik etabliert eine Verbindung zwischen der aktuellen Position im Text und den bis zu diesem Punkt bekannten Informationen, ohne zukünftige Inhalte preiszugeben. Mit jeder Trainingsinstanz verschiebt sich die Maske schrittweise weiter über den Text, entsprechend dem Lesefluss. Während der Abfragephase, auch \"Inference\" genannt, wird dann zunächst das erste Token der Antwort generiert und im nächsten Schritt der Eingabe hinzugefügt, um das zweite Token zu generieren. Sobald die maximale Größe des sog. \"Context-Windows\" (=maximale Anzahl von Tokens in einer Eingabe) erreicht ist, wird das erste Token der Eingabe wieder entfernt. Dieses Context-Window und stellt quasi das Gedächtnis eines Transformer-Modells dar. \n",
    "\n",
    "Im Transformermodell werden sowohl Eingaben als auch Ausgaben intern als Vektoren repräsentiert. Die Umwandlung von Textworten in aussagekräftige Vektoren, die Funktionsweise weiterer Komponenten wie Attention-Blöcke und Feed Forward-Netze sowie die Rückführung von Vektoren zu Wörtern werden in den nachfolgenden Kapiteln beschrieben. Zudem werden zentrale Fachbegriffe und Konzepte wie „Positional Encoding“, „Embedding“ und „Normalisierung“ dargestellt. Darüber hinaus lassen sich alle Schritte in einem simulierbaren Transformermodell in diesem interaktiven Artikel schrittweise nachvollziehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b568b",
   "metadata": {},
   "source": [
    "### <a id=\"architekturübersicht\"></a>Architekturübersicht\n",
    "\n",
    "#### <a id=\"encoder-decoder\"></a>Encoder-Decoder\n",
    "Eine Encoder-Decoder Architektur setzt sich aus zwei voneinander getrennten neuronalen Netzen zusammen: dem Encoder und dem Decoder, die aber gemeinsam trainiert werden. Diese Architektur wurde von Cho et al. [14] vorgeschlagen und findet vor allem im Kontext von maschineller Übersetzung ihre Anwendung. Der wesentliche Vorteil im Kontext von Übersetzungen besteht darin, dass sowohl der Encoder als auch der Decoder ausgestauscht werden können. Theoretisch könnte man somit für jede Sprache einen Encoder und einen Decoder trainieren, um dann Übersetzungen zwischen beliebigen Sprachpaaren zu realisieren.\n",
    "Der Encoder komprimiert dabei die Quellsprache zu einer Repräsentation im sogenannten \"Latent Space\" - ein Vektorraum von abstrakten Repräsentationen der durch die Verarbeitung der Eingabedaten durch das Modell gebildet wird. Der Decoder hingegen expandiert diese verdichteten Vektoren zurück in die Zielsprache.\n",
    "Transformer für die Textvervollständigung können auch ausschließlich auf Decodern beruhen [16], dann werden die Informationen der Eingabedaten direkt verarbeitet, anstatt dezidiert im Latent Space kodiert zu werden. Trotzdem kann man auch hier in den verschiedene Zwischenschritten eine kodierte Vektorrepräsentation der Eingabedaten finden.\n",
    "\n",
    "In unserem Beispielarchitektur wird der Encoder zum Kodieren der Eingabe verwendet. Der Decoder verwendet sowohl die Eingabedaten direkt, als auch die vom Encoder erzeugte Repräsentation, um eine Vorhersage für die Ausgabe zu machen.\n",
    "Hierbei kommen zwei Mechanismen Self-Attention, um eine einzelne Eingabe zu verarbeiten, und Cross-Attention, um bereits kodierte Repräsentationen zu kombinieren, zum Einsatz. Ein Encoder verwendet nur Self-Attention, ein Decoder verwendet sowohl Self-Attention wie auch Cross-Attention.\n",
    "\n",
    "In <a href=\"#fig:fig1\">Abbildung 1</a> sieht man wie Encoder und Decoder zusammenwirken. Der Encoder besteht aus Self-Attention Modulen, daran zu erkennen, dass Source und Target Input aus derselben Vektordarstellung des davorliegenden Blocks besteht. Durch Wiederholung dieser Architektur, kodiert der Encoder die Informationen, die er aus der Eingabe erhält. Der Decoder kombiniert die kodierte Repräsentation des Encoder mit den Informationen, die er aus der Eingabe erhält. Dafür nutzt er Cross-Attention Module, die Source und Target Input aus verschiedenen Quellen kombinieren. In unserer Architektur besteht dabei der Source Input aus der vom Encoder erzeugten Repräsentation und Target Input aus der im vorherigen Modul vom Decoder erzeugten Repräsentation. Näheres zu den verschiedenen Attention Mechanismen siehe im Kapitel [Attention](#attention).\n",
    "\n",
    "<i><b style=\"color:red;\">Der Decoder nutzt nicht direkt die Eingaben des Encoders, sondern nur den Latent Space des Encoders. Die weitere Eingabe in den Decoder ist der selbst von Decoder generierte Kontext. Bitte einmal in Text und Grafik differenzieren.</b></i> \n",
    "\n",
    "<i><b style=\"color:red;\">Achte bitte darauf, dass alle Begriffe (mehrmals) verlinkt sind, z.B. Dropout (habe ich direkt geändert), PosEncoding, u.s.w. Initial mit \"\", dann aber immer auch noch verlinken auf die jeweilige Passage.</i></b> \n",
    "\n",
    "Desweiteren kann man hier sehen, wie die Input-Pipeline aufgebaut ist. Eine Nutzereingabe wird zuerst durch den Tokenizer zerlegt, um dann durch ein trainierbares Input Embedding in das Encoder-/Decoder-Embedding verwandelt zu werden. Dieses wird dann noch durch das Positional Emcoding mit Positionsinformationen versehen und dann in die Transformer-Layer überführt.\n",
    "\n",
    "In <a href=\"#fig:fig1\">Abbildung 1</a> sind alle Architekturelemente des Encoders im hellblau hinterlegten Kasten zu finden, die des Decoders hingegen im dunkelblau hinterlegten Kasten zu finden. Der Nutzer Input, sowie der Generator, der aus der Vektorausgabe des Decoder einen Text generiert sind weder Teil des Encoder noch des Decoder.\n",
    "Das sich der Encoder und Decoder sehr ähnlich sehen ist kein Zufall. Prinzipiell handelt es sich um den selben Aufbau in beiden Teilmodellen. Allerdings sind diese Strukturen tatsächlich doppelt notwendig, da sowohl die Input-Pipeline, wie z.B. das Embedding, als auch die Transformer Layers in beiden Teilen für verschiedene Aufgaben trainiert werden.\n",
    "Graue Elemente sind der Input und Output, sowie die Architekturelemente mit Hyperparametern, in diesem Fall der <a href=\"#dropout\">Dropout</a>, da die Dropout Rate vor dem Training des Modells festgelegt wird. Schwarze Elemente, wie z.B. das Positional Encoding oder der Logarithmische Softmax sind deterministische Module, die durch das Modelltraining unverändert bleiben und auch keine Hyperparameter akzeptieren.\n",
    "Gelbe Elemente stellen Architekturelemente dar, die trainierbare Parameter enthalten, z.B. die Embedding Weights und die einzelnen Transformer Layers, die in <a href=\"#fig:fig4\">Abbildung 4</a> noch genauer dargestellt werden.\n",
    "Die blauen Elemente stellen die verschiedenen Datenelemente dar, die durch die Operationen des Modells ineinander transformiert werden. Sind zwei blaue Elemente miteinander verbunden, heißt das, dass es sich um die selben Daten handelt.\n",
    "Informationen fließen durch das Modell immer nur in Pfeilrichtung. Speziell in unserer Abbildung ist die Wiederholung der Transformer Layer. Wir deuten hier ein, wie zwei Transformer Layer miteinander verkettet werden, dies geschieht allerdings in der Architektur n-mal, wobei $n$ einer der Hyperparameter eines Transformer Modells ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847af04f",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig2\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_model_architecture.jpg\" style=\"max-width: 100%; max-height: 150vh; height: auto;\" alt=\"Transformer als Encoder-Decoder\"/>\n",
    "    <figcaption>Abbildung 1: Transformer als Encoder-Decoder</figcaption>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0d18b",
   "metadata": {},
   "source": [
    "#### <a id=\"architekturvarianten\"></a> Architekturvarianten und verwirklichte Modelle\n",
    "\n",
    "Transformer können auch als reine Encoder oder reine Decoder-Architektur verwendet werden. Encoder verdichten Informationen (z.B. zur Klassifikation, Sentiment-Analyse, oder Clustering), während Decoder generativ (z.B. zur Textfortsetzung oder Bildgenerierung) eingesetzt werden, um aus einer verdichteten Repräsentation wieder Informationen zu generieren. Wie beschrieben war die Encoder-Decoder Architektur vor allem für Aufgaben des maschinellen Übersetzens gedacht. In der aktuellen Umsetzung sieht man jedoch häufig reine Encoder oder Decoder Architekturen. Dies liegt daran, dass die Architekturkomplexität dabei geringer ist und die spezifische Encoder-Decoder Architektur in anderen Aufgabenfeldern bzgl. Anforderungen und Trainingseffizienz gleichauf liegt. In der folgenden Tabelle [23] findet sich eine Auflistung der bekanntesten Modelle nach Architekturtyp.\n",
    "\n",
    "| Encoder           | Encoder-Decoder | Decoder        |\n",
    "|-------------------|-----------------|----------------|\n",
    "| BERT, DistillBERT | T5              | GPT            |\n",
    "| RoBERTa           | BART            | GPT-2          |\n",
    "| XLM, XLM-R        | M2M-100         | GPT-3          |\n",
    "| ALBERT            | BugBird         | GPT-4          |\n",
    "| ELECTRA           |                 | GPT-Neo, GPT-J |\n",
    "| DeBERTa           |                 |                | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398698e4",
   "metadata": {},
   "source": [
    "Hier können Sie nun einen Beispielsatz zuerst vom Encoder kodieren lassen, um ihn dann im nächsten Schritt vom Decoder dekodieren zu lassen und damit eine Ausgabe zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798807b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.504807400Z",
     "start_time": "2024-01-16T09:57:25.462149900Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "transformer = model.model\n",
    "\n",
    "input_widget_enc_dec = widgets.Text(\n",
    "    value='Encoder-Decoder Test',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_enc = widgets.Button(description='Wende den Encoder auf die Eingabe an.',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "button_widget_dec = widgets.Button(description='Wende den Decoder auf die Eingabe an.',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_enc = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "output_widget_dec = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def encode():\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    if len(tensor_input.shape) == 0:                                           # Überprüft, ob der Eingabetensor im korrekten Format ist\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # Falls nicht, wird eine Dimension hinzufügt \n",
    "    \n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')\n",
    "    \n",
    "    context = transformer.encode(input_without_eos, None)                      # Kodierung des Inputsatzes von (Transformer-Modell)\n",
    "    return context, string_value, lookup\n",
    "\n",
    "\n",
    "def decode(): \n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)   # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "    if len(tensor_input.shape) == 0:                                           # wie bei der Encodierung\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # wie bei der Encodierung\n",
    "\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # wie bei der Encodierung\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    \n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')                      \n",
    "    context = transformer.encode(input_without_eos, None)                     \n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "                              \n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):                        # Schleife durch jedes Token des Satzes\n",
    "      output_array = output_array.write(i, value)                              # Speichern des Tokens im Output array\n",
    "\n",
    "    dec_input = output_array.concat()[tf.newaxis]                              # Output Array wird zu einem einzigen Tensor konkateniert \n",
    "                                                                               # und anschließend um eine zusätzliche Dimension erweitert\n",
    "\n",
    "    dec_out = transformer.decode(context, None, dec_input, None)               # Decoder des Transformer-Modells wird verwendet, um den dec_input-Tensor \n",
    "                                                                               # unter Verwendung des zuvor berechneten Kontexts zu decodieren.\n",
    "\n",
    "    return dec_out, string_value, lookup\n",
    "\n",
    "def on_button_click_enc(b):\n",
    "  with output_widget_enc:\n",
    "    output_widget_enc.clear_output()  # clear the previous output\n",
    "    context, tokens, lookup = encode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(context)\n",
    "\n",
    "\n",
    "def on_button_click_dec(b):\n",
    "  with output_widget_dec:\n",
    "    output_widget_dec.clear_output()\n",
    "    dec_out, tokens, lookup = decode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(dec_out)\n",
    "\n",
    "button_widget_enc.on_click(on_button_click_enc)\n",
    "button_widget_dec.on_click(on_button_click_dec)\n",
    "\n",
    "ui = widgets.VBox([\n",
    "  input_widget_enc_dec, \n",
    "  widgets.HBox([\n",
    "    widgets.VBox([\n",
    "      button_widget_enc, \n",
    "      output_widget_enc],\n",
    "      layout=widgets.Layout(width='50%')),\n",
    "    widgets.VBox([\n",
    "      button_widget_dec, \n",
    "      output_widget_dec],\n",
    "      layout=widgets.Layout(width='50%'))\n",
    "    ])\n",
    "  ])\n",
    "display(ui)\n",
    "\n",
    "\n",
    "#print('tok_out', tokenized_input)\n",
    "#print('enc_out', context)\n",
    "#print(\"dec_out\", dec_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa505ea8-2a43-43f6-936e-0d45b4c96f75",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "Über der Grafik sind die verarbeiteten Tokens dargestellt. Die Positionierung der Tokens entlang der y-Achse der Grafik zeigt die Reihenfolge der Tokens im Beispielsatz an, wobei Leerzeichen nicht berücksichtigt werden. Das Model erkennt unterschiedliche Wörter mit Hilfe der \"##\" Zeichen. Durch diese Zeichen sieht das Modell, dass ein Token mit \"##\" Zeichenkette zu dem vorherigen Token gehört wie im Beispiel bei dem Wort \"Encoder\", welches aus den Tokens 'e', '##n', '##co', '##der' besteht. Weitere Informationen in Abschnitt [Byte-Pair Encoding](#byte-pair-encoding).\n",
    "\n",
    "Schaut man sich die Wörter vor der Tokenisierung an, sieht man das alle Wörter nun kleingeschrieben werden. Durch derartige Zusammenfassungen wird die Größe des Vokabulars reduziert. Linguistische und grammatische Informationen, die aufgrund der Zusammenführung verloren zu gehen scheinen, bleiben i.d.R. durch die Position und den Kontext des Wortes im Satz erhalten, wie zum Beispiel Substantivierungen. Dies führt dazu, dass das Modell stärker auf die Position des Wortes im Satz achtet. Der Tokenizer fügt zudem ein Start- und Endtoken hinzu. Das Endtoken wird im Modell nicht weiterverwendet, während das Starttoken die Position 0 in der weiteren Verarbeitung und Visualisierung einnimmt.\n",
    "\n",
    "Die x-Achse repräsentiert die Tiefe der Token bzw. die Anzahl der Dimensionen des Vektors, der jedes Token darstellt. In diesem Fall beträgt die Tiefe 512, was bedeutet, dass jedes Token durch 512 verschiedene Werte charakterisiert wird. Diese Vektoren werden so gebildet, dass Tokens, die eine ähnliche Bedeutung haben oder aus einem Themenfeld stammen, ähnlichere Vektoren haben. Im Gegensatz dazu haben unähnliche Token eher unähnliche Vektoren. Eine hohe Dimensionalität der Vektoren kann allerdings dazu führen, dass Ähnlichkeiten primär in individuellen Vektorwerten erkennbar sind, wodurch die allgemeine Übersichtlichkeit eines Vektors im Vergleich zu einem ansonsten ähnlichen Vektor beeinträchtigt werden kann. Trotzdem tendieren ähnliche Vektoren dazu, auch in ihrer Gesamtorientierung Übereinstimmungen aufzuweisen. <b style='color:red;'><i>Die vorherigen beiden Sätze erschließen sich mir nicht. - So besser?</b></i> Zusätzlich ist in den Vektoren die Reihenfolge der Wörter im Satz kodiert, was als sogenanntes [Positional Encoding](#positional-encoding) bezeichnet wird.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028a3317",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### <a id=\"architekturbloecke\"></a>Architekturblöcke\n",
    "\n",
    "Prinzipiell lassen sich Transformer in mehrere Module einteilen: die Umwandlung der Eingabe in eine Vektorrepräsentation, die Abbildung der Eingaberepräsentation durch die Attention-Layers in einen Ausgabevektor, die Umwandlung des Ausgabevektor in eine menschenlesbare Datenform als Ausgabe.\n",
    "In <a href=\"#fig:fig1\">Abbildung 1</a> können wir die Ausgabe leicht von den anderen Elementen unterscheiden, sie besteht aus all denen Teilen die außerhalb der Umrandungen für Encoder und Decoder zu finden sind. Die Eingabe wiederum besteht aus dem sich für Encoder und Decoder gleichenden Teil der die Nutzer Input Daten in den Source und Target Input für die Transformer Layer umwandelt.\n",
    "\n",
    "1. [Eingabe](#input)\n",
    "\n",
    "Die Eingabe wandelt die Eingabedaten in eine Form, die für die Matrixtransformation genutzt werden kann. Wie diese Umwandlung aussieht unterscheidet sich für jeden Eingabedatentypen. In unserem Beispiel nutzen wir Textdaten, die wir durch <a href=\"#Tokenization\">Tokenization</a> [17] und <a href=\"#embedding\">Embedding</a> [18] in Tensoren verwandeln. Ein Tensor ist eine mathematische Entität um multidimensionale Daten darzustellen. Ein Skalar (eine einzelne Zahl) ist ein Tensor der 0. Ordnung, ein Vektor (eine eindimensionale Liste von Werten) ein Tensor der 1. Ordnung, eine Matrix ein Tensor der 2. Ordnung (eine zweidimensionale Tabelle von Zahlen) und multidimensionale Arrays von Zahlen ein Tensor höherer Ordnung. \n",
    "\n",
    "2. [Attention](#attention)\n",
    "\n",
    "Die Attention-Layers verarbeiten Daten in Tensorform und liefern eine Abbildung von den Eingabetensoren auf die Ausgabetensoren, die jeweils von den Eingabe- und Ausgabemodulen interpretiert wird.\n",
    "Man kann verschiedene Varianten von Attention-Layers dahingehend unterscheiden wie sich ihre Eingabedaten zusammensetzen. Jede Attention-Layer erhält als Eingabedaten dabei immer drei Tensoren. Diese werden Query, Key und Value genannt. Aus diesen berechnet eine Attention-Layer eine Ausgabe. Die Query fragt nach dem relevanten Kontext und repräsentiert das aktuelle Token für das das Modell Kontextinformationen sucht. Key stellt den Zusammenhang zwischen der Query und allen anderen Token her. Die Query wird mit allen Keys multipliziert und wenn der sich ergebende Wert groß ist, dann ist der Zusammenhang der zwei Token ebenfalls groß. Der Value gewichtet diesen Zusammenhang zur Bestimmung der Aufmerksamkeitswerte zwischen zwei Token (Attention Scores).\n",
    "\n",
    "Typischerweise werden Attention-Module dadurch unterschieden aus welcher Quelle Query, Key und Value stammen. Es wird unterschieden zwischen:\n",
    "\n",
    "- \"Self-Attention\" hierbei stammen Query, Key und Value aus einer Quelle. Jedes Token wird mit allen anderen Token der Sequenz verglichen, um kontextuelle Beziehungen in der Sequenz selbst zu extrahieren (z.B. in dem Satz \"Die Katze jagt die Maus um das Haus.\" könnte für das Wort \"Katze\" das Wort \"jagt\" eine hohe Relevanz haben, weil es die Aktion der Katze beschreibt). \n",
    "- \"Source-Attention\" oder \"Cross-Attention\" hierbei stammt Query aus einer anderen Quelle als Key und Value, z.B. der Decoder verarbeitet seine Ausgabe als Query, um das nächste Token zu berechnen. Dieses Queries werden mit den Keys und Values der Eingabe des Encoders abgeglichen. Die Keys dienen dazu, die Relevanz der Encoder Information für den aktuellen Query zu bestimmen.\n",
    "\n",
    "Desweiteren wird nach Art des Maskings unterschieden. Masking verdeckt immer einen Teil der Daten. In Transformern gibt es folgende Arten von Masking:\n",
    "\n",
    "- \"Subsequent Masking\", verdeckt alle Token nach dem zu prognostizierenden Token im Decoder-Attention-Block, z.B. \"Diese Eingabe ist [...]\" eine maskierte Version von \"Diese Eingabe ist ab hier maskiert.\"\n",
    "- \"Padding Masking\", verdeckt sog. \"Padding Tokens\", das sind Platzhalter Tokens mit der eine Sequenz auf die Länge des Context Windows aufgefüllt wird, um eine einheitliche Länge von Token als Eingabe zu erzeugen. Wenn wir z.B. nur Sätze mit fünf Wörtern erlauben, dann ist \"Ein kurzer Satz. <i>pad</i> <i>pad</i>\" eine erlaubte Version des Satzes \"Ein kurzer Satz.\" und \"Ein kurzer Satz. [...]\" wiederum eine maskierte Variante des erlaubten Satzes.\n",
    "\n",
    "3. Ausgabe\n",
    "\n",
    "Die Ausgabe interpretiert die Daten die das Attention-Modul erzeugt und formt sie in eine für menschlichen Gebrauch nützliche Form um, wie z.B. Textdaten oder Bilddaten. Dafür wird in Transformern oft eine Log-Softmax-Funktion verwendet, die auf die Tensorausgabe des letzten Attention-Blockes angewandt wird, um Wahrscheinlichkeitsverteilungen über mögliche Ausgabewerte, z.B. alle im Transformer kodierten Token bei Texten oder über alle sog. Bildpatches, d.h. Sammlungen von Pixeln, bei Bilddaten zu erzeugen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86373b1",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"input\"></a>Input\n",
    "\n",
    "Der erste Teil eines Transformermodells besteht aus der Eingabepipeline. Diese verarbeitet die Eingabe, z.B. die Texteingabe eines Nutzers, und bereitet sie auf die Verarbeitung in den Attention-Modulen vor. Die Attention-Module arbeiten über eine Attention-Matrix, die aus der jeweiligen Eingabe eine Ausgabe berechnet. Wir müssen also aus einer Eingabe in Textform eine Vektorrepräsentation erzeugen, die alle notwendigen Informationen für das Modell enthalten, um mithilfe von Matrixmanipulationen nützliche Vorhersagen zu machen.\n",
    "\n",
    "In <a href=\"#fig:fig2\">Abbildung 2</a> sehen sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt. \n",
    "Wie zu erkennen ist, werden in der Eingabepipeline während des Training eines Transformermodells ausschließlich die Weights für das Embedding verändert, alle anderen Funktionen sind rein deterministisch und bleiben damit vom Trainingsprozess unbeeinflusst.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caafaa5",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig2\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_input_pipeline.jpg\" style=\"height: auto; max-width: 50%; max-height: 150vh; \" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "    <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4578ca",
   "metadata": {},
   "source": [
    "Prinzipiell besteht die Eingabepipeline aus drei Modulen, die in den folgenden drei Abschnitten genauer erläutert werden:\n",
    "\n",
    "1. Die [Tokenization](#Tokenization), die den Text mithilfe eines Symbolalphabets in eine Zahlenkodierung umwandelt. So wird z.B. \"Transformer\" in die Zahlenfolge \"2 61 4334 93 6622 202 3\" umgewandelt.\n",
    "2. Das [Embedding](#embedding), das diese Kodierung mithilfe eines trainierbaren Algorithmus in eine Vektordarstellung umwandelt. Das Embedding lernt die komplexe Struktur eines Textes so darzustellen, dass sie informativ für die nachfolgenden Module ist. Wie genau diese Umwandlung aussieht ist dabei aufgrund der stochastischen Natur von Deep Learning Modellen nur schlecht logisch nachzuvollziehen. Eine Kodierung könnte z.B. die obige Zahlenfolge \"2 61 4334 93 6622 202 3\" in eine zweidimensinalen Vektor der Form (7, 512) mit Einträgen zwischen -1 und 1 verwandeln.\n",
    "3. Das [Positional Encoding](#positional-encoding) ein mit der Transformer-Architektur eingeführter Mechanismus. Im Gegensatz zu RNNs, die die Eingabedaten sequenziell präsentiert bekommen [19], enthalten bei Transformermodellen die Eingaben keine Information zur relativen Position der Tokens. Diese fehlenden Informationen werden in diesem Schritt manuell hinzugefügt indem <a href=\"#pos-enc-formula\">Sinuskurven</a> mit verschiedener Frequenz und Phase über die Eingabedaten gelegt werden.\n",
    "\n",
    "### <a id=\"Tokenization\"></a>Tokenization\n",
    "\n",
    "Bei der Tokenization wird der Satz in Textform z.B. \"Das ist ein Testsatz.\" in einen Zahlencode verwandelt. Hierfür kommen verschieden Methoden in Frage. Einer der simpelsten Methoden ist es jedem Buchstaben eine eindeutige Zahl zuzuordnen. Da hier jeder Buchstabe einzeln kodiert werden muss, führt das allerdings zu einer sehr langen Kodierung. Obwohl das also eine mögliche Kodierung wäre, gibt es bessere Verfahren.\n",
    "\n",
    "Die entscheidenden Faktoren für eine gute Kodierung sind\n",
    "- die Vollständigkeit der Kodierung, also ob beliebige Texte kodiert werden können, \n",
    "- die Vektorlänge, die eine Kodierung benötigt und \n",
    "- die Größe des dafür nötigen Vokabulars.\n",
    "\n",
    "Die Kodierung mit einzelnen Buchstaben ist vollständig (man kann beliebige Zeichenkombinationen kodieren) und besitzt ein kurzes Vokabular (26 für alle Buchstaben plus Sonderzeichen), allerdings ist die Länge der kodierten Vektoren sehr groß.\n",
    "\n",
    "Benutzt man ein Vokabular aus Wörtern, wird die Länge der Kodierung verkürzt, allerdings besteht die Gefahr der Unvollständigkeit. Um das nach Möglichkeit zu verhindern, benötigt man ein Vokabular, dass den Raum der möglichen Wörter vollständig abdeckt. Das Vokabular der Kodierung müsste also sehr groß sein.\n",
    "\n",
    "Die Probleme mit obigen Methoden hat dazu geführt, dass sich gemischte Verfahren entwickelt haben, die an einem möglichst großem Korpus trainiert werden. Es gibt einerseits Top-Down Verfahren, die von einem aus dem Korpus extrahierten Wort-Vokabular ausgehen und dann lernen unbekannte Worte in Teilworte zu zerlegen, die in das Vokabular mitaufgenommen werden. Andererseits gibt es Bottom-Up Verfahren, die von einem Buchstabenvokabular ausgehen und häufig wiederkehrende Kombinationen in dieses explizit aufnehmen.\n",
    "Die Transformerarchitektur nach [1] verwendet ein solches Bottom-Up Verfahren namens Byte-Pair Encoding [13], dass sich als effizient erwiesen hat und ein relativ kompaktes Vokabular ermöglicht dabei aber die Länge der Tokenization klein hält. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5e1d1",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"byte-pair-encoding\"></a>Byte-Pair Encoding\n",
    "\n",
    "Das Byte-Pair Encoding Verfahren nutzt ein Vokabular mit einer festgelegten Länge. In unserer Implementation des Tokenizer nutzten wir ein Vokabular von der Länge 8000. Das Vokabular wird dabei folgendermaßen erstellt:\n",
    "\n",
    "  1. Ein Text, der für die Erstellung des Vokabulars verwendet wird, wird in eine Sequenz von Buchstaben zerlegt. Wortenden werden mit einem zusätzlichen Symbol kodiert. Z.B. wird \"Ein Satz\" in \"[start]\", \"e\", \"i\", \"n\", \"s\", \"a\", \"t\", \"z\", \"[ende]\" zerlegt.\n",
    "  2. Alle vorhandenen Symbole werden automatisch in das Vokabular aufgenommen.\n",
    "  3. Nun wird das häufigste 2-Gramm, also zwei aufeinander folgende Symbole, gesucht, das im Text zu finden ist.\n",
    "  4. Dieses wird ins Vokabular aufgenommen und im Text durch ein einzelnes Symbol ersetzt.\n",
    "  5. Dieser Prozess wird nun wiederholt bei Schritt 3 (d.h. 2-Gramme aus dem ersten Durchlauf werden zu 4-Grammen im zweiten Durchlauf u.s.w.) bis die vorgegebene Länge des Vokabulars erreicht ist.\n",
    "\n",
    "Dabei können sowohl ganze Wörter ins Vokabular aufgenommen werden, wenn sie denn oft genug auftauchen (bespielweise werden die Worte \"a\", \"the\", \"and\" bei englischen Texte sicherlich mitaufgenommen werden), aber auch einzelne Wortteile wie z.B. \"en##\", \"##ment\" oder \"##ed\" werden in diesem Vokabular sicherlich vorkommen, um seltene Kombinationen wie \"enablement\" in die Wortteile \"en##\", \"able\" und \"##ment\" zerlegen zu können oder grammatikalische Formen wie \"wanted\" zu bilden. Die Zeichenfolge \"##\" beschreibt dabei, dass hier ein anderer Wortteil anschließen muss.\n",
    "\n",
    "In unserem Testbeispiel ist zu sehen, wie Ihre Eingabe in Tokens getrennt und dann in eine Kodierung umgewandelt wird, je nachdem, welche Position das Token in unserem Vokabular hat.\n",
    "Wie Sie sehen, enthält das Byte-Pair Encoding Vokabular auch ein [START]- und [END]-Token für Satzanfang und Satzende, sowie Elemente vom Typ 'abc##' oder '##abc'. Die Elemente mit Doppel-'#' stellen eine Sequenz am Anfang bzw. Ende eines Wortes dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60193c08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.516820100Z",
     "start_time": "2024-01-16T09:57:25.499286900Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_tok = widgets.Text(\n",
    "    value='Tokenizer test',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_tok = widgets.Button(description='Run tokenizer on input',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_tok = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def tokenize(input_widget_tok):\n",
    "    tokens = tokenizer.tokenize(input_widget_tok.value)                    # Erstellung der Tokens als Index für Vokabular\n",
    "    lookup = tokenizer.lookup(tokens)                                      # Abrufen der Zeichenkette des Index im Vokabular                    \n",
    "    \n",
    "    return tokens, lookup\n",
    "    \n",
    "def on_button_click(b):\n",
    "    with output_widget_tok:\n",
    "        output_widget_tok.clear_output()                                                        \n",
    "        tokens, lookup = tokenize(input_widget_tok)\n",
    "\n",
    "        VisualWrapper.display_text('Tokens die aus der Eingabe mit Byte-Pair Encoding extrahiert werden:'.rjust(100) + ', '.join([token.decode('utf-8').rjust(10) for token in lookup.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "        VisualWrapper.display_text('Ihre Positionsnummer im Alphabet des Byte-Pair Encoding Algorithmus:'.rjust(100) + ', '.join([str(token).rjust(10) for token in tokens.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "\n",
    "button_widget_tok.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_tok, button_widget_tok, output_widget_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5b84a",
   "metadata": {},
   "source": [
    "### <a id=\"embedding\"></a>Embedding\n",
    "\n",
    "\n",
    "Die Sequenzlänge des Tokenizers variiert in Abhängigkeit von der Länge der Eingabe sowie dem Byte-Pair-Encoding, da gleichlange Tokenfolgen unterschiedlich lang kodiert werden können. Für die Parallelisierung des Trainingsprozesses ist es für Transformer jedoch notwendig eine gleichbleibende Eingabelänge beizubehalten.\n",
    "\n",
    "Dies wird dadurch gelöst, dass Padding Tokens zum Einsatz kommen, also Tokens, die keine Information kodieren, sondern der Eingabe beigefügt werden, um die erforderliche Länge zu erreichen. Anschließend wird die Ausgabe des Tokenizers mithilfe einer trainierbaren Matrix in das notwendige Vektorformat gebracht.\n",
    "\n",
    "Das Embedding sorgt dafür, dass die Ausgabe des Tokenizer in einen Vektor der Modellgröße $d_{model}$ eingebettet wird. Das heißt jedes Token, das zuvor durch eine Zahl kodiert wurde, wird nun durch einen Vektor der Länge $d_{model}$ kodiert. \n",
    "Das Embedding ist Teil der vom Modell gelernten Parameter (siehe <a href=\"#fig:embedding\">Abbildung 3</a>) und somit nicht deterministisch gegeben. Welche Informationen aus der Ausgabe des Tokenizers wo gespeichert werden ist somit nicht nachvollziehbar.\n",
    "Vorstellen kann man sich aber, dass in jedem der $d_{model}$ Parametern einige der Informationen gespeichert werden, die für das jeweilige Token wichtig sind. Beispielsweise die Bedeutung des Tokens, seine grammatikalische Form, in welchem Kontext es benutzt wurde, steht es am Anfang eines Satzes oder am Ende, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ec3f9",
   "metadata": {},
   "source": [
    "<figure id=\"fig:embedding\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_embedding.jpg\" style=\"max-width: 25%; max-height: 150vh; height: auto;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "    <figcaption>Abbildung 3: Gewichte der Eingabepipeline</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b34b4d",
   "metadata": {},
   "source": [
    "Wie ein solches Embedding aussieht und wie es sich verändert, wenn man beispielsweise neue Teile an den Satz anfügt können Sie in der nachfolgenden Simulation ausprobieren. Der Eingabetext wird erst vom Tokenizer in Tokens umgewandelt und dann durch das Embedding in einen Tensor.\n",
    "\n",
    "An jeder Position (vertikal dargestellt) ist dann das Embedding des Tokens an dieser Stelle zu sehen (horizontal dargestellt). Die farbliche Kodierung stellt dabei das Zahlenspektrum dar, indem sich die Vektoreinträge bewegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a1d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.563139100Z",
     "start_time": "2024-01-16T09:57:25.524821600Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingExample():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "        self.input_widget = widgets.Text(\n",
    "            value = 'Einbettung Test',\n",
    "            description = 'Ihre Eingabe:',\n",
    "            continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "            layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "        )\n",
    "\n",
    "        self.button_widget = widgets.Button(description='Einbettung erstellen',\n",
    "                                    layout = widgets.Layout(width='auto'))\n",
    "\n",
    "        self.output_widget = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "        self.old_context = None\n",
    "\n",
    "    def create_tokenized_embeddings(self):\n",
    "        tokens = self.tokenizer.tokenize(self.input_widget.value)                                 # Tokenisierung der Eingabe\n",
    "        tokens_all = tokens[tf.newaxis, :, :]                                                     # Hinzufügen einer weiteren Dimension\n",
    "        input_without_eos = tokens[tf.newaxis, :, :-1]                                            # Auswahl der Tokens bis zum [END] Token\n",
    "        token_input = self.tokenizer.detokenize(tokens_all)                                       # Nur zur Ausgabe Zwecken\n",
    "        string_value = token_input.numpy()[0][0].decode('utf-8')                                  # Nur zur Ausgabe Zwecken\n",
    "        lookup = tokenizer.lookup(input_without_eos)                                              # Nur zur Ausgabe Zwecken\n",
    "        lookup = [item.decode('utf-8') for sublist in lookup.numpy()[0] for item in sublist]      # Nur zur Ausgabe Zwecken\n",
    "        print(\"Wörter: \", string_value)\n",
    "        print(\"Tokens: \", lookup)\n",
    "        context = model.model.enc_embed(input_without_eos)                                        # Erstellung des Kontext Embedding \n",
    "        VisualWrapper.display_text('So sieht die Einbettung der Eingabe aus.')\n",
    "        VisualWrapper.color_bar(context.to_tensor())\n",
    "        if self.old_context is not None:\n",
    "             padded_context, padded_old_context = self.pad_tensors(context, self.old_context)     # Erstellung des Padding Vektors der Eingaben\n",
    "             VisualWrapper.display_text('So unterscheiden sich die alte und die neue Einbettung voneinander.')\n",
    "             context_diff = padded_context - padded_old_context                                   # Berechnung der Unterschiede beider Vektoren\n",
    "             VisualWrapper.color_bar(context_diff)\n",
    "\n",
    "        self.old_context = context\n",
    "\n",
    "    \n",
    "    def on_button_click(self, b):\n",
    "        with self.output_widget:\n",
    "            self.output_widget.clear_output()  # clear the previous output\n",
    "            VisualWrapper.reset_visualiser()\n",
    "            self.create_tokenized_embeddings()\n",
    "    \n",
    "    def pad_tensors(self, ragged_tensor1, ragged_tensor2):\n",
    "        \"\"\"Funktion um die Tensoren der Eingabe auf die gleiche Länge zu transformieren\"\"\"\n",
    "        tensor1 = ragged_tensor1.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "        tensor2 = ragged_tensor2.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "\n",
    "        shape1 = tf.shape(tensor1)\n",
    "        shape2 = tf.shape(tensor2)\n",
    "\n",
    "        target_shape = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            target_shape.append(tf.maximum(shape1[i], shape2[i]))                                # Die maximale Größe der Dimension wird an die Zielform angehängt.\n",
    "\n",
    "        target_shape = tf.stack(target_shape)                                                    # Umwandlung der Zielform in einen Tensor\n",
    "\n",
    "\n",
    "        paddings1 = []\n",
    "        paddings2 = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            paddings1.append([0, target_shape[i] - shape1[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "            paddings2.append([0, target_shape[i] - shape2[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "\n",
    "        paddings1 = tf.stack(paddings1)                                                          # Konvertieren der Paddings in Tensoren\n",
    "        paddings2 = tf.stack(paddings2)                                                          # Konvertieren der Paddings in Tensoren\n",
    "\n",
    "        tensor1_padded = tf.pad(tensor1, paddings1)                                              # Tensoren an die Zielform anpassen\n",
    "        tensor2_padded = tf.pad(tensor2, paddings2)                                              # Tensoren an die Zielform anpassen\n",
    "\n",
    "        return tensor1_padded, tensor2_padded\n",
    "\n",
    "emb_ex = EmbeddingExample()\n",
    "\n",
    "VisualWrapper.display_text('Hier können Sie einen Text einbetten lassen. Wenn du die Eingabe veränderst wird außerdem gezeigt, wie sich die Einbettung geändert hat.')\n",
    "\n",
    "emb_ex.button_widget.on_click(emb_ex.on_button_click)\n",
    "display(emb_ex.input_widget, emb_ex.button_widget, emb_ex.output_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef7ac5-da74-40c0-be78-f0f866ca9fd9",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesen Beispiel ist zu sehen, wie die Werteverteilung eines Embeddings grafisch dargestellt werden kann.\n",
    "\n",
    "*Codeerläuterung:* Das Embedding wird in dem von uns implementierten [Code](https://github.com/LangLoffelLako/TF_simulator_tensorflow/blob/main/interactive_inference.ipynb) der Funktion *create_tokenized_embeddings()* erstellt. Dazu wird zu erst der Eingabetext vom Tokenizer in Tokens unterteilt (Zeile 20). Die Tokens können Sie über der Grafik sehen. In Zeile 29 werden diese dann vom Transformer Modell in die Embeddings umgewandelt. \\n\"\n",
    "\n",
    "Verwendet man zum Beispiel das Eingabebeispiel \"Einbettung Test\" und als zweites \"Einbettung Test neu\", sieht man im zweiten Bild wie die zusätzlichen Positionen dazu kommen. Die Visualisierung von \"Einbettung Test\" zeigt auf der y-Achse sieben Tokens. Bei dem Satz \"Einbettung Test neu\" kommen dann 3 Tokens dazu, diese sind \"#n\", \"##e\", \"##u \". Ebenso kann man ein Muster bei dem Wertebereich erkennen. Der Bereich der Tiefe zwischen 0bis 256 bewegt sich hauptsächlich im Wertebereich -1 bis 2 und der Bereich von 257 bis 512 im Wertebereich 0 bis -3. Dies geht auf die Sinus-Funktion des Positional Encodings zurück, welche auf das Embedding angewendet wird. Diese hat für diese Bereiche stark unterschiedliche Werte. \n",
    "\n",
    "Die Werte in den ersten 256 Positionen können bestimmte Merkmale oder Eigenschaften der Wörter repräsentieren, während der Bereich der Positionen von 266 bis 512 andere Merkmale oder Eigenschaften widerspiegelt. Diese getrennte Darstellung ermöglicht dem Modell, komplexe Beziehungen und Muster in den Daten zu erfassen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e70583",
   "metadata": {},
   "source": [
    "\n",
    "### Positional Encoding <a id=\"positional-encoding\"></a>\n",
    "\n",
    "Da im Embedding keine Informationen über die relative Position der verschiedenen Worte kodiert werden, muss diese manuell hinzugefügt werden. Hierfür verwendet die Transformerarchitektur für jede Position des Embeddings (also jedes enkodierte Token) eine veränderte Sinuskurve. Es ändern sich die Frequenz, also die Abstände der Nulldurchgänge, sowie die Phase, also die x-Werte der Nulldurchgänge. [1] Der sich ergebende Wert wird dem Embedding an der jeweiligen Stelle hinzugefügt.\n",
    "\n",
    "Dadurch lassen sich die verschiedenen Worte sehr gut voneinander trennen. Die Idee dahinter ist, dass:\n",
    "\n",
    "die grobe Position eines Wortes anhand der langfrequenten Sinuskurven bestimmt werden kann, da sie sich über die gesamte Länge der Eingabe nur allmählich verändern und die Werte des Embeddings insgesamt in eine bestimmte Richtung verschieben. Beispielsweise besitzen die Worte im hinteren Teil der Eingabe größere Werte als die im vorderen Teil der Eingabe. Dies ist in der untenstehenden Simulation an großflächigen Rot- und Grünverschiebungen zu erkennen.\n",
    "die genaue Position durch die hochfrequenten Sinuskurven bestimmt werden kann, da diese sich bereits für benachbarte Vektoren klar unterscheiden. Dadurch wird deutlich, welches Wort an welcher Stelle im Embedding kodiert wurde. Dies entspricht den sehr chaotisch wirkenden Bereichen in der untenstehenden Simulation.\n",
    "Die für diese Verschiebungen verwendeten Formeln sind deterministisch für Position und Tiefe des Embeddings festgelegt und lauten:\n",
    "\n",
    "<a id=\"pos-enc-formula\"></a>\n",
    "\n",
    "$$ PE(\\text{pos}, i) = \\begin{cases} \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), & \\text{falls } i \\text{ gerade ist} \\\\ \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), & \\text{falls } i \\text{ ungerade ist}, \\end{cases} $$\n",
    "\n",
    "wobei\n",
    "\n",
    "$\\text{pos}$ die Position des Tokens in der Sequenz ist,\n",
    "$i$ der Index der Dimension in der Positionsverschlüsselung ist,\n",
    "$d_{\\text{model}}$ die Dimensionalität des Modells ist.\n",
    "\n",
    "In der untenstehenden Simulation ist zu sehen, wie das Positional Encoding beispielhaft für ein 1024 x 257 langes und tiefes Embedding aussieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d875186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.823595Z",
     "start_time": "2024-01-16T09:57:25.551970Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact(length=(2,2048,1), depth=(2,512,1))\n",
    "def print_pos_enc(length, depth):\n",
    "    VisualWrapper.color_bar(positional_encoding(length, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb626ffc-d9b6-4181-92d3-1bf3618660b4",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "An diesem Beispiel sieht man den Effekt des Positional Encodings auf verschiedene lange bzw. tiefe Einbettungen. Wenn man mit den zwei Reglern spielt, sieht man, verschieden große Zooms auf den Effekt. Erhöht man den \"length\"-Regler sieht man die Auswirkungen, welche die Länge des Satzes betreffen. Die Werte die durch das Positional Encoding zu dem Vektor hinzugefügt werden sind dabei jedoch immer dieselben. Zum Bespiel wird an Stelle 100 immer derselbe Wert hinzugefügt, egal ob die maximale Länge des Satzes (\"length\") sich verändert. Dasselbe gilt auch für die Tiefe jedes einzelnen Vektors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389fa0d1",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"trainingsmethoden\"></a>Trainingsmethoden\n",
    "\n",
    "### <a id=\"dropout\"></a>Dropout\n",
    "\n",
    "Dropout [11] ist eine Methode, die während des Trainingsprozesses eines neuronalen Netzes genutzt wird, um zu verhindern, dass die gelernte Ausgabe eines Modells sich zu sehr auf einen einzelnen Prädikator stützt.\n",
    "Dafür wird zwischen zwei Schritten desselben Modells, eine Dropout-Layer eingefügt. Diese setzt zufällig einige der vom ersten Modellteil generierten Ausgabe auf einen vordefinierten Wert (meistens -inf), um den nachfolgenden Schichten diese Information vorzuenthalten. Da diese Operation zufällig erfolgt, müssen die nachfolgenden Teile des Modells lernen ihre Ausgabe auch ohne diese Information zu erstellen. Somit lernt das Model seine Vorhersage auf eine möglichst breite Kombination an Merkmalen aufzubauen und man verhindert, dass Vorhersagen nur aufgrund eines einzigen Merkmals der vorherigen Ausgabe gemacht werden.\n",
    "\n",
    "Ein gutes Beispiel ist das Ende eines Satzes vorherzusagen. In europäischen Sprachen wird ein Satz fast immer mit einem Punkt beendet, also ist es eine gute Strategie zu lernen, dass ein Satz durch einen Punkt beendet wird. Doch ein Modell, dass einen Punkt als einziges Merkmal eines Satzendes nutzt ist wenig robust. Wenn man an falscher Stelle einen Punkt setzt oder ihn an einem Satzende durch ein anderes Zeichen ersetzt werden die Vorhersagen des Models schlecht sein. Dabei gibt es auch andere Hinweise auf ein Satzende, z.B. das Vorkommen eines Verbs in der deutschen Sprache oder von Ort und Zeitangaben im Englischen.\n",
    "\n",
    "Um dem Modell keine Informationen vorzuenthalten, wenn es tatsächlich eingesetzt wird, ist das Dropout immer nur während des Trainings aktiv und wird danach abgeschalten, sodass während der Inferenzphase keine Informationen gelöscht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b358029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.618429300Z",
     "start_time": "2024-01-16T09:57:25.825653400Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_drop = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_drop = widgets.Text(value = 'Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test',\n",
    "                                 description = 'Ihre Eingabe:',\n",
    "                                 continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                                 layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "                                 )\n",
    "\n",
    "length_widget_drop = widgets.IntSlider(value=30,\n",
    "                           min=2,\n",
    "                           max=2048,\n",
    "                           description='Länge des Tensors:',\n",
    "                           continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                           )\n",
    "depth_widget_drop = widgets.IntSlider(value=512,\n",
    "                          min=2,\n",
    "                          max=512,\n",
    "                          description='Tiefe des Tensors:',\n",
    "                          continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                          )\n",
    "dropout_widget = widgets.FloatSlider(value=0.1,\n",
    "                              min=0,\n",
    "                              max=0.9,\n",
    "                              step=0.1,\n",
    "                              description='Dropoutrate:',\n",
    "                              continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "def dropout_function(length, depth, dropout, input):\n",
    "\n",
    "    # Erstellung der Dropout Layer\n",
    "    dropout_layer = layers.Dropout(dropout)                                                         # Anwendung des Dropout auf die Layer\n",
    "    one_tensor = tf.ones([length, depth])                                                           # Erstellung eines Arrays aus 1-en\n",
    "    dropout_tensor = dropout_layer(one_tensor, training=True)                                       # Anwendung des Dropout auf die Layer\n",
    "\n",
    "    # Erstellung der Kontext Layer\n",
    "    tokens = tokenizer_drop.tokenize(input)                                                         # Tokenisierung des Inputs\n",
    "    input_without_eos = tokens[tf.newaxis, :, 1:-1]                                                 # Auswahl der Tokens bis zum [END] Token\n",
    "    context = model.model.enc_embed(input_without_eos)                                              # Erstellung des Embedding durch das Modell\n",
    "    context_drop = dropout_layer(context, training=True)                                            # Anwendung des Dropout auf das Embedding\n",
    "\n",
    "    return dropout_tensor, context_drop, context\n",
    "\n",
    "def out(length, depth, dropout, input):\n",
    "    VisualWrapper.reset_visualiser()                                                   \n",
    "    dropout_tensor, context_drop, context = dropout_function(length, depth, dropout, input)\n",
    "    VisualWrapper.color_bar(dropout_tensor)\n",
    "                               \n",
    "    VisualWrapper.color_bar(context.to_tensor())                     \n",
    "    VisualWrapper.color_bar(context_drop.to_tensor())\n",
    "    \n",
    "\n",
    "output_widget_dropout = widgets.interactive_output(out,\n",
    "                                                   {'length': length_widget_drop, 'depth': depth_widget_drop, 'dropout': dropout_widget, 'input': input_widget_drop}, \n",
    "                                                   )\n",
    "\n",
    "display(length_widget_drop, depth_widget_drop, dropout_widget, input_widget_drop, output_widget_dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d176e24-7e39-4e92-95d5-3d24eef076ec",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesem Beispiel wird der Effekt von Dropout auf die Layer und das Embedding dargestellt.\n",
    "\n",
    "*Codeerläuterung:* Dafür werden im von uns implementierten [Code](https://github.com/LangLoffelLako/TF_simulator_tensorflow/blob/main/interactive_inference.ipynb) in der Funktion *dropout_function()* jeweils Dropout auf die Layer und auf das Embedding angewendet. Dafür wird in Zeile 36 und 42 jeweils die jeweilige Transformation auf den beiden Objekten angewendet.\n",
    "\n",
    "Damit wird also ein gewisser Teil, welche mit dem Parameter \"Dropoutrate\" bestimmt wird, der Werte Layer bzw. des Embeddings den weiteren Verarbeitungsschritten vorenthalten. Dieser Parameter ist ein prozentualer Wert, d.h. bei einem Wert von 0.2 werden 20% der Werte vorenthalten. \n",
    "Für das Beispiel können dieses Mal die Länge des Tensors (Eingabe) und die Tiefe des Tensors bestimmt werden. Ebenso kann die Dropoutrate verändert werden.\n",
    "\n",
    "In der ersten Grafik sieht man welche Werte in einem uniformen Vektor vom Dropout verändert werden. In den beiden darauffolgenden Grafiken wird das Dropout auf den im Textfeld eingegebenen Beispieltext angewandt, nachdem er durch den Tokenizer und ein Embedding in Vektorform gebracht wurde. Die erste Grafik zeigt den vollständigen Vektor und die zweite Grafik den Vektor, der vom Dropout verändert wurde.\n",
    "Hier sieht man die ausgelassenen Positionen sehr gut. Mit dem Erhöhen der Dropout Rate, werden diese mehr. Außerdem kann man erkennen, dass sich, wenn man den Dropout erhöht, der Wertebereich ebenfalls ausweitet. Dies geschieht, da in der Tensorflow-Implementation die durch das Dropout unveränderten Werte mit $1 / (1-\\text{Dropoutrate})$ skaliert werden, um die Summe aller Werte konstant zu halten.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d169f9",
   "metadata": {},
   "source": [
    "### <a id=\"normalization\"></a>Normalisierung\n",
    "\n",
    "Normalisierung ist eine Technik, die von [20] eingeführt wurde. In Deep Neural Networks, die mit nicht-linearen Aktivierungsfunktionen wie der Sigmoid-Funktion\n",
    "\n",
    "$$g(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "trainiert werden gilt, dass $g'(x) \\rightarrow 0$ für $|x| \\rightarrow \\infty$. \n",
    "\n",
    "Das führt dazu, dass diese Modelle in einen Bereich geraten können, in dem $g'(x)$ sehr klein wird. Dadurch wird auch das Training durch Stochastic Gradient Descent (SGD) minimal, sodass das Training des Modells stagniert. Man spricht vom Vanishing Gradient Problem.\n",
    "\n",
    "In neuronalen Netzen ist hierbei das Problem, dass die tieferen Layer des Modells, z.B. eine Layer $z = g(Wx + b)$ mit der Sigmoid-Funktion g versucht mithilfe seiner trainierbaren Werte $W$ und $b$ den Output des gesamten vorherigen Netzes zu gewichten. Dabei werden sowohl $W$ und $b$ abhängig von vorherigen Werten $x$ trainiert und hängen somit selbst auch von $x$ ab. Da sich während des Trainings alle Layers des Netzes fortwährend aktualisieren, ändert sich auch der Input $x$ fortwährend, sodass ein Training späterer Schichten erst möglich ist, wenn die vorhergehenden sich weitgehend stabilisiert haben. \n",
    "Dieser Effekt wird von [20] Internal Covariate Shift genannt. \n",
    "\n",
    "Je tiefer das neuronale Netz, umso größer sind diese Veränderungen, da es mehr Schichten gibt, die sich verändern können. Die Tiefe einer neuronalen Netzes erhöht also die Wahrscheinlichkeit das ein Vanishing Gradient Problem auftritt und ein effektives Training frühzeitig aufhört.\n",
    "\n",
    "Transformer wie sie in [1] beschrieben sind nutzen um diesem Problem entgegenzuwirken Layer Normalization, einen Normalisierungsalgorithmus den [21] entwickelt hat. Eine Normalisierung führt dazu, dass zumindest der Wertebereich indem sich der Input $x$ aufhält während des gesamten Trainings stabil bleibt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9d2b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.709361500Z",
     "start_time": "2024-01-16T09:57:26.620436400Z"
    }
   },
   "outputs": [],
   "source": [
    "VisualWrapper.reset_visualiser()\n",
    "tokenizer_norm = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input=\"Test\"\n",
    "\n",
    "tensor_input = tf.convert_to_tensor(input)\n",
    "if len(tensor_input.shape) == 0:\n",
    "    tensor_input = tensor_input[tf.newaxis]\n",
    "\n",
    "tokenized_input = tokenizer_norm.tokenize(tensor_input).to_tensor()                             # Anwendung eines Tokenizers mit Normalisierung\n",
    "input_without_eos = tokenized_input[:, :-1]\n",
    "context = model.model.encode(input_without_eos, None)\n",
    "\n",
    "VisualWrapper.visualize_data(id='layer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57791aa1",
   "metadata": {},
   "source": [
    "### <a id=\"residual-connection\"></a> Residual Connection\n",
    "\n",
    "Die Idee für das Nutzen von Residual Connections kommt von [22]. Die Autoren stellten fest, dass bei neuronalen Netzen sowohl die Genauigkeit während des Trainings als auch die Genauigkeit auf dem Testdatensatz mit zunehmender Tiefe schlechter wird.\n",
    "\n",
    "Da durch Normalization bereits sichergestellt ist, dass das Vanishing Gradient Problem nicht auftritt, scheitert die Optimierung der neuronalen Netze aus anderen Gründen.\n",
    "Einer der Gründe hierfür liegt vermutlich darin, dass die tieferen Schichten eines Modells zu Beginn des Trainings sehr viel stärker zur Ausgabe beitragen, als die vorhergehenden Schichten. Sie werden somit zuerst trainiert. Die weniger tiefen Schichten werden erst ausreichend trainiert, wenn in den tiefen Schichten keine Optimierung mehr möglich ist. \n",
    "\n",
    "Um dafür zu sorgen, dass direkt mit Beginn des Trainings alle Teile des Modells gleichmäßig trainiert werden bieten sich Residual Connections an.\n",
    "Sie ersetzen eine Schicht F(x) durch\n",
    "\n",
    "$$H(x) = F(x) + x.$$\n",
    "\n",
    "In das Ergebnis von H(x) geht also sowohl der Output, als auch der Input von F direkt mit ein. Wendet man dieses Prinzip auf die Schichten tiefer neuronaler Netze an, sorgt das dafür, dass gleich zu Beginn der Output der wenig tiefen Schichten relevant in den Output des gesamten Netzes einfließt, denn es gilt für das gesamte Netz $N$:\n",
    "\n",
    "$$N(x) = H_n(H_{n-1}(x)) + H_{n-1}(x) = H_n(H_{n-1}(x)) + H_{n-1}(H_{n-2}(x)) + … + H_2(H_1(x)) + H_1(x)$$\n",
    "\n",
    "Wie man in <a href=\"#fig:fig4\">Abbildung 4</a> sehen kann, haben alle Attention-Module sowie alle Feed Forward Layer in einem Transformermodell eine residuale Verbindung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO:\n",
    "# Hier fehlt noch die Implementierung der Residual Connection als Simulation.\n",
    "# In der nachfolgenden Simulation können Sie sehen, wie sich die Ausgabe einer neuronalen Schicht verändert, wenn man ihr eine Residual Connection beifügt. Der Effekt auf den gesamten Trainingsprozess lässt sich dabei natürlich nur schwer darstellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf4718db1a7166",
   "metadata": {},
   "source": [
    "## <a id=\"layers\"></a> Layers\n",
    "\n",
    "Der größte Teil der Verarbeitung findet in den Transformer Layer statt. Diese werden in <a href=\"fig:fig4\">Abbildung 4</a> detailliert dargestellt. Innerhalb der Transformer kann man die Attention Layer, einige dazwischen liegende Schritte und zuletzt eine Feed Forward Layer unterscheiden.\n",
    "\n",
    "#### Erklärung der Grafik\n",
    "\n",
    "In unserer Grafik werden alle Elemente in Datenelemente (blau) z.B. der Source und Target Input, deterministische Prozesse (schwarz) z.B. die Matrix Multiplikation verschiedener Matrizen, Prozesse mit trainierbaren Parametern (gelb) z.B. die Normailsierung des Output und Prozesse mit Hyperpararmetern (grau), z.B. Dropout unterschieden.\n",
    "Generell durchlaufen die Daten dabei unsere Grafik entlang der Pfeile von den beiden Input-Optionen am unteren Ende, die übrigens auch äquivalent sein können (siehe gestrichelter Pfeil mit Gleichheitszeichen), zum Output am oberen Ende der Grafik.\n",
    "Gestrichelte Pfeile sind dabei nur in manchen der <a href=\"attention-mechanismen\">verschiedenen Attention-Mechanismen</a> vorhanden, wie weiter unten erläutert wird (siehe Link). So ist zum Beispiel das Masking optional.\n",
    "Zu sehen ist in der Grafik, wie der Source und Target Input parallel verarbeitet als Query, Key und Value verarbeitet wird. Query und Key gemeinsam werden dann gegebenenfalls mit einer Mask versehen, bevor sie wieder mit dem Value zusammengeführt werden. Die Ergebnisse der n verschiedenen Attention-Köpfen (n ist dabei ein Hyperparameter des Modells) werden verkettet und zu einem Vektorembedding zusammengeführt. Dieses wird mit dem residualen Target Embedding addiert und so als Eingabe in die Feed Forward Layer gegeben. Diese berechnet den Ouput der Transformer Layer und als Kombination aus Output und residualem Input wird diese ausgegeben.\n",
    "\n",
    "Wie diese Mechanismen im Detail funktionieren wird im folgenden Kapitel geklärt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e0f5d",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig4\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_layer_architecture.jpg\" style=\"max-width: 100%; max-height: 175vh; height: auto; margin: auto;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "    <figcaption>\n",
    "      Abbildung 4: Transformerarchitektur <br>\n",
    "      In dieser Abbildung wird der Aufbau einer Attention Layer gezeigt. Dabei werden die verschiedenen Varianten (Self-, Cross-, Masked-Attention) parallel dargestellt (siehe gpunktierte Linien als Alternativen). Die beiden Input Embeddings werden von mehreren Attention-Köpfen parallel verarbeitet, um dann gemeinsam mit dem Target Embedding addiert von einer Feed Forward Layer zum Output Embedding der Attention Layer transformiert zu werden.\n",
    "    </figcaption>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c08fb1",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"attention\"></a> Attention\n",
    "\n",
    "Die Neuerung von Transformern im Vergleich zu vorangegangenen Lösungen für Neural Machine Translation (NMT) ist es, allein auf Attention als Mechanismus für das Verarbeiten von Sprache zu setzen. Attention wurde auch vorher schon von [15] zur Verbesserung von RNNs zur Übersetzung von Texten verwendet.\n",
    "\n",
    "Der Attention-Mechanismus, wie ihn [1] beschreiben, orientiert sich dabei an der Idee einer Suchanfrage des Ausgabetextes and sich selbst, bzw. den Eingabetext. Die Attention-Layer bekommt dabei zwei oder eigentlich drei Eingaben: \n",
    "\n",
    "1. den Query ($Q$), \n",
    "2. den Key ($K$),\n",
    "3. den Value ($V$).\n",
    "\n",
    "In der Praxis erhalten aber Key und Value in Transformern immer dieselbe Eingabe und häufig sind Query, Key und Value sogar alle identisch. Aus welcher Quelle Query, Key und Value kommen unterscheidet verschiedene Formen von Attention. So nennen wir Self-Attention denjenigen Fall indem $Q=K=V$ gilt und Cross-Attention denjenigen Fall indem der Query aus der Ausgabe des Encoder besteht und Key und Value beide aus der Ausgabe eines Decoder-Blocks stammen (siehe <a href=\"fig:fig4\">Abbildung 4</a>).\n",
    "\n",
    "Um zu erklären, wie Attention funktioniert, sollten wir aber zunächst davon ausgehen, dass Query-, Key- und Value-Eingabe verschieden sind. Ich schreibe bewusst von der Eingabe, da in jeder Attention-Layer zunächst eine Eingabe $Q'$, $K'$, $V'$ mit Hilfe von gewichteten Matrix $W^Q$, $W^K$ und $W^V$ in Query, Key und Value \n",
    "\n",
    "$$Q=Q′ \\times W^Q$$\n",
    "$$K=K′ \\times W^K$$\n",
    "$$V=V′ \\times W^V$$ \n",
    "\n",
    "umgewandelt werden. Die gewichteten Matrizen $W^Q$, $W^K$, $W^V$ sind die trainierbaren Weights der Attention-Layer. Alle nachfolgenden Prozesse sind deterministischer Natur. Das bedeutet, dass diese Matrizen festlegen, welches Ergebnis die Attention-Layer liefert.\n",
    "Eine gute grafisch aufbereitete Erklärung dessen, was hier beschrieben wird findet sich übrigens bei [9].\n",
    "\n",
    "Attention unterscheidet sich deutlich von vorherigen Verfahren. Am ehsten ist es vergleichbar mit der oftmals bei der Programmierung verwendeten Datenstruktur \"Dictionary\". Ein Dictionary ist eine unsortierte Liste mit \"Key-Value\" Paaren. Hierbei lässt sich ein Wert, Objekt, Variable (Value) in der Liste speichern, welcher wiederum über den Key abgefragt werden kann. Folgende Grafik zeigt die Gemeinsamkeiten und Unterschiede zu dem Attention-Mechanismus.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da880e3c13929c09",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig_attention\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_attention.png\" style=\"max-width: 100%; max-height: 150vh; height: auto;\" alt=\"Abbildung 4: Vergleich von Attention mit Datenstruktur 'Dictionary'.\"/>\n",
    "    <figcaption>Abbildung 5: Vergleich von Attention mit einem klassischen Dictionary</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47d379a5e3fcd5",
   "metadata": {},
   "source": [
    "#### Erklärung der Grafik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f91d9b-05cc-4af9-a709-a2d3dbcf785a",
   "metadata": {},
   "source": [
    "\n",
    "In beiden dargestellten Szenarien, \"Dictionary\" und \"Attention\", wird das Konzept von Key-Value-Paaren genutzt, wobei die Keys als Referenzindizes fungieren und die Values die eigentlichen zu verarbeitenden Informationen enthalten. Queries dienen in beiden Fällen dazu, relevante Daten aus diesen Paaren zu selektieren, und am Ende wird jeweils ein Output generiert, der aus den Informationen der Value-Komponenten resultiert.\n",
    "\n",
    "Jedoch gibt es markante Unterschiede zwischen den beiden Ansätzen. Im Dictionary findet eine direkte Zuordnung statt, bei der ein Query-Element einem Key zugeordnet und das zugehörige Value direkt als Output übernommen wird. Bei \"Attention\" wird hingegen eine gewichtete Kombination der Values vorgenommen, die von der Relevanz der Keys, bestimmt durch die Queries, abhängt. Dies spiegelt sich auch in der Art der Beziehungen wider: Während im \"Dictionary\" eine eindeutige 1:1-Beziehung herrscht, besteht im \"Attention\"-Mechanismus eine 1:n-Beziehung, bei der ein Query mehrere Keys beeinflusst. Dementsprechend ist die Ausgabe im \"Dictionary\" statisch und hängt ausschließlich von der direkten Übereinstimmung ab, während sie im \"Attention\"-Modell dynamisch ist und durch die berechneten Gewichtungen eine nuanciertere Informationszusammenstellung ermöglicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f0686",
   "metadata": {},
   "source": [
    "#### <a id=\"vorteile\"></a>Vorteile von Transformern\n",
    "\n",
    "Die Einführung des Attention-Mechanismus in Transformer-Architekturen hat zu bedeutenden Verbesserungen in der Verarbeitung natürlicher Sprache geführt, insbesondere im Vergleich zu traditionellen rekurrenten neuronalen Netzwerken (RNNs). Einer der herausragenden Vorteile des Attention-Mechanismus ist seine Fähigkeit zur parallelen Verarbeitung von Daten. Im Gegensatz zu den sequenziellen Verarbeitungsgrenzen von RNNs ermöglicht diese Eigenschaft eine wesentlich effizientere Datenverarbeitung. Während für eine sequenzielle Verarbeitung die Komplexität von der Textlänge $n$ exponentiell abhängt $\\exp(n)$ gilt für die parallele Verarbeitung wie in Transformern nur eine lineare Abhängigkeit $a \\times n$. Das ist eine dramtische Verbesserung.\n",
    "\n",
    "Ein weiterer entscheidender Fortschritt, den der Attention-Mechanismus mit sich bringt, ist dass er bei der Verarbeitung der Daten an Position $n$ uneingeschränkt auf alle vorherige $n-1$ Daten zugreifen kann. Im Unterschied zu dem festen, oft begrenzten Gedächtnis der RNNs, das sich Daten von Position $1$ bis zur verarbeitung an Position $n$ bereits $n-1 \\text{-mal}$ merken musste, erlaubt der dynamische und kontextabhängige Speicher des Attention-Mechanismus eine umfassendere und flexiblere Berücksichtigung von Informationen. Dies ist besonders nützlich für das Verständnis und die Verarbeitung komplexer Sprachstrukturen.\n",
    "\n",
    "Besonders bemerkenswert ist auch, wie der Attention-Mechanismus die Handhabung von Langzeitabhängigkeiten verbessert. Durch die Fähigkeit, direkte Verbindungen zwischen weit auseinanderliegenden Elementen einer Sequenz herzustellen, können Transformer-Modelle effektiver mit Langzeitabhängigkeiten umgehen, was bei RNNs oft eine Herausforderung darstellt. Diese Fähigkeit verbessert das Verständnis und die Generierung von Sprache über längere Textabschnitte hinweg erheblich.\n",
    "\n",
    "Schließlich ermöglicht der Attention-Mechanismus eine verbesserte Kontextverarbeitung. Die Fähigkeit, die Bedeutung von Wörtern und Phrasen im Kontext ihres Auftretens zu erfassen, führt zu einem präziseren und tieferen Verständnis der Sprache. Diese kontextuelle Bewusstheit, die über die Fähigkeiten traditioneller RNNs hinausgeht, ist entscheidend für anspruchsvolle sprachverarbeitende Aufgaben wie z.B. Übersetzung oder Zusammenfassungen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40a6df",
   "metadata": {},
   "source": [
    "#### <a id=\"attention-function\"></a> Attention als Funktion\n",
    "\n",
    "Die Funktion, die die Attention für uns berechnet, bekommt die Eingaben $Q$, $K$, $V$, also Query, Key und Value, die aus der Multiplikation der Eingabe mit den Gewichtsmatrizen entstanden sind. Sie lautet:\n",
    "\n",
    "$$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{\\text{model}}}}\\right)V$$\n",
    "\n",
    "Es wird also zuerst das Kreuzprodukt aus $Q$ und $K$ gebildet. Dieses Produkt wird mit $\\sqrt{d_{\\text{model}}}$ skaliert (den Grund dafür findest du im folgenden [Kapitel](#skalierung-mit-sqrd_k)), und auf dieses Ergebnis wird dann die Softmax-Funktion $\\sigma(x)$ angewandt.\n",
    "Diese Funktion lässt sich am besten positionsweise beschreiben:\n",
    "\n",
    "$$\\sigma(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^n \\exp(x_j)} \\text{ für } i=1, \\dots, n.$$\n",
    "\n",
    "Nennen wir also\n",
    "\n",
    "$$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{model}}}\\right) = \\text{Score}(Q,K),$$\n",
    "\n",
    "dann ist\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{Score}(Q,K) \\times V$$\n",
    "\n",
    "eine Funktion, die $V$ mit einem Vektor multipliziert, wobei für den Vektor gilt $|\\text{Score}(Q,K)| = 1$.\n",
    "\n",
    "$\\text{Score}(Q,K)$ ist also ein Vektor exakt der Länge von $V$, der angibt, mit welchem Anteil jeder Eintrag von $V$ in den Ausgabevektor $\\text{Attention}(Q,K,V)$ eingehen soll. Dabei summiert sich $\\text{Score}(Q,K)$ zu $1$, es handelt sich also tatsächlich um eine Gewichtung der Einträge von $V$.\n",
    "\n",
    "Da mit $\\text{Score}(Q,K)$ nun also eine Gewichtung besteht, wie stark die Ausgabe $\\text{Attention}(Q,K,V)_i$ von $V_j$ abhängt, kann man dieses Verhältnis als Matrix grafisch darstellen. Dies geschieht in der Simulation unten, bei der für den vorgegebenen Satz eine Gewichtung aus einer der Attention-Layer dargestellt wird. Der Eintrag in der $i$-ten Zeile und $j$-ten Spalte gibt dabei den Einfluss von $V_j$ auf $\\text{Attention}(Q,K,V)_i$ an.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffd9c1",
   "metadata": {},
   "source": [
    "##### <a id=\"skalierung-mit-sqrd_k\"></a> Skalierung mit $\\sqrt{d_{model}}$\n",
    "\n",
    "Zuletzt sollte noch kurz erklärt werden, weshalb innerhalb der Funktion $\\text{Attention}$ mit $\\sqrt{d_{model}}$ skaliert wird. Das Problem des Vanishing Gradients kann auch innerhalb der $\\text{Attention}$-Funktion auftreten, da hier $\\text{softmax}(QK^T)V$ berechnet wird und das Skalarprodukt $QK^T$ mit $d_{model}$ skaliert. Somit gilt, dass die Softmax-Funktion, definiert durch\n",
    "    \n",
    "$$\\sigma(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^N \\exp(z_j)} \\text{ für } j = 1, \\dots, N$$\n",
    "\n",
    "leicht in einen saturierten Bereich mit extrem kleinen Gradienten gerät. Deshalb wird in der Umsetzung der Architektur $QK^T$ mit $sqr(d_{model})$ skaliert: \n",
    "\n",
    "$$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{model}}}\\right),$$\n",
    "\n",
    "und so die Skalierung der Attention mit $d_{model}$ minimiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afaf5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.743517300Z",
     "start_time": "2024-01-16T09:57:26.711359600Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_attn = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "attn_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_attn = widgets.Text(\n",
    "    value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_attn = widgets.Button(description='Embedding berechnen',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_attn = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def create_tokenized_embeddings():\n",
    "        tensor_input = tf.convert_to_tensor(input_widget_attn.value)                # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)    # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "        if len(tensor_input.shape) == 0:                                            # Überprüft, ob der Eingabetensor im korrekten Format ist                                     \n",
    "            tensor_input = tensor_input[tf.newaxis]                                 # Falls nicht, wird eine Dimension hinzufügt \n",
    "\n",
    "    \n",
    "        tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()              # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "        input_without_eos = tokenized_input[:, :-1]\n",
    "        context = transformer.encode(input_without_eos, None)                       # Erstellung der Kontext-Vektoren vom Transformer-Modell\n",
    "\n",
    "        # Write the input tokens (excluding the last one) to the output array\n",
    "        for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "            output_array = output_array.write(i, value)\n",
    "\n",
    "        dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "        dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_attn:\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 6\n",
    "        #output_widget_attn.clear_output()  # clear the previous output\n",
    "        create_tokenized_embeddings()\n",
    "        VisualWrapper.visualize_data(id='attention')\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 1\n",
    "            \n",
    "\n",
    "button_widget_attn.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_attn, button_widget_attn, output_widget_attn)\n",
    "\n",
    "# TODO: In diesem Codeblock müssen noch einige Anpassungen am Text geschehen.\n",
    "# TODO: Die Aufmerksamkeitsmatrizen sind momentan 2x2 Matrizen. Hier gibt es einen Bug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca50aa-8c8c-49e4-955b-3c0791913f5d",
   "metadata": {},
   "source": [
    "#### <a id=\"multi-headed-attention\"></a> Multi-headed Attention\n",
    "\n",
    "Multi-headed Attention ist eine Erweiterung des Attention-Mechanismus, die darauf abzielt, die komplexen Strukturen von Texten besser zu erfassen. In herkömmlichen Modellen konkurrieren zahlreiche Beziehungen und Verbindungen innerhalb eines Textes um die Aufmerksamkeit eines einzigen Mechanismus, was zu einer Überlastung führen kann. Durch die Einführung von Multi-headed Attention wird diese Einschränkung überwunden, indem mehrere, parallel arbeitende Attention-Ströme geschaffen werden, von denen sich jeder auf unterschiedliche Aspekte des Textes konzentriert.\n",
    "\n",
    "Diese spezialisierten \"Köpfe\" können verschiedene Typen von Zusammenhängen innerhalb der Eingabedaten verarbeiten. Ein Kopf könnte sich auf die Beziehung zwischen Subjekten und Prädikaten konzentrieren, ein anderer auf die Kohärenz thematischer Elemente, und ein dritter könnte die Verbindung zwischen Haupt- und Nebensätzen analysieren. Ob das so passiert kann natürlich nicht nachgewiesen werden. Analysiert man allerdings die Aktivierungsmatrix der verschiedenen Köpfe, so kann man klare Unterschiede feststellen, sodass eine Spezialisierung anzunehmen ist. Durch diese Aufteilung wird vermieden, dass die Köpfe in Konkurrenz zueinander treten; stattdessen ergänzen sie sich, was zu einer umfassenderen Analyse führt.\n",
    "\n",
    "Die resultierenden, von jedem Kopf generierten Outputs werden anschließend zu einer einzigen, zusammenhängenden Darstellung kombiniert. Diese Synthese bietet eine reichhaltige, vielschichtige Perspektive auf die Eingabedaten, die weit über das hinausgeht, was mit einem einzigen Attention-Mechanismus möglich wäre. Multi-headed Attention ist somit ein Schlüsselelement, das die Fähigkeit von Modellen verbessert, subtile und komplexe Muster in Daten zu erkennen und darauf zu reagieren.\\\n",
    "\n",
    "Mathematisch betrachtet werden dazu zu Beginn $h$ gewichtete Matrizen \n",
    "\n",
    "$$W_i^Q, W_i^K, W_i^V \\quad i = 1, \\ldots, h$$ \n",
    "\n",
    "eingeführt. Diese erzeugen also $h$ verschiedene Matrixtripel \n",
    "\n",
    "$$Q_i = Q W_i^Q, \\quad K_i = K W_i^K, \\quad V_i = V W_i^V \\quad i = 1, \\ldots, h$$\n",
    "\n",
    "und somit ergeben sich $h$ verschiedene Ausgaben \n",
    "\n",
    "$$H_i = \\text{Attention}(Q_i, K_i, V_i) \\quad i = 1, \\ldots, h.$$\n",
    "\n",
    "Diese werden nun zu einer einzigen Ausgabe zusammengeführt, indem wir \n",
    "\n",
    "$$\\text{MultiHeadAttention}(Q,K,V) = \\text{Concat}(H_1, \\ldots, H_h) W^O$$\n",
    "\n",
    "berechnen. Dabei ist $\\text{Concat}(A_1, \\ldots, A_n)$ das Hintereinanderschreiben mehrerer Matrizen und $W^O$ eine weitere trainierbare Matrix, die die verschiedenen Ausgaben $H_1, \\ldots, H_h$ gewichtet. Deshalb sehen wir in der obigen Ausgabe auch mehrere Matrizen, die $\\text{Score}(Q_i, K_i)$ darstellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb503571-d491-4561-9e54-f245685c66fc",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig_mhattention\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_mhattention.png\" style=\"max-width: 100%; max-height: 150vh; height: auto;\" alt=\"Abbildung x: Beispiel Multi-headed Attention.\"/>\n",
    "    <figcaption>Abbildung 6: Beispiel Multi-headed Attention</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96638e79-6c3f-4bdb-8ecd-34a293a64358",
   "metadata": {},
   "source": [
    "### Erklärung der Grafik\n",
    "\n",
    "Die Abbildung zeigt zwei Beispiele für die Visualisierung von Multi-headed Attention in einem Transformer-Modell. Jedes Diagramm repräsentiert die Aufmerksamkeitsverteilung eines eigenen \"Kopfes\" innerhalb des Attention-Mechanismus, und zwar für einen gegebenen Satz \"Ich besuchte den Kurs Digital Leadership und lernte\".\n",
    "In beiden Diagrammen sind die vertikalen Balken proportional zur Stärke der Aufmerksamkeit, die jedes Wort vom jeweiligen Kopf erhält. Ein höherer Balken bedeutet, dass das entsprechende Wort eine stärkere Aufmerksamkeit erhält, wenn das Modell versucht, die Bedeutung des Satzes zu interpretieren oder eine Aufgabe wie die Übersetzung durchzuführen.\n",
    "\n",
    "Die Wörter am unteren Rand jedes Diagramms sind die Eingabewörter, und die kleinen \"v\" und \"k\" Symbole repräsentieren die Values und Keys im Attention-Mechanismus. Das \"q\" steht für den Query-Vektor, der in diesem Fall auf das Wort \"lernte\" zeigt, was bedeutet, dass die Visualisierung die Aufmerksamkeit aus der Perspektive dieses spezifischen Wortes darstellt.\n",
    "\n",
    "Attention-Kopf 1 fokussiert sich auf die Entitäten. Hier sehen wir, dass dieser Kopf vor allem auf die Wörter \"Kurs\", \"Digital\", und \"Leadership\" Aufmerksamkeit legt. Diese Wörter sind als Entitäten (Namen von Personen, Orten oder spezifischen Objekten) identifiziert worden, was darauf hindeutet, dass dieser Kopf darauf trainiert ist, solche Entitäten im Text zu erkennen und hervorzuheben.\n",
    "Rechts: Attention-Kopf 2 fokussiert sich auf die syntaktisch relevanten Wörter\n",
    "\n",
    "Der zweite Kopf scheint die Aufmerksamkeit auf die Wörter \"Ich\", \"besuchte\" aber auch das query Wort \"lernte\" selbst zu richten. Die beiden letzten Wörter sind Verben und \"Ich\" ist das zugehörige Pronomen. Dieser Kopf ist somit auf die Identifizierung syntaktischer Strukturen ausgerichtet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebaa9f-8183-4a31-96e1-e4dbe5813a06",
   "metadata": {},
   "source": [
    "### <a id=\"masking\"></a> Masking\n",
    "\n",
    "Das Maskieren des Inputs ist eine wichtige  Komponente der Transformerarchitektur. Beim Masking handelt es sich in Wirklichkeit um zwei Mechanismen, die zwar dieselbe Funktionsweise besitzen, aber sehr unterschiedliche Aufgaben in der Architektur übernehmen. Einerseits das Subsequent Masking, andererseits das Padding Masking. Das Padding Masking stellt jediglich sicher, dass nur Positionen mit Inhalt vom Transformer verarbeitet werden, während das Subsequent Masking dafür sorgt, dass der Decoder des Transformers autoregressiv ist. Das bedeutet, bei einer Vorhersage für eine Ausgabe an der Position $i$, soll das Modell nur Informationen aus den Eingabepositionen $1,\\ldots,i-1$ nutzen. Das Zusammenspiel der Beiden sehen Sie in der folgenden Abbildung (<a href=\"#fig:fig_masking\">Abbildung 7</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e465d9",
   "metadata": {},
   "source": [
    "#### <a id=\"padding-masking\"></a> Padding Masking\n",
    "\n",
    "Das Padding Masking ist notwendig, da Transformer sequenzielle Daten so verarbeiten, als ob sie eine fixe Länge $d_{model}$ hätten. Das geschieht deshalb, weil Transformer so lernen können jede Position der Ausgabelänge $d_{model}$ parallel vorherzusagen. \n",
    "\n",
    "Um während des Trainings auch Daten mit einer Länge größer oder kleiner $d_{model}$ zu nutzen, werden längere Sequenzen abgeschnitten und kürzere mit Nullen aufgefüllt. Diese Nullen müssen dann mit Hilfe von Padding Masking für das Training irrelevant gemacht werden. Das geschieht indem man alle Positionen die eine Null enthalten für das Modell auf $-\\infty$ setzt, sodass sie beim Gradient Descent nicht berücksichtigt werden.\n",
    "\n",
    "#### <a id=\"subsequent-masking\"></a> Subsequent Masking\n",
    "\n",
    "Das Subsequent Masking benutzt die gleich Technik des Werte auf $-\\infty$ setzen. Dabei wird aber nicht das mit irrelevanten Informationen angefüllte Ende des Eingabevektors auf $-\\infty$ gesetzt, sondern es werden diejenige Werte von $Score(Q, V)$ auf $-\\infty$ gesetzt, die einen Wert $V_j$, für die Ausgabe $i$ $Attention(Q,K,V)_i$ gewichten würden, obwohl $j>i$ gilt. Also es wird beschränkt, dass $Score(Q, V)$ nur diejenigen Werte von $V$ einbeziehen darf, die bei der Vorhersage für den $i$-ten Wert schon bekannt sind.\n",
    "\n",
    "Dass das relevant ist liegt daran, dass beim Training von Transformern der komplette Input gleichzeitig verwertet wird. Ein Satz wird als Ganzes vom Transformer verarbeitet und er erzeugt eine Vorhersage, welches Token an einer bestimmten Position in diesem Satz vorkommt. Dabei erhält er in der Eingabe aber schon die Information, welches Token an dieser Position tatsächlich steht, da er den kompletten Satz als Eingabe bekommen hat. Dies muss nun also innerhalb des Modells mit einer Maskierung wieder rückgängig gemacht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9065dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wir sollten überlegen in einer zukünftigen Version eine Simulation zur Darstellung des Masking einzufügen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e3202-2841-4260-991f-e3919582cffa",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig_masking\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_masking.jpg\" style=\"max-width: 100%; max-height: 150vh; height: auto;\" alt=\"Abbildung 7: Zwei Varianten des Maskings im Transformer Modell.\"/>\n",
    "    <figcaption>Abbildung 7: Die zwei Varianten des Maskings im Transformer Modell</figcaption>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0ea31",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"attention-mechanismen\"></a> Verschiedene Attention-Mechanismen\n",
    "\n",
    "In der Architektur werden verschiedene Attentionstypen unterschieden. Es gibt dabei zwei Variablen die beeinflussen, welche Art von Attention wir verwenden. Die erste Variable ist, woher die Eingaben $Q', K'$ und $V'$ kommen. Die zweite Variable ist die Maskierung, die wir auf $\\text{Score}(Q,K)$ anwenden.\n",
    "\n",
    "\n",
    "#### <a id=\"self-attention\"></a> Self-Attention\n",
    "Wir sprechen von Self-Attention, wenn gilt \n",
    "\n",
    "$$Q'=K'=V'.$$ \n",
    "\n",
    "Wenn sich $\\text{Score}(Q,K)$ also bildlich gesprochen daraus ergibt, welche Attention jede Position der Eingabe auf eine andere Position derselben Eingabe legt und diese Attention auf die Eingabe selbst angewandt wird.\n",
    "\n",
    "#### <a id=\"cross-attention\"></a> Cross-Attention\n",
    "\n",
    "Wie sprechen von Cross-Attention, wenn gilt \n",
    "\n",
    "$$K' = V'$$\n",
    "\n",
    "aber $Q'$ von diesen beiden Werten verschieden ist.\n",
    "Wenn sich $\\text{Score}(Q,K)$ also daraus ergibt, welche Attention jede Position einer Eingabe $Q'$ auf die Positionen einer zweiten Eingabe $K'$ gibt und dieser Attentionsscore auf die zweite Eingabe angewandt wird.\n",
    "\n",
    "Dies ist zum Beispiel in Transformern der Fall, wenn $Q'$ sich aus der Ausgabe des Encoder ergibt und $K' = V'$ ein Zwischenergebnis des Decoders ist.\n",
    "\n",
    "#### <a id=\"masked-attention\"></a> Masked Attention\n",
    "\n",
    "Ein Fall von Masked-Attention liegt dann vor, wenn bestimmte Werte von $\\text{Score}(Q,K)$ maskiert werden. Das ist zum Beispiel beim Subsequent Masking der Fall, hier wird $\\text{Score}(Q,K)_{i,j} = -\\infty$ gesetzt für alle Einträge $j>i$. Dadurch wird verhindert, dass die Ausgabe $\\text{Attention}(Q,K,V)_i$ sich auf die Werte $V_j, j>i $ stützt. Zum Beispiel wird während des Trainings im Decoder dadurch verhindert, dass das Model lernt Informationen aus den zukünftigen Einträgen $V_j, j>i$ zu benutzen, um $V_i$ vorherzusagen. Man sieht das gut in der Darstellung von $\\text{Score}(Q,K)$ in unserer <a href=\"#simulation_attention\">Simulation der Attention-Matrizen</a>. Dort liegen die Werte für $j>i$ meistens nahe bei $0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e5aaf9",
   "metadata": {},
   "source": [
    "## <a id=\"simulation\"></a> Vollständige Simulation\n",
    "\n",
    "Zuletzt findet sich hier nun noch eine Simulation des kompletten Inferenzvorgangs innerhalb eines Transformermodells. Diese Simulation zeigt alle der vorher genannten Schritte in einem Prozess und liefert eine tatsächliche Vorhersage für den hier eingegebene Text.\n",
    "Da unser Modell im Vergleich zu großen in der Wirtschaft eingesetzen Modellen nur mit sehr wenig Traninigsdaten und -zeit trainiert wurde, ist seine Vorhersageleistung sehr beschränkt und es wird keinen vernünftigen Text liefern. Die Mechanismen die dabei implementiert wurdens, sind allerdings identisch zu denen sehr großer Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cf257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.762518500Z",
     "start_time": "2024-01-16T09:57:26.729423800Z"
    }
   },
   "outputs": [],
   "source": [
    "inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_inf = widgets.Text(\n",
    "    value='Test sentence',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  \n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_inf = widgets.Button(description='Run interactive inference',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_inf = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_inf:\n",
    "        output_widget_inf.clear_output()  \n",
    "        inference_model(input_widget_inf.value, interactive=True) \n",
    "        inference_model.print_results(visualisation=True)\n",
    "\n",
    "button_widget_inf.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_inf, button_widget_inf, output_widget_inf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc7ff0",
   "metadata": {},
   "source": [
    "# <a id=\"bibliographie\"></a> Bibliographie\n",
    "[1] Vaswani, A. et al. Attention Is All You Need. Preprint at http://arxiv.org/abs/1706.03762 (2017).\n",
    "\n",
    "[2] Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Computation 9, 1735–1780 (1997).\n",
    "\n",
    "[3] Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks 5, 157–166 (1994).\n",
    "\n",
    "[4] Cho, K., van Merrienboer, B., Bahdanau, D. & Bengio, Y. On the Properties of Neural Machine Translation: Encoder–Decoder Approaches. in Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation 103–111 (Association for Computational Linguistics, 2014). doi:10.3115/v1/W14-4012.\n",
    "\n",
    "[6] Kaplan, J. et al. Scaling Laws for Neural Language Models. Preprint at http://arxiv.org/abs/2001.08361 (2020).\n",
    "\n",
    "[5] Goodfellow, I., Bengio, Y. & Courville, A. Deep learning. (The MIT Press, 2016).\n",
    "\n",
    "[7] Radford, A. et al. Language Models are Unsupervised Multitask Learners. (2019).\n",
    "\n",
    "[8] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Preprint at http://arxiv.org/abs/1810.04805 (2019).\n",
    "\n",
    "[9] Alammar, J. The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. https://jalammar.github.io/illustrated-transformer/ (2018).\n",
    "\n",
    "[10] Encoder-Decoder. Understanding The Model Architecture | by Naoki | Medium. https://naokishibuya.medium.com/transformers-encoder-decoder-434603d19e1.\n",
    "\n",
    "[11] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. (2014).\n",
    "\n",
    "[12] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[13] Gage, P. A New Algorithm for Data Compression. (1994).\n",
    "\n",
    "[14] Cho, K. et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Preprint at http://arxiv.org/abs/1406.1078 (2014).\n",
    "\n",
    "[15] Bahdanau, D., Cho, K. & Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate. Preprint at http://arxiv.org/abs/1409.0473 (2016).\n",
    "\n",
    "[16] OpenAI. GPT-4 Technical Report. Preprint at http://arxiv.org/abs/2303.08774 (2023).\n",
    "\n",
    "[17] Grefenstette, G. & Tapanainen, P. What is a word, What is a sentence? Problems of Tokenization.\n",
    "\n",
    "[18] Lin, Z. et al. A Structured Self-attentive Sentence Embedding. Preprint at http://arxiv.org/abs/1703.03130 (2017).\n",
    "\n",
    "[19] Schmidt, R. M. Recurrent Neural Networks (RNNs): A gentle Introduction and Overview. Preprint at http://arxiv.org/abs/1912.05911 (2019).\n",
    "\n",
    "[20] Ioffe, S. & Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Preprint at http://arxiv.org/abs/1502.03167 (2015).\n",
    "\n",
    "[21] Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer Normalization. Preprint at http://arxiv.org/abs/1607.06450 (2016).\n",
    "\n",
    "[22] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[23] Tunstall, L., von Werra, L., Wolf, T. Natural Language Processing Mit Transformern. https://www.oreilly.com/library/view/natural-language-processing/9781098157081/.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
