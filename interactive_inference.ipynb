{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2ab5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging and decorators\n",
    "import logging as log\n",
    "import psutil\n",
    "\n",
    "# system tools\n",
    "import pathlib\n",
    "\n",
    "# general modules\n",
    "import numpy as np\n",
    "\n",
    "# tensorflow modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# necessary for visualization and user input\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact_manual, interactive, interact, VBox, HTML\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbcc6862",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# logging settings\n",
    "log.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(processName)s %(threadName)s %(funcName)-20s %(message)s',\n",
    "        # log.INFO for normal run\n",
    "    level=log.INFO,\n",
    "        # log.DEBUG for diagnostics\n",
    "    # level=log.DEBUG,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# paths\n",
    "train_file_path = \"datasets/bookscorpusopen/processed_512\"\n",
    "val_file_path = \"datasets/corpus/processed_512\"\n",
    "\n",
    "vocab_path = 'datasets/vocab.txt'\n",
    "\n",
    "# tokenizer\n",
    "tokenizer_name = 'story_corpus_tokenizer'\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8c1c742",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def do_nothing(*args, **kwargs):\n",
    "    \"\"\"Placeholder for VisualWrapper\"\"\"\n",
    "    pass\n",
    "\n",
    "def clones(layer_class, N, **kwargs):\n",
    "    \"\"\"Produce N identical layers\"\"\"\n",
    "    log.debug(f'execute with class {layer_class.__class__.__name__} and N={N}')\n",
    "    return [layer_class(**kwargs) for _ in range(N)]\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"\"\"Mask out subsequent positions.\"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed09592b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class VisualWrapper():\n",
    "    \"\"\"This is a mixin-Class for the tensorflow layers that enable visualization during non-training sessions.\"\"\"\n",
    "    instances = []          # save instances of VisualWrapper for reset_counter classmethod (see below)\n",
    "    n_vis_layers_per_class = {\n",
    "        'StoryTokenizer': 1,\n",
    "        'EncoderDecoder': 1,\n",
    "        'EncoderStack': 1,\n",
    "        'MultiHeadedAttention': 1,\n",
    "        'StoryTokenizer': 1,\n",
    "        'PositionalEmbedding': 1,\n",
    "        'Generator': 1,\n",
    "        'ResidualSublayer': 1,\n",
    "    }\n",
    "    vis_data = []\n",
    "\n",
    "    def __init__(self, vis_on_count=None):\n",
    "        \"\"\"\n",
    "        Initialize a VisualWrapper instance.\n",
    "\n",
    "        Args:\n",
    "            vis_on_count (list, optional):  A list of counts on which to perform a visualizations. \n",
    "                                            If not provided, no operations will be performed on any count.\n",
    "            enabler (bool, optional):       A flag used to control whether visualization is enabled. \n",
    "                                            If False, it ensures no child class does perform any visualization.\n",
    "                                            Defaults to False.\n",
    "\n",
    "        The initialized instance is appended to the `VisualWrapper.instances` list, \n",
    "        the reset_counter classmethod resets the counters of all instances in the list.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        self.counter = 0\n",
    "        self.vis_on_count = vis_on_count if vis_on_count else [0]\n",
    "        if type(self).__name__ in self.n_vis_layers_per_class:\n",
    "            num_instances = sum(isinstance(obj, type(self)) for obj in VisualWrapper.instances)\n",
    "            if num_instances < self.n_vis_layers_per_class[type(self).__name__]:\n",
    "                log.debug(f'append {self} to VisualWrapper.instances')\n",
    "                VisualWrapper.instances.append(self)\n",
    "\n",
    "    def increase_count(self):\n",
    "        \"\"\"Increase counter\"\"\"\n",
    "        log.debug(f'execute')\n",
    "        self.counter += 1\n",
    "\n",
    "    # TODO: Enter standard texts and labels.\n",
    "    def save_data(self, \n",
    "                  text, \n",
    "                  x, \n",
    "                  mode_x, \n",
    "                  text_x,\n",
    "                  y=None, \n",
    "                  z=None,\n",
    "                  mode_y=None,  \n",
    "                  mode_z=None,\n",
    "                  text_y=None,\n",
    "                  text_z=None, \n",
    "                  x_axis=None, \n",
    "                  y_axis=None,\n",
    "                  id=None):\n",
    "        \"\"\"Saving data for visualization\"\"\"\n",
    "        log.debug(f'execute')\n",
    "        if self in self.instances:\n",
    "            if self.counter in self.vis_on_count:\n",
    "                self.increase_count()\n",
    "                log.debug(f'append data to vis_data')\n",
    "                self.vis_data.append({'x': x, \n",
    "                                    'y': y,\n",
    "                                    'z': z,\n",
    "                                    'mode_x': mode_x,\n",
    "                                    'mode_y': mode_y,\n",
    "                                    'mode_z': mode_z,\n",
    "                                    'text': text,\n",
    "                                    'text_x': text_x,\n",
    "                                    'text_y': text_y,\n",
    "                                    'text_z': text_z,\n",
    "                                    'x_axis': x_axis,\n",
    "                                    'y_axis': y_axis,\n",
    "                                    'id':id})\n",
    "\n",
    "    @classmethod         \n",
    "    def vis_data_generator(cls, id=None):\n",
    "        log.debug(f'initialize generator')\n",
    "        for data in cls.vis_data:\n",
    "            if id == None:\n",
    "                log.debug(f'yield {data}')\n",
    "                yield data\n",
    "            elif data['id'] == id:\n",
    "                log.debug(f'yield {data} with id {id}')\n",
    "                yield data\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def visualize_data(cls, id=None):\n",
    "        \"\"\"\n",
    "        Visualizes data_x (data_y) if tensorflow is not in training mode and global self.shoule_visualize is True.\n",
    "        This only happens while self.counter is in self.vis_on_count (counter is increased by 1 with every execution)\n",
    "\n",
    "        Args:\n",
    "            data_x (unspecific often tensors):              The data that is visualized. TODO: Implement type check, to catch errors in advance\n",
    "            mode (string):                                  One of the modes available in choose_func (see methods) to select the visualisation format.\n",
    "            training (bool):                                Boolean parameter used by tensorflow to differentiate training and inference.\n",
    "                                                            Only visualize when not in training mode.\n",
    "            text (string):                                  Explanatory text giving information about the visualisation data.\n",
    "                                                            Printed before visualisation is displayed.\n",
    "            data_y (unspecific often tensors, optional):    TODO: Implement for multiple data visualization\n",
    "            vis_diff (bool, optional):                      TODO: Implement for multiple data visualisation\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        \n",
    "        vis_data_gen = cls.vis_data_generator(id=id)\n",
    "\n",
    "        button = widgets.Button(description=\"Click to proceed\")\n",
    "        output = widgets.Output()\n",
    "\n",
    "        def on_button_clicked(b):\n",
    "            log.debug(f'execute')\n",
    "            with output:\n",
    "                # if all checks for visualization are passed execute visualisation\n",
    "\n",
    "                try:\n",
    "                    data = next(vis_data_gen)\n",
    "                    text = data['text']\n",
    "                    display_values = []\n",
    "                    display_values.append((data['x'], data['mode_x'], data['text_x']))\n",
    "                    display_values.append((data['y'], data['mode_y'], data['text_y']))\n",
    "                    display_values.append((data['z'], data['mode_z'], data['text_z']))\n",
    "                    \n",
    "                    log.debug(f'visualise data: {data}')\n",
    "        \n",
    "                    # print explanatory text\n",
    "                    cls.display_text(text)\n",
    "\n",
    "                    for data in display_values:\n",
    "                        # choose the correct visualization function\n",
    "                        visualisation_func = cls.choose_func(data[1])\n",
    "                        # print explanatory text\n",
    "                        cls.display_text(data[2])\n",
    "                        # apply visualization function to data_x\n",
    "                        visualisation_func(data[0])\n",
    "                except StopIteration:\n",
    "                    log.debug(f'Vis data generator exhausted.')\n",
    "                    b.disabled = True\n",
    "\n",
    "        button.on_click(on_button_clicked)\n",
    "        box = VBox([output, button])\n",
    "        display(box)\n",
    "\n",
    "    @classmethod    \n",
    "    def choose_func(cls, mode):\n",
    "        \"\"\"\n",
    "        This function returns an executable function for the chosen 'mode'.\n",
    "\n",
    "        Args:\n",
    "            mode (string): The string indicating the visualization mode to apply.\n",
    "\n",
    "        Returns:\n",
    "            function: An executable function taking one input argument. This argument should be the data to be visualized.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        if mode == 'color_bar':\n",
    "            return lambda x: cls.color_bar(x)\n",
    "        elif mode == 'print':\n",
    "            return lambda x: cls.display_text(x)\n",
    "        elif mode == 'reduce_dim':\n",
    "            return lambda x: cls.reduce_dim(x)\n",
    "        elif mode == 'matrix':\n",
    "            return lambda x: cls.matrix_repr(x)\n",
    "        else:\n",
    "            # return a placeholder function, if no valid 'mode' is given.\n",
    "            return do_nothing\n",
    "\n",
    "    @classmethod\n",
    "    def display_text(cls, text):\n",
    "        log.debug(f'execute')\n",
    "        if isinstance(text, str):\n",
    "            display(HTML('<p style=\"font-size:18px; color:blue;\">' + text + '</p>'))\n",
    "\n",
    "    @classmethod\n",
    "    def color_bar(cls, tensor, xlabel=None, ylabel=None):\n",
    "        \"\"\"\n",
    "        Use matplotlib to plot a colorbar that visualizes the values of a 1-D-tensor.\n",
    "\n",
    "        Args:\n",
    "            tensor (tf.tensor): The tensor to be visualized\n",
    "        \"\"\" \n",
    "        log.debug(f'execute')\n",
    "        # labels for the plot TODO: Generalize such that the labels are valid for all data types.\n",
    "        x_label = xlabel or 'Tiefe'\n",
    "        y_label = xlabel or 'Position'\n",
    "\n",
    "        # Assuming data[0] is a numpy array.\n",
    "        # If it's a ListWrapper or another list-like object, convert it to a numpy array.\n",
    "        # TODO: Doesn't work. Check for error.\n",
    "        data_array = np.array(tf.squeeze(tensor))\n",
    "\n",
    "        # If the array is 1D, reshape it into a 2D array with one column\n",
    "        if data_array.ndim != 2:\n",
    "            log.error('Error: Expected a 1D tensor')\n",
    "            return\n",
    "\n",
    "        # Set the size of the plot (you can adjust the dimensions as needed)\n",
    "        fig, ax = plt.subplots(figsize=(10, 2))\n",
    "\n",
    "        # Use matshow to create a color-coded visualization\n",
    "        cax = ax.matshow(data_array, cmap='jet', aspect='auto')\n",
    "\n",
    "        # Add colorbar\n",
    "        fig.colorbar(cax, label='Wertebereich')\n",
    "\n",
    "        # Set labels\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "\n",
    "        # Set x and y tick locations to the middle of the cells\n",
    "        #ax.set_xticks(np.arange(data_array.shape[1]), minor=False)\n",
    "        #ax.set_yticks(np.arange(data_array.shape[0]), minor=False)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def matrix_repr(cls, matrix):\n",
    "        log.debug(f'execute')\n",
    "        matrix = np.array(tf.squeeze(matrix))\n",
    "\n",
    "        # If the tensor is not 3D, print an error message and return\n",
    "        if matrix.ndim != 3:\n",
    "            log.error('Error: Expected a 3D tensor')\n",
    "            return\n",
    "\n",
    "        # Calculate the number of subplots\n",
    "        n_plots = matrix.shape[0]\n",
    "\n",
    "        # Define the subplot grid dimensions (trying to get a roughly square grid)\n",
    "        n_rows = int(np.sqrt(n_plots))\n",
    "        n_cols = n_plots // n_rows if n_plots % n_rows == 0 else n_plots // n_rows + 1\n",
    "\n",
    "        # Create a figure\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "\n",
    "        # Flatten axes for easier iteration\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        # Iterate over each matrix in the tensor\n",
    "        for i in range(n_plots):\n",
    "            # Create a color-coded visualization of the matrix\n",
    "            im = axes[i].imshow(matrix[i, :, :], cmap='jet')\n",
    "            axes[i].set_xlabel('Eingabe')\n",
    "            axes[i].set_ylabel('Ausgabe')\n",
    "\n",
    "            # Set the title of the plot to indicate which matrix is being visualized\n",
    "            axes[i].set_title(f'Aufmerksamkeitskopf {i + 1}')\n",
    "\n",
    "        # Add colorbar, associating with the last image created\n",
    "        fig.colorbar(im, ax=axes.ravel().tolist(), label='Wertebereich')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def reduce_dim(cls, tensor):\n",
    "        \"\"\"\n",
    "        Reduces the dimensionality of the input tensor using PCA and plots the result.\n",
    "\n",
    "        This function first scales the input tensor by its minimum absolute value, then applies PCA to reduce its \n",
    "        dimensionality to 3. It then creates a 3D quiver plot of the reduced data.\n",
    "\n",
    "        Args:\n",
    "            tensor (np.ndarray): The input tensor to be reduced and visualized. \n",
    "\n",
    "        Shows:\n",
    "            A 3D matplotlib plot of the tensor after dimensionality reduction using PCA.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        # Reduce the first dimension, to create a 1-D numpy array.\n",
    "        array = np.squeeze(tensor, axis=0)\n",
    "\n",
    "        # Scale the array by its minimum absolute value to normalize the data\n",
    "        scaled_array = array / np.min(np.abs(array))\n",
    "\n",
    "        # Apply PCA for dimensionality reduction.\n",
    "        # This reduces the dimensions of the data to 3.\n",
    "        # TODO: PCA must be trained. Alternative algorithms could be tsne or umap.\n",
    "        pca = PCA(n_components=3)\n",
    "        reduced_array = pca.fit_transform(scaled_array)\n",
    "\n",
    "        # Create a new figure and a set of subplots. \n",
    "        # The figure size is set to (3,3) to maintain a square aspect ratio. \n",
    "        # TODO: Find best size for plot\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        # Add another subplot to create a 3D plot.\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Create a quiver plot to visualize each point as a vector from the origin\n",
    "        ax.quiver(0, 0, 0, reduced_array[:, 0], reduced_array[:, 1], reduced_array[:, 2], arrow_length_ratio=0.1)\n",
    "\n",
    "        # Label each component (PCA dimension) on the axes.\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.set_zlabel('Component 3')\n",
    "        # Set a title for the plot\n",
    "        # TODO: Generalize the title\n",
    "        ax.set_title('Embeddings')\n",
    "\n",
    "        # Set the plot boundaries to be the maximum value in the reduced array.\n",
    "        boundaries = np.max(reduced_array)\n",
    "        ax.set_xlim([-boundaries, boundaries])\n",
    "        ax.set_ylim([-boundaries, boundaries])\n",
    "        ax.set_zlim([-boundaries, boundaries])\n",
    "\n",
    "        # Disply the plot\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def reset_visualiser(cls):\n",
    "        \"\"\"Reset the counter for all instances of the class.\"\"\"\n",
    "        log.debug(f'execute')\n",
    "        for instance in cls.instances:\n",
    "            instance.counter = 0\n",
    "        cls.vis_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87ec7d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputOutputWidget():\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_value, \n",
    "                 input_description,\n",
    "                 button_description,\n",
    "                 output_function):\n",
    "        self.input = widgets.Text(value=input_value, \n",
    "                                  description=input_description,\n",
    "                                  continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                                  layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "                                  )\n",
    "        self.button = widgets.Button(description=button_description,\n",
    "                                     layout = widgets.Layout(width='auto'))\n",
    "\n",
    "        self.output = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "        self.out_func = output_function\n",
    "\n",
    "    def on_button_click(self, b):\n",
    "        with self.output:\n",
    "            self.output.clear_output()  # clear the previous output\n",
    "            self.out_func()\n",
    "\n",
    "    def display(self):\n",
    "        self.button.on_click(self.on_button_click)\n",
    "        display(self.input, self.button, self.out_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8de18cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder(tf.keras.Model, VisualWrapper):\n",
    "    \"\"\"\n",
    "    Defines a Transformer model for sequence-to-sequence tasks.\n",
    "\n",
    "    This class implements the Transformer architecture, which consists of an Encoder and Decoder, each built from multiple stacked self-attention and feedforward layers.\n",
    "    Inherits from both the TensorFlow Keras Model class for building ML models and a custom VisualWrapper class for data visualization.\n",
    "\n",
    "    Attributes:\n",
    "        encoder_stack (Encoder):                The encoder component of the Transformer.\n",
    "        decoder_stack (Decoder):                The decoder component of the Transformer.\n",
    "        enc_embed (tf.keras.layers.Embedding):  The input embedding layer for the encoder.\n",
    "        dec_embed (tf.keras.layers.Embedding):  The input embedding layer for the decoder.\n",
    "        generator (tf.keras.layers.Dense):      The output linear layer.\n",
    "\n",
    "    Note: We use two seperate embeddings, because the encoder get's the data with start token, while the decoder get's the data without start token.\n",
    "    Note: To receive actual output from the model it is necessary to run call on the input and then the generator on the output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_stack, decoder_stack, enc_embed, dec_embed, generator):\n",
    "        \"\"\"\n",
    "        Initialize the EncoderDecoder model together with the parent VisualWrapper.\n",
    "        The EncoderDecoder model is an visualization enabler, that means, that it enables visualization for all its sublayer, if self.counter in self.vis_on_count.\n",
    "\n",
    "        Args:\n",
    "            encoder_stack (layers.Layer):   A Encoder object, consisting of a stack of self-attention and feedforward layers.\n",
    "                                            The stack size is determined within the object.\n",
    "            decoder_stack (layers.Layer):   A Decoder object, consisting of a stack of self-attention, source-attention and feedforward layers.\n",
    "                                            The stack size is determined within the object.\n",
    "            enc_embed (layers.Layer):       An embedding layer for the encoder input.\n",
    "            dec_embed (layers.Layer):       An embedding layer for the decoder input.\n",
    "            generator (layers.Layer):       The final linear layer that generates predictions.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self)\n",
    "\n",
    "        self.encoder_stack = encoder_stack\n",
    "        self.decoder_stack = decoder_stack\n",
    "        self.enc_embed = enc_embed\n",
    "        self.dec_embed = dec_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def encode(self, inputs, pad_mask, training=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (Tensor):            Input data tensor to encode.\n",
    "            pad_mask (Tensor):          Mask tensor to ignore padding tokens in the input.\n",
    "            training (bool, optional):  Boolean flag indicating whether the model is in training mode. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor:                     The output of the encoder stack.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        return self.encoder_stack(self.enc_embed(inputs), \n",
    "                                  pad_mask, \n",
    "                                  training=training)\n",
    "\n",
    "    def decode(self, enc_input, pad_mask, inputs, subseq_mask, training=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            enc_input (Tensor):         Encoded input data tensor to decode.\n",
    "            pad_mask (Tensor):          Mask tensor to ignore padding tokens in the input.\n",
    "            inputs (Tensor):            Input data tensor for the decoder.\n",
    "            subseq_mask (Tensor):       Mask tensor to ignore subsequent tokens in the input.\n",
    "            training (bool, optional):  Boolean flag indicating whether the model is in training mode. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor:                     The output of the decoder stack.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        return self.decoder_stack(self.dec_embed(inputs), \n",
    "                                  enc_input, \n",
    "                                  pad_mask, \n",
    "                                  subseq_mask, \n",
    "                                  training=training)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (tuple):             Tuple of Tensors (enc_input (Tensor, dtype=tf.float32), \n",
    "                                                          dec_input, (Tensor, dtype=tf.float32)\n",
    "                                                          pad_mask, (Tensor, dtype=tf.bool)\n",
    "                                                          subseq_mask(Tensor, dtype=tf.bool)\n",
    "                                                         ).\n",
    "            training (bool, optional):  Boolean flag indicating whether the model is in training mode. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output of the model.\n",
    "        \"\"\"\n",
    "        # We need to unpack the input, as tensorflows model.fit method requires the input to be passed as a single parameter,\n",
    "        # but it actually contains (enc_input, dec_input, pad_mask, subseq_mask) as a tuple.\n",
    "        enc_input, dec_input, pad_mask, subseq_mask = inputs\n",
    "\n",
    "        # the following is only used to visualize model input and output data\n",
    "    \n",
    "        input_emb_enc = self.enc_embed(enc_input)\n",
    "        input_emb_dec = self.dec_embed(dec_input)\n",
    "        self.save_data(text='Zuerst wird der Eingabetext in ein für das Modell verarbeitbares Embedding verwandelt.',\n",
    "                       x=input_emb_enc,\n",
    "                       mode_x='color_bar',\n",
    "                       text_x='This is the embedding created by the encoder.',\n",
    "                       y=input_emb_dec,\n",
    "                       z=input_emb_dec-input_emb_enc,\n",
    "                       mode_y='color_bar',\n",
    "                       mode_z='color_bar',\n",
    "                       text_y='This is the embedding created by the decoder.',\n",
    "                       text_z='Here you can see how the two embeddings differ from each other.',\n",
    "                       x_axis='X-Achse',\n",
    "                       y_axis='Y-Achse')\n",
    "\n",
    "        return self.decode(self.encode(enc_input, pad_mask, training), \n",
    "                           pad_mask,\n",
    "                           dec_input, \n",
    "                           subseq_mask, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85567461",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LayerNorm(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    Implements the Layer Normalization technique, a type of normalization performed on inputs across features.\n",
    "\n",
    "    Inherits from both the TensorFlow Keras Layer class for building custom layers, and a custom VisualWrapper class for data visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        features (int):             The size of the input data.\n",
    "            eps (float, optional):  A small constant added to the variance to avoid dividing by zero. Defaults to 1e-6.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self)\n",
    "\n",
    "        # Initialize the scale and offset parameters\n",
    "        self.a_2 = self.add_weight(shape=(input_size,), initializer='ones', name=self.name + \"a_2\")\n",
    "        self.b_2 = self.add_weight(shape=(input_size,), initializer='zeros', name=self.name + \"b_2\")\n",
    "        self.eps = eps\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Performs the layer normalization on the input data.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (Tensor):  The input data.\n",
    "\n",
    "        Returns:\n",
    "            norm_out (Tensor):      The normalized data.\n",
    "        \"\"\"\n",
    "        # Compute the mean and variance of the input data\n",
    "        mean, var = tf.nn.moments(input_tensor, axes=-1, keepdims=True)\n",
    "\n",
    "        # Compute the standard deviation\n",
    "        std = tf.math.sqrt(var + self.eps)\n",
    "\n",
    "        # Perform the layer normalization\n",
    "        norm_out = self.a_2 * (input_tensor - mean) / std + self.b_2\n",
    "\n",
    "        return norm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df40980e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ResidualSublayer(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    A layer that applies a sublayer to the input, followed by dropout, and then adds the input to the result.\n",
    "    This follows the 'pre-norm' variation of the Transformer architecture, where Layer Normalization is applied before the sublayer.\n",
    "\n",
    "    !!! This layer is used to wrap the attention sublayer and the feedforward layer in the encoder stack and decoder stack. !!!\n",
    "    \n",
    "    Inherits from both the TensorFlow Keras Layer class for building custom layers, and a custom VisualWrapper class for data visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (int):         The number of features in the input data.\n",
    "            dropout (float):    The rate of dropout to apply after the sublayer.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, input_tensor, sublayer, training=None):\n",
    "        \"\"\"\n",
    "        Applies the sublayer to the input after normalization, applies dropout, and then adds the input to the result.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (Tensor):      The input data.\n",
    "            sublayer (layers.Layer):    The sublayer to apply to the input data.\n",
    "            training (bool):            Indicates whether the layer should behave in training mode (apply dropout) or in inference mode (do not apply dropout).\n",
    "\n",
    "        Returns:\n",
    "            residual_out (Tensor): The output data.\n",
    "        \"\"\"\n",
    "        # Apply normalisation and sublayer\n",
    "        norm_input = self.norm(input_tensor)\n",
    "        sublayer_out = sublayer(norm_input)\n",
    "\n",
    "        sublayer_dropout = self.dropout(sublayer_out, training=True)\n",
    "\n",
    "        self.save_data(\n",
    "                       text=\"Visualize difference before/after dropout.\",\n",
    "                       x = sublayer_out,\n",
    "                       mode_x='color_bar',\n",
    "                       text_x='Das ist die Ausgabe einer der Transformerblöcke',\n",
    "                       y = sublayer_dropout,\n",
    "                       mode_y=\"color_bar\",\n",
    "                       text_y='Das sind die Veränderungen, die eine eingefügte Dropout-Layer einführt.',\n",
    "                       z= sublayer_out-sublayer_dropout,\n",
    "                       mode_z='color_bar',\n",
    "                       text_z='Hier sieht man wie die Werte durch das Dropout verändert wurde.')\n",
    "            \n",
    "        # compute residual output by applying dropout to the sublayer output and adding to the input\n",
    "        residual_out = input_tensor + self.dropout(sublayer_out, training=training)\n",
    "\n",
    "        return residual_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b73c58ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EncoderStack(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    This class represents the Encoder part of the Transformer model, which is composed of a stack of identical layers.\n",
    "    Each layer in the Encoder Stack consists of two sub-layers: a Multi-head Self-Attention mechanism, and a Position-wise\n",
    "    Fully Connected Feed-Forward network.\n",
    "    A residual connection is employed around each of the two sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N, size, **kwargs):\n",
    "        \"\"\"\n",
    "        Inititalize the EncoderStack instance\n",
    "        Args:\n",
    "            layer (layers.layer):   An instance of a layer, which will be cloned N times to form the encoder stack.\n",
    "            N (int):                The number of layers in the encoder stack.\n",
    "            size (int):             The dimensionality of the input/ouput space of the encoder.\n",
    "            **kwargs (various):     Additional keyword arguments. They contain the parameters of the layers in self.layers, such that they can\n",
    "                                    be passed to the clone function that initializes the layers.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self)\n",
    "        self.layers = clones(layer, N, size=size, **kwargs) # Creating N identical layer stacks to form the encoder\n",
    "        self.norm = LayerNorm(size)\n",
    "\n",
    "    def call(self, input_tensor, mask, training=None):\n",
    "        \"\"\"\n",
    "        This function propagates the input through the encoder stack, by applying succesively each layer in the self.layers attribute.\n",
    "        This is aquivalent to running the attention layer and the fully connected feed-forward layer N times, \n",
    "        before finally normalising and returning an output.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (Tensor): The input to the encoder.\n",
    "            mask (Tensor of dtype tf.Boolean): A boolean mask tensor for padding positions within the input.\n",
    "            training (bool, None): A boolean indicating whether to run the layer in training mode or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            (Tensor):              The output of the encoder stack.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            input_tensor = layer(input_tensor, mask, training=training)\n",
    "\n",
    "        encoder_out = self.norm(input_tensor, training=training)\n",
    "\n",
    "        self.save_data(text='Hier sieht man, welchen Unterschied die Anwendung der Layernorm am Ende des Encoders macht.', \n",
    "                       x=input_tensor, \n",
    "                       mode_x='color_bar', \n",
    "                       text_x='Dies ist die Ausgabe des Encoder bevor die Layernorm angewandt wird.', \n",
    "                       y=encoder_out, \n",
    "                       mode_y='color_bar', \n",
    "                       text_y= \"Dies ist die Ausgabe des Encoders nach der Anwendung der Layernorm.\",\n",
    "                       z=input_tensor-encoder_out,\n",
    "                       mode_z='color_bar',\n",
    "                       text_z='Das ist der Unterschied.',\n",
    "                       id='layer')\n",
    "\n",
    "        return encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1024c9ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    This class represents a single layer within the Encoder stack of the Transformer model.\n",
    "    Each EncoderLayer consists of two sub-layers: \n",
    "        - a Multi-head Self-Attention mechanism, \n",
    "        - a Position-wise Fully Connected Feed-Forward network.\n",
    "    A residual connection is employed around each of the two sub-layers.\n",
    "    \n",
    "    Note:   The residual sublayers do themselves not contain sublayers, because of two reasons:\n",
    "                - Like that we can clone the ResidualSublayer, instead of having to write out each single sublayer\n",
    "                - During forward pass, we need to pass different information to the sublayers e.g. mask, training, x, context.\n",
    "                  This process is simplified if the ResidualSublayer can be skipped.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        \"\"\"\n",
    "        Initializes the EncoderLayer\n",
    "        Args:\n",
    "            size (int):                    The dimensionality of the input/output space of the encoder.\n",
    "            self_attn (layers.Layer):      The Multi-head Self-Attention mechanism.\n",
    "            feed_forward (layers.Layer):   The Position-wise Fully Connected Feed-Forward network.\n",
    "            dropout (float):               The dropout rate to be applied to the output during training.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(ResidualSublayer, \n",
    "                               N=2, \n",
    "                               size=size, \n",
    "                               dropout=dropout)\n",
    "\n",
    "    def call(self, input_tensor, mask, training=None):\n",
    "        \"\"\"\n",
    "        This function propagates the input through the attention layer and the feed-forward layer.\n",
    "\n",
    "        The Self-Attention mechanism (sublayer[0]) takes three times the input_tensor as input for query, key, value\n",
    "        it hiddes the padding through a padding mask, passed through the mask argument, and returns the rearranged output.\n",
    "\n",
    "        The Feed-Forward network takes the output of the Self-Attention mechanism and creates meaningful information for the next Encoder-Layer.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor (Tensor):              The input to the encoder layer.\n",
    "            mask (Tensor of dtype tf.Boolean):  A boolean mask tensor for padding positions within the input.\n",
    "            training (bool, optional):          A boolean indicating whether to run the layer in training mode or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            ff_output (Tensor):                 The output of the encoder layer.\n",
    "        \"\"\"\n",
    "        attn_output = self.sublayer[0](input_tensor, \n",
    "                                        lambda x: self.self_attn(x, x, x, \n",
    "                                                                 mask, \n",
    "                                                                 training=training), \n",
    "                                        training=training)\n",
    "        ff_output = self.sublayer[1](attn_output, \n",
    "                                     lambda x: self.feed_forward(x, \n",
    "                                                                 training=training), \n",
    "                                     training=training)\n",
    "        return ff_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f15181ee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DecoderStack(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    This class represents the Decoder part of the Transformer model, which is composed of a stack of identical layers.\n",
    "    Each layer in the Decoder Stack consists of three sub-layers: \n",
    "        - a Masked Multi-head Self-Attention mechanism, \n",
    "        - a Multi-head Self-Attention mechanism over the Encoder's output,\n",
    "        - a Position-wise Fully Connected Feed-Forward network.\n",
    "    A residual connection is employed around each of the three sub-layers, followed by layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N, size, **kwargs):\n",
    "        \"\"\"\n",
    "        Inititalize the DecoderStack instance\n",
    "        Args:\n",
    "            layer (layers.layer):   An instance of a layer, which will be cloned N times to form the decoder stack.\n",
    "            N (int):                The number of layers in the decoder stack.\n",
    "            size (int):             The dimensionality of the input/output space of the decoder.\n",
    "            **kwargs (various):     Additional keyword arguments. They contain the parameters of the layers in self.layers, such that they can\n",
    "                                    be passed to the clone function that initializes the layers.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self)\n",
    "        self.layers = clones(layer, N, size=size, **kwargs)\n",
    "        self.norm = LayerNorm(size)\n",
    "\n",
    "    def call(self, input_tensor, enc_memory_tensor, src_mask, tgt_mask, training=None):\n",
    "        \"\"\"\n",
    "        This function propagates the input through the decoder stack, by applying succesively each layer in the self.layers attribute.\n",
    "        This is equivalent to running the masked attention layer, the attention layer over the encoder's output, \n",
    "        and the fully connected feed-forward layer N times, before finally normalising and returning an output.\n",
    "\n",
    "        Args:\n",
    "            input_tensor (Tensor):                  The input to the decoder.\n",
    "            enc_memory_tensor (Tensor):             The output of the encoder, serves as the \"memory\" in the Transformer model.\n",
    "            src_mask (Tensor of dtype tf.Boolean):  A boolean mask tensor for padding positions within the source input.\n",
    "            tgt_mask (Tensor of dtype tf.Boolean):  A boolean mask tensor for padding and preventing \"future\" information \n",
    "                                                    in attenting to the source input.\n",
    "            training (bool, None):                  A boolean indicating whether to run the layer in training mode or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            decoder_out (Tensor):                   The output of the decoder stack.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            input_tensor = layer(input_tensor, \n",
    "                                 enc_memory_tensor, \n",
    "                                 src_mask, tgt_mask, \n",
    "                                 training=training)\n",
    "            \n",
    "        decoder_out = self.norm(input_tensor, training=training)\n",
    "        \n",
    "        return decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc821d2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    This class represents a single layer within the Decoder stack of the Transformer model.\n",
    "    Each DecoderLayer consists of three sub-layers:\n",
    "        - a Masked Multi-head Self-Attention mechanism,\n",
    "        - a Multi-head Self-Attention mechanism that interacts with the output of the encoder,\n",
    "        - a Position-wise Fully Connected Feed-Forward network.\n",
    "    A residual connection is employed around each of the three sub-layers.\n",
    "    \n",
    "    Note: The residual sublayers do not themselves contain sublayers, because of two reasons:\n",
    "      - This way, we can clone the ResidualSublayer, instead of having to write out each single sublayer.\n",
    "      - During the forward pass, we need to pass different information to the sublayers e.g. masks, training, context. \n",
    "        This process is simplified if the ResidualSublayer can be skipped.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        \"\"\"\n",
    "        Initializes the DecoderLayer\n",
    "        Args:\n",
    "            size (int):                    The dimensionality of the input/output space of the decoder.\n",
    "            self_attn (layers.Layer):      The Masked Multi-head Self-Attention mechanism.\n",
    "            src_attn (layers.Layer):       The Masked Multi-head Source-Attention mechanism that interacts with the encoder output.\n",
    "            feed_forward (layers.Layer):   The Position-wise Fully Connected Feed-Forward network.\n",
    "            dropout (float):               The dropout rate to be applied to the output during training.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self)\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(ResidualSublayer, N=3, size=size, dropout=dropout)\n",
    "\n",
    "    def call(self, input_tensor, enc_memory, src_mask, tgt_mask, training=None):\n",
    "        \"\"\"\n",
    "        This function propagates the input through the decoder layer.\n",
    "\n",
    "        The Masked Self-Attention mechanism (sublayer[0]) takes three times the input_tensor as input for query, key, value.\n",
    "        It hides the padding and future positions from affecting the current token's attention calculation through a combined padding and lookahead mask, \n",
    "        passed through the tgt_mask argument, and returns the rearranged output.\n",
    "\n",
    "        The Encoder-Decoder Attention mechanism (sublayer[1]) takes the output from the previous Masked Self-Attention mechanism as the query and the encoder \n",
    "        output (memory) as both the key and the value. It also employs a padding mask (src_mask) on the encoder output and returns the attention-combined output.\n",
    "\n",
    "        The Feed-Forward network (sublayer[2]) takes the output of the Encoder-Decoder Attention mechanism and creates meaningful information for the next Decoder-Layer.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor (Tensor):                             The input to the decoder layer.\n",
    "            enc_memory (Tensor):                        The output of the encoder, serves as the \"memory\" in the Transformer model.\n",
    "            src_mask (Tensor of dtype tf.Boolean):  A boolean mask tensor for padding positions within the source input.\n",
    "            tgt_mask (Tensor of dtype tf.Boolean):  A boolean mask tensor for padding and preventing \"future\" information in self-attention mechanism within the target input.\n",
    "            training (bool, optional):              A boolean indicating whether to run the layer in training mode or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            ff_out (Tensor):                The output of the decoder layer.\n",
    "        \"\"\"\n",
    "        self_attn_out = self.sublayer[0](input_tensor, \n",
    "                                         lambda x: self.self_attn(x, x, x, \n",
    "                                                                  tgt_mask, \n",
    "                                                                  training=training),\n",
    "                                         training=training)\n",
    "        src_attn_out = self.sublayer[1](self_attn_out, \n",
    "                                        lambda x: self.src_attn(x, enc_memory, enc_memory, \n",
    "                                                                src_mask,\n",
    "                                                                training=training),\n",
    "                                        training=training)\n",
    "        ff_out = self.sublayer[2](src_attn_out, \n",
    "                                  lambda x: self.feed_forward(x,\n",
    "                                                              training=training),\n",
    "                                  training=training)\n",
    "\n",
    "        return ff_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c88a0163",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    Implements the Position-wise Feed-Forward Network (FFN) for the Transformer model.\n",
    "    The FFN consists of two fully connected layers with a ReLU activation in between.\n",
    "\n",
    "    Attributes:\n",
    "        dense_in (Dense):    First dense layer.\n",
    "        dense_out (Dense):   Second dense layer.\n",
    "        dropout (Dropout):   Dropout layer.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                d_model (int):      Output dimensionality of the Transformer.\n",
    "                d_ff (int):         Inner-layer dimensionality.\n",
    "                dropout (float):    Dropout rate after the ReLU activation.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self)\n",
    "        self.dense_in = layers.Dense(d_ff)\n",
    "        self.dense_out = layers.Dense(d_model)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, input_tensor, training=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the FFN. Applies both layers and ReLU activation to the input tensor.\n",
    "        Dropout inbetween the layers, as the ResidualLayer that wraps around will again perform a dropout\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "            training (bool, optional): Indicates whether to run the layer in training mode or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            (Tensor): Output tensor.\n",
    "        \"\"\"\n",
    "        return self.dense_out(self.dropout(tf.nn.relu(self.dense_in(input_tensor)), training=training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05fa20ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Generator(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    This class serves as the final layer of the Transformer model, generating the predicted output.\n",
    "    It applies a dense layer to the final output of the Transformer model and then a log softmax function \n",
    "    across the vocabulary dimension. This results in a distribution over the possible output tokens for each \n",
    "    position in the sequence, where the value of each token is the log probability of that token being the \n",
    "    output for that position.\n",
    "\n",
    "    Attributes:\n",
    "        proj (Dense): Dense layer that is applied to the final output of the Transformer model. It increases \n",
    "        the dimensionality of the input to be the size of the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab (int): Size of the output vocabulary.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "\n",
    "        self.proj = layers.Dense(vocab)\n",
    "\n",
    "    def call(self, input_tensor, training=None):\n",
    "        \"\"\"\n",
    "        This method applies the Dense layer and log softmax function to its input.\n",
    "        \n",
    "        Args:\n",
    "            input_tensor (Tensor):      The input tensor, which is the final output from the Transformer model.\n",
    "            training (bool, optional):  Indicates whether to run the layer in training mode or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            result (Tensor):    A tensor of the same shape as the input, but the last dimension is now the size \n",
    "                                of the vocabulary. Each value in this tensor is the log probability of the corresponding token \n",
    "                                being the output for the position in the sequence.\n",
    "        \"\"\"\n",
    "        result = tf.nn.log_softmax(self.proj(input_tensor), axis=-1)\n",
    "\n",
    "        self.save_data(text=f\"Am Ende wird eine logarithmische Softmax-Layer genutzt, um die Ausgabe des Modells auf die Länge des Vokabulars zu erweitern.\", \n",
    "                       x=tf.math.exp(result), \n",
    "                       mode_x='color_bar',\n",
    "                       text_x='Hier sieht man die finalen Wahrscheinlichkeiten, die das Modell generiert.')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9375337d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None, training=None):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention'\n",
    "\n",
    "    The attention function computes a weighted sum of the value vectors, with the weights being determined by the similarity of the\n",
    "    query vector with the corresponding key vector. The dot product of the query and key serves as the measure of similarity, and is scaled\n",
    "    by the square root of the dimension of the key vector to avoid the issue of the dot product growing large for large dimensions.\n",
    "\n",
    "    Args:\n",
    "        query, key, value (Tensor):                     The query, key and value vectors. \n",
    "                                                        These typically have shape (batch_size, num_heads, seq_len, depth).\n",
    "                                                        (seq_len as we want to calculate the attention for each position simultaneously)\n",
    "        mask (Tensor of dtype tf.Boolean, optional):    A mask to apply to the attention scores before softmax, \n",
    "                                                        in order to prevent attention to certain positions. \n",
    "                                                        The shape should be broadcastable to shape (batch_size, num_heads, seq_len, seq_len???).\n",
    "        dropout (Dropout, optional):                    Dropout layer to apply to the attention scores after softmax.\n",
    "        training (bool, optional):                      Whether the model is in training mode.\n",
    "\n",
    "    Returns:\n",
    "        output (Tensor):                                The result of applying attention mechanism to the value vectors.\n",
    "        p_attn (Tensor):                                The attention weights after applying softmax and dropout.\n",
    "    \"\"\"\n",
    "    log.debug(f'execute')\n",
    "    # Compute the dot product of the query and key vectors and scale by sqrt(d_k)\n",
    "    d_k = tf.cast(query.shape[-1], dtype=tf.float32)\n",
    "    scores = tf.matmul(query, tf.transpose(key, perm=[0, 1, 3, 2])) / tf.sqrt(d_k)\n",
    "\n",
    "    # Apply the mask to the scores before softmax\n",
    "    if mask is not None:\n",
    "        mask = tf.cast(mask, dtype=tf.bool)\n",
    "        scores = tf.where(mask, scores, tf.fill(tf.shape(scores), -1e9))\n",
    "\n",
    "    # Apply softmax to the scores to get the attention weights\n",
    "    p_attn = tf.nn.softmax(scores, axis=-1)\n",
    "\n",
    "    # Apply dropout to the attention weights\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn, training=training)\n",
    "\n",
    "    # Compute the weighted sum of the value vectors, using the attention weights\n",
    "    attn_out = tf.matmul(p_attn, value)\n",
    "\n",
    "    return attn_out, p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c664ea1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    MultiHeadedAttention layer is a key part of the Transformer model, enabling it to pay attention to different parts of the input for each output.\n",
    "    This is achieved by having multiple 'heads', each of which independently computes a scaled dot product attention over the input.\n",
    "    The outputs of these heads are then concatenated and linearly transformed to produce the final output.\n",
    "\n",
    "    Attributes:\n",
    "        d_k (int):                          Dimensionality of the query, key, and value vectors, \n",
    "                                            which should be identical for each head.\n",
    "        h (int):                            Number of heads.\n",
    "        query, key, value, linear (Dense):  These are the layers that perform the linear transformations for the input.\n",
    "        attn (Tensor, optional):            Tensor storing the attention values from the last forward pass.\n",
    "        dropout (Dropout):                  Dropout layer applied after the attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h (int):                    Number of attention heads.\n",
    "            d_model (int):              Dimensionality of the model.\n",
    "            dropout (float, optional):  Dropout rate.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0, 1])\n",
    "\n",
    "        assert d_model % h == 0 # make sure the number of attention heads are such that they can equally distribute over the input tensor\n",
    "        self.d_k = d_model // h # determine the size for the attention heads\n",
    "        self.h = h\n",
    "        self.query, self.key, self.value, self.linear = clones(layers.Dense, N=4, units=d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, query, key, value, mask=None, training=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the MultiHeadedAttention layer.\n",
    "        Applies linear transformations to the input, applies scaled dot product attention, then applies dropout, concatenates the heads,\n",
    "        and applies a final linear transformation.\n",
    "\n",
    "        Args:\n",
    "            query, key, value (Tensor):                     Input tensors. Value and query are (in our case) always the same.\n",
    "            mask (Tensor of dtype tf.Boolean, optional):    Boolean mask tensor for padding positions within the input.\n",
    "            training (bool, optional):                      Indicates whether to run the layer in training mode or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            result (Tensor):                                The output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads\n",
    "            mask = tf.expand_dims(mask, 1)\n",
    "\n",
    "        # find out how many batches are processed in parallel\n",
    "        nbatches = tf.shape(query)[0]\n",
    "\n",
    "        # Transform the input data into a shape that can be used as matrix input for the attention function.\n",
    "        # The original size is d_model, the trainable matrices self.query, self.key, self.value transform this input tensor\n",
    "        # into a tensor of the same size, but now we have to think of it as being of size h * d_k. Such that each section of size d_k,\n",
    "        # will be passed through the attention mechanism independently. That is why all this transformations have to be done afterwards.\n",
    "        # [nbatches, -1, self.h, self.d_k] does split the tensor into h smaller tensors of size d_k \n",
    "        # (nbatches is only there for working with batches of data). The Permutation ensures, that the h tensors are of shape (1, d_k), \n",
    "        # such that they can be processed.\n",
    "        query, key, value = [\n",
    "            tf.transpose(tf.reshape(lin_layer(input), [nbatches, -1 , self.h, self.d_k]), perm=[0, 2, 1, 3]) \n",
    "            for lin_layer, input in zip([self.query, self.key, self.value], (query, key, value))\n",
    "        ]\n",
    "\n",
    "        att_out, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout, training=training)\n",
    "\n",
    "        att_out_unmasked, attn_unmasked = attention(query, key, value, mask=None, dropout=None, training=training)\n",
    "\n",
    "        # Now we reverse the whole process and reshape the output into vectors of shape (nbatches, 1, d_model) again.\n",
    "        att_out = tf.reshape(tf.transpose(att_out, perm=[0, 2, 1, 3]), (nbatches, -1, self.h * self.d_k))\n",
    "\n",
    "        # visualization functions\n",
    "        self.save_data(text=f\"Man wendet wiederholt den Aufmerksamkeitsmechanismus auf das Embedding des Modells an\", \n",
    "                       x = att_out, \n",
    "                       mode_x=\"color_bar\",\n",
    "                       text_x=\"Hier sehen wir die Ausgabe die bei der Anwendung auf das Embedding entsteht.\",\n",
    "                       y = self.attn,\n",
    "                       mode_y=\"matrix\",\n",
    "                       text_y=f\"Dafür werden mehrere Aufmerksamkeitsköpfe (in Matrixform) parallel auf das Embedding angewandt und danach linear in ein neuen Embeddingvektor überführt. Hier sehen sie die dabei genutzten Aufmerksamkeitsmatrizen.\",\n",
    "                       id='attention')\n",
    "\n",
    "        # This finally mixes the results of the different heads together into one output vector\n",
    "        linear_output = self.linear(att_out)\n",
    "\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "61f08e1f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    \"\"\"\n",
    "    Generate positional encoding for a given length and depth to provide positional information.\n",
    "    Positional encoding is a technique where each position in the input sequence is assigned a \n",
    "    unique vector representation.\n",
    "\n",
    "    The encoding vector alternates between the sine and cosine functions of different \n",
    "    frequencies, which allows the model to distinguish the position of the inputs.\n",
    "\n",
    "    The positional encoding function uses a specific ratio to scale down the angle rates \n",
    "    exponentially (1 / (10000**(depth/depth))). It means that for lower dimensions in the \n",
    "    positional encoding, the angle rate is high which means the positional encoding is \n",
    "    changing rapidly for lower dimensions. For higher dimensions, the angle rate is low \n",
    "    which means the positional encoding is changing slowly. It gives a balance between \n",
    "    low and high frequency information.\n",
    "\n",
    "    Args:\n",
    "        length (int):   Length of the sequence for which positional encoding is to be generated.\n",
    "        depth (int):    The number of dimensions for the positional encoding. Equals the embedding size.\n",
    "\n",
    "    Returns:\n",
    "        Tensor:         A 2D Tensor of shape (length, depth) containing the positional encoding vectors.\n",
    "    \"\"\"\n",
    "    log.debug(f'execute')\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]    # Creates a numpy array of shape (sequence_length, 1)\n",
    "                                                    # filled with the numbers 1 to sequence length\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth  # Creates a numpy array of shape (1, depth/2)\n",
    "                                                    # filled with the numbers 1 to depth/2 divided by depth\n",
    "\n",
    "    angle_rates = 1 / (10000**depths) \n",
    "    angle_rads  = positions * angle_rates           # broadcasting such that now element [i,j] is pos(i) * angle(j)\n",
    "\n",
    "    # as we have above chosen depth/2 we can now concatenate sines and cosines to aquire an vectore of size depth\n",
    "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acff5760",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    A Keras layer to apply positional encoding on top of embeddings.\n",
    "\n",
    "    This layer creates embeddings for discret input vectors created by a tokenizer\n",
    "    and applies positional encoding to these embeddings to provide positional information.\n",
    "    The positional encoding is pre-computed in the constructor for efficiency and it is added to the output \n",
    "    of the embedding layer in the `call` method. The dropout is used to train the embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, dropout):\n",
    "        \"\"\"\n",
    "        Initializes Positional Embeddings\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int):   The size of the input token vector.\n",
    "            d_model (int):      The dimension used for the embeddings and positional encoding passed to the model.\n",
    "            dropout (float):    Value used for dropout.\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self)\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "        # calculate positional encoding\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def call(self, input_token_vec, training=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the embedding and positional encoding layers.\n",
    "        \n",
    "        Args:\n",
    "            input_token_vec (Tensor):   Input tensor of shape `(batch_size, sequence_length)`.\n",
    "            training (bool, optional):  Indicator for the mode (training or inference) of the model.\n",
    "\n",
    "        Returns:\n",
    "            y (Tensor):     The output tensor after applying embedding, positional encoding, and dropout. \n",
    "                            It has the shape of `(batch_size, sequence_length, d_model)`.\n",
    "        \"\"\"\n",
    "\n",
    "        length = tf.shape(input_token_vec)[1]\n",
    "\n",
    "        emb = self.embedding(input_token_vec) # is now a tensor of shape (batch_size, length, d_model)\n",
    "        emb_scaled = emb * tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # This factor sets the relative scale of the embedding and positional_encoding\n",
    "        emb_pos_enc = emb_scaled + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        y = self.dropout(emb_pos_enc)\n",
    "    \n",
    "        self.save_data(text=f\"Das ist das Embedding wie es von {self.__class__.__name__} aus dem Input Byte-Pair Encoding erzeugt wird.\", \n",
    "                       x=emb_scaled,\n",
    "                       mode_x='color_bar',\n",
    "                       text_x='Hier sieht man das Embedding ohne Positionsinformationen.',\n",
    "                       y=y,\n",
    "                       mode_y='color_bar', \n",
    "                       text_y='Das Positionale Encoding verändert das ursprüngliche Embedding, um Positionsinformationen hinzuzufügen.',\n",
    "                       z=emb_pos_enc-emb_scaled,\n",
    "                       mode_z='color_bar',\n",
    "                       text_z='Hier sieht man welche Veränderung die Positionsinformationen dem ursprünglichen Embedding hinzufügen.'\n",
    "                       )\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e44ec0f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class StoryTokenizer(tf.Module, VisualWrapper):\n",
    "    \"\"\"\n",
    "    The StoryTokenizer class is designed to perform tokenization and detokenization tasks using the BERT tokenizer.\n",
    "    \n",
    "    Methods:\n",
    "        tokenize:               Tokenize a string with BERT Tokenizer, add [Start] and [End] tokens.\n",
    "        detokenize:             Detokenize a token vector, clean the string of the reserved tokens.\n",
    "        lookup:                 Return the tokens a string is composed of.\n",
    "        add_start_end:          Add [Start], [End] toknes to a raggend token vector.\n",
    "        cleanup_text:           Remove reserved tokens from a string.\n",
    "        get_vocab_size:         Return the length of the vocabulary used by the tokenizer.\n",
    "        get_vocab_path:         Return the path of the vocabulary filee.\n",
    "        get_reserved_tokens:    Return a list of all reserved tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, reserved_tokens, vocab_path):    \n",
    "        \"\"\"\n",
    "        Initialize a StoryTokenizer\n",
    "\n",
    "        Args:\n",
    "            reserved_tokens (list of strings):  A list of strings with special tokens\n",
    "            vocab_path (string):                The path to the vocabulary file\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self)\n",
    "\n",
    "        self.tokenizer = tf_text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        # read in the vocabulary from file.\n",
    "        vocab = pathlib.Path(vocab_path).read_text(encoding='utf-8').splitlines()\n",
    "        self.vocab = tf.Variable(vocab)        \n",
    "\n",
    "    def tokenize(self, strings, training=None):\n",
    "        \"\"\"\n",
    "        Tokenizes the input strings and adds start and end tokens.\n",
    "\n",
    "        Args:\n",
    "            strings (tf.Tensor):        The strings to be tokenized.\n",
    "            training (bool, optional):  If True, the model is in training mode. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            out (tf.RaggedTensor):      The tokenized strings with added start and end tokens.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        encoded = self.tokenizer.tokenize(strings)\n",
    "        merged_enc = encoded.merge_dims(-2, -1)\n",
    "        out = self.add_start_end(merged_enc)\n",
    "\n",
    "        self.save_data(text=f\"In einem ersten Schritt erstellt der Tokenizer ein Byte-Pair Encoding des Satzes\",\n",
    "                       x=out,\n",
    "                       mode_x='print',\n",
    "                       text_x='Das ist die erzeugte Tokenliste.')\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def detokenize(self, tokenized, training=None):\n",
    "        \"\"\"\n",
    "        Detokenizes the input token IDs back into text strings.\n",
    "        Any reserved tokens (except for \"[UNK]\") are removed from the detokenized text.\n",
    "\n",
    "        Args:\n",
    "            tokenized (tf.RaggedTensor): The token IDs to be detokenized.\n",
    "            training (bool, optional): If True, the model is in training mode. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The detokenized text.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "\n",
    "        self.save_data(text=f\"Zuletzt werden die Daten durch einen Tokenizer in Textform überführt.\",\n",
    "                       x=self.lookup(tokenized),\n",
    "                       mode_x='print', \n",
    "                       text_x=\"Dies ist also die Vorhersage, die das Modell für die unterschiedlichen Positionen liefert.\"\n",
    "                       )\n",
    "\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return self.cleanup_text(self._reserved_tokens, words)\n",
    "    \n",
    "    def lookup(self, token_ids):\n",
    "        \"\"\"\n",
    "        Converts token IDs to their corresponding token strings from the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            token_ids (tf.RaggedTensor or tf.Tensor): The token IDs to be converted.\n",
    "\n",
    "        Returns:\n",
    "            tf.RaggedTensor or tf.Tensor: The corresponding token strings.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_start_end(ragged):\n",
    "        \"\"\"\n",
    "        Adds start and end tokens to the input token IDs.\n",
    "\n",
    "        Args:\n",
    "            ragged (tf.RaggedTensor): The input token IDs.\n",
    "\n",
    "        Returns:\n",
    "            tf.RaggedTensor: The token IDs with added start and end tokens.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        # Create vectores for the [Start] and [End] tokens.\n",
    "        START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "        END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "        # fill up dim 0 and concat in dim 1 to handle batches.\n",
    "        count = ragged.bounding_shape()[0]\n",
    "        starts = tf.fill([count, 1], START)\n",
    "        ends = tf.fill([count, 1], END)\n",
    "        return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_text(reserved_tokens, token_txt):\n",
    "        \"\"\"\n",
    "        Removes any reserved tokens (except for \"[UNK]\") from the input text.\n",
    "\n",
    "        Args:\n",
    "            reserved_tokens (list of str): The list of reserved tokens.\n",
    "            token_txt (tf.Tensor): The input text.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The cleaned up text.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        # Create a regular expression searching for reserved tokens\n",
    "        bad_tokens = list(filter(lambda token: token != \"[UNK]\", reserved_tokens))\n",
    "        bad_tokens_re = \"|\".join(bad_tokens)\n",
    "\n",
    "        # Search and delete reserved tokens from the token_txt tensor\n",
    "        bad_cells = tf.strings.regex_full_match(token_txt, bad_tokens_re)\n",
    "        ragged_result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "        # join the text\n",
    "        result = tf.strings.reduce_join(ragged_result, separator=' ', axis=-1)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "    \n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "    \n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68d432a9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_model(src_vocab, \n",
    "               tgt_vocab, \n",
    "               N=6, \n",
    "               d_model=512, \n",
    "               d_ff=2048, \n",
    "               h=8, \n",
    "               dropout=0.1) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Constructs a Transformer model from the given hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        src_vocab (int):            The size of the source vocabulary.\n",
    "        tgt_vocab (int):            The size of the target vocabulary.\n",
    "        N (int, optional):          The number of layers in the Transformer's encoder and decoder stacks. Default is 6.\n",
    "        d_model (int, optional):    The dimension of the Transformer's embedding space. Default is 512.\n",
    "        d_ff (int, optional):       The dimension of the feed forward network model. Default is 2048.\n",
    "        h (int, optional):          The number of attention heads. Default is 8.\n",
    "        dropout (float, optional):  The dropout rate. Default is 0.1.\n",
    "\n",
    "    Returns:\n",
    "        model (tf.keras.Model): A Transformer model constructed from the provided hyperparameters.\n",
    "\n",
    "    This function constructs an Encoder-Decoder model using the specified hyperparameters. \n",
    "    The Encoder and Decoder stacks each consist of N layers. \n",
    "    Each layer in the Encoder stack consists of a multi-headed self-attention mechanism, \n",
    "    followed by position-wise fully connected feed-forward network. \n",
    "    Each layer in the Decoder stack consists of a multi-headed self-attention mechanism, \n",
    "    a multi-headed source-attention mechanism over the Encoder's output, \n",
    "    and position-wise fully connected feed-forward network.\n",
    "    \"\"\"\n",
    "    log.debug(f'execute')\n",
    "    model = EncoderDecoder(\n",
    "                EncoderStack(\n",
    "                    EncoderLayer,\n",
    "                    N=N, \n",
    "                    size=d_model, \n",
    "                    dropout=dropout, \n",
    "                    self_attn=MultiHeadedAttention(h, d_model), \n",
    "                    feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout)),\n",
    "                DecoderStack(\n",
    "                    DecoderLayer, \n",
    "                    N=N, \n",
    "                    size=d_model, \n",
    "                    dropout=dropout,\n",
    "                    self_attn=MultiHeadedAttention(h, d_model), \n",
    "                    src_attn=MultiHeadedAttention(h, d_model), \n",
    "                    feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout)),\n",
    "                PositionalEmbedding(\n",
    "                    src_vocab, \n",
    "                    d_model,\n",
    "                    dropout),\n",
    "                PositionalEmbedding(\n",
    "                    tgt_vocab, \n",
    "                    d_model,\n",
    "                    dropout),\n",
    "                Generator(tgt_vocab)\n",
    "            )\n",
    "    log.debug(f'model set up')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "724ef710",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader():\n",
    "    \"\"\"\n",
    "    Documentation\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 tokenizer,\n",
    "                 d_model = 512,\n",
    "                 n_stacks = 6,\n",
    "                 h_att = 8,\n",
    "                 load_model = False,\n",
    "                 model_load_path = None\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Docstring\n",
    "        \"\"\"\n",
    "        log.debug(f'initialize {self.__class__.__name__}')\n",
    "        # class modules\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # var for model compile\n",
    "        self.vocab_size = tokenizer.get_vocab_size()\n",
    "        self.d_model = d_model\n",
    "        self.n_stacks = n_stacks\n",
    "        self.h_att = h_att\n",
    "        \n",
    "        # var for load and save\n",
    "        self.load_model = load_model\n",
    "        self.model_load_path = model_load_path\n",
    "\n",
    "        # compile model and load model weights if applicable\n",
    "        self.model = self.set_up_model()\n",
    "        if load_model:\n",
    "            self.load_model_weights(self.model, self.d_model, self.model_load_path)\n",
    "        \n",
    "    def set_up_model(self):\n",
    "        # set_up_model\n",
    "        model = make_model(self.vocab_size, \n",
    "                            self.vocab_size, \n",
    "                            d_model = self.d_model,\n",
    "                            N = self.n_stacks,\n",
    "                            h = self.h_att)\n",
    "        log.debug(f'model set up')\n",
    "\n",
    "        VisualWrapper.reset_visualiser()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def load_model_weights(self, model, d_model, model_folder):\n",
    "        \"\"\"\n",
    "        Load the latest model weights if available.\n",
    "\n",
    "        Args:\n",
    "            model (tf.keras.Model):         The model to which the weights will be loaded.\n",
    "            d_model (int):                  The dimension of the Transformer architecture.\n",
    "            model_folder (str, optional):   The directory from which to load the weights. \n",
    "                                            Default is None.\n",
    "\n",
    "        Returns:\n",
    "            model (tf.keras.Model):         The model with the loaded weights.\n",
    "            \n",
    "        This function loads the weights from the latest trained model found in the provided model_folder \n",
    "        or from the latest model in the current directory if load_latest is True.\n",
    "        \"\"\"\n",
    "        log.debug(f'execute')\n",
    "        # TODO: Ensure architecture sizes match.\n",
    "        if model_folder is not None:\n",
    "            log.debug(f'model_folder={model_folder}')\n",
    "            # Load weights from the specified model folder\n",
    "            directories = [pathlib.Path(model_folder)]\n",
    "        else:\n",
    "            directories = sorted(pathlib.Path('.').glob('model_N*_h*'), key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "\n",
    "        log.debug(f'load_dir={directories}')\n",
    "\n",
    "        # Load weights from the latest trained model\n",
    "        latest_weights = None\n",
    "        if directories:\n",
    "            latest_dir_path = directories[0]\n",
    "            # Get all the h5 files inside the directory and sort them\n",
    "            h5_files = sorted(latest_dir_path.glob('*.h5'))\n",
    "\n",
    "            if h5_files:\n",
    "                # Pick the last epoch file (or final_model file if it exists)\n",
    "                latest_weights = h5_files[-1]\n",
    "\n",
    "        log.debug(f'model weights extracted')\n",
    "\n",
    "        # Load weights if we found a previously trained model\n",
    "        if latest_weights is not None:\n",
    "            log.debug(f'Loading weights from {latest_weights}')\n",
    "            \n",
    "            # Create a dummy input matching the input shape of the model\n",
    "            # TODO: Ensure that the shape and type of the dummy_input match with the actual input that your model is going to receive.\n",
    "            dummy_input = tf.random.uniform(shape=[1,d_model]), tf.random.uniform(shape=[1,d_model]), None, None\n",
    "            # Call the model on the dummy input\n",
    "            _ = model.generator(model(dummy_input))\n",
    "\n",
    "            model.load_weights(latest_weights)\n",
    "        log.debug(f'model loaded with weights')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11872c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordComplete(tf.Module, VisualWrapper):\n",
    "  \"\"\"\n",
    "    This class defines a complete sequence generation model for a Transformer. \n",
    "    It uses a given tokenizer and Transformer model to generate sequences.\n",
    "  \"\"\"\n",
    "  def __init__(self, \n",
    "               tokenizer, \n",
    "               transformer, \n",
    "               max_length=512, \n",
    "               pad_id=0,\n",
    "               dtype=tf.Tensor, \n",
    "               decode_result=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        tokenizer (Tokenizer):          Tokenizer object to convert raw text into tokens.\n",
    "        transformer (tf.keras.Model):   A Transformer model used for sequence generation.\n",
    "        max_length (int, optional):     The maximum length of sequences that can be generated.\n",
    "                                        Default is 512.\n",
    "        dtype (tf.Tensor, optional):    The datatype of the output tensor. Default is tf.Tensor.\n",
    "        decode_result (bool, optional): If True, decode the output tensor into a string. \n",
    "                                        Default is True.\n",
    "    \"\"\"\n",
    "    log.debug(f'initialize {self.__class__.__name__}')\n",
    "    super().__init__()\n",
    "    VisualWrapper.__init__(self)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.transformer = transformer\n",
    "    self.max_length = max_length\n",
    "    self.pad_id = pad_id\n",
    "    self.dtype = dtype\n",
    "    self.decode_result = decode_result\n",
    "  \n",
    "  def __call__(self, input, decode=True, encoding='utf-8', interactive=False):\n",
    "    \"\"\"\n",
    "    Performs the sequence generation.\n",
    "\n",
    "    Args:\n",
    "        input (str or tf.Tensor):   The input sequence.\n",
    "        decode (bool, optional):    If True, the output sequence is decoded into a string. \n",
    "                                    Default is True.\n",
    "        encoding (str, optional):   The encoding to use when decoding the output sequence. \n",
    "                                    Default is 'utf-8'.\n",
    "        training (bool, optional):  Whether the model is currently training. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        text (str or tf.Tensor):    The generated text. If decode_result is True, this is a string.\n",
    "                                    Otherwise, it is a tensor.\n",
    "        tokens (tf.Tensor):         The tensor of generated tokens.\n",
    "    \"\"\"\n",
    "    # during model set-up visualise data is created\n",
    "    VisualWrapper.reset_visualiser()\n",
    "\n",
    "    # initialize loading widget\n",
    "    if interactive:\n",
    "      load_bar = widgets.FloatProgress(value=0,\n",
    "                                       min=0,\n",
    "                                       max=self.max_length,\n",
    "                                       description='Lädt',\n",
    "                                       bar_style='info',\n",
    "                                       style={'bar_color': 'green'},\n",
    "                                       orientation='horizontal')\n",
    "      display(load_bar)\n",
    "\n",
    "    # TODO: Bug with empty strings as input\n",
    "    # Convert input to tensor if it is not already\n",
    "    # Create a dynamic tensor to store output\n",
    "    # Make sure tensor_input is 2-D\n",
    "    tensor_input = tf.convert_to_tensor(input)\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    if len(tensor_input.shape) == 0:\n",
    "      tensor_input = tensor_input[tf.newaxis]\n",
    "\n",
    "    # tokenize and encode input\n",
    "    # Identify end token of the input\n",
    "    tokenized_input = self.tokenizer.tokenize(tensor_input).to_tensor()\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    context = self.transformer.encode(input_without_eos, None)\n",
    "    end = tokenized_input[-1][-1]\n",
    "\n",
    "    # Write the input tokens (excluding the last one) to the output array\n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "      output_array = output_array.write(i, value)\n",
    "\n",
    "    # Start the generation of sequence from the last position of the input to max_length\n",
    "    for i in tf.range(output_array.size(), self.max_length):\n",
    "    \n",
    "      if interactive:\n",
    "        load_bar.value=i\n",
    "\n",
    "      # Prepare input for decoder\n",
    "      # Decode the input\n",
    "      dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "      decode = self.transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "      # Create logits predictions and select the last predicted token\n",
    "      predictions = self.transformer.generator(decode)\n",
    "      predictions_last = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "      predicted_id = tf.argmax(predictions_last, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the decoder as its input again.\n",
    "      output_array = output_array.write(i, predicted_id[0][0])\n",
    "\n",
    "      # break the loop, if [End] token is predicted\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "    \n",
    "    if interactive:\n",
    "      load_bar.value = load_bar.max\n",
    "    # Create a tensor for detokenization\n",
    "    # Detokenize\n",
    "    # Create tokens from detokenized output again\n",
    "    output = output_array.concat()[tf.newaxis]\n",
    "    text = self.tokenizer.detokenize(output)\n",
    "    tokens = self.tokenizer.lookup(output)\n",
    "\n",
    "    # If decode_result is True, decode the text tensor into a string\n",
    "    if self.decode_result:\n",
    "      text = text.numpy()[0].decode(encoding)\n",
    "      print(text)\n",
    "  \n",
    "  def print_results(self, visualisation=False):\n",
    "    if visualisation:\n",
    "      VisualWrapper.visualize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cff4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelLoader(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                            d_model=512,\n",
    "                            n_stacks=2,\n",
    "                            h_att=4,\n",
    "                            load_model=True,\n",
    "                            model_load_path=\"model_N2_h4_d512_t20230831-134344\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0c84f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "#input = 'School held little appeal'\n",
    "\n",
    "#process = psutil.Process()\n",
    "#print(process.memory_info().rss)\n",
    "\n",
    "#output = inference_model('School held little appeal')\n",
    "\n",
    "#inference_model.print_results()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b25d199d",
   "metadata": {},
   "source": [
    "# Interaktive Erklärung der Transformerarchitektur\n",
    "## Einleitung\n",
    "### Kurzübersicht Transformer\n",
    "\n",
    "Transformer-Modelle sind eine von Vaswani et al. [^vaswani2017] vorgeschlagene Architektur für das Modellieren von sequentiellen Daten. Im Gegensatz zu den vorher genutzten Architekturen wie RNNs [Cite RNN] oder CNNs [Cite CNN] ermöglichen Transformer allerdings das parallele Verarbeiten der sequentiellen Daten während des Trainings und ermöglichen dadurch mit erheblich reduzierter Trainingszeit und -rechenkapazität sequentielle Daten zu verarbeiten [Cite comparisson].\n",
    "\n",
    "Um diese parallele Verarbeitung zu erreichen nutzen Transformermodelle Aufmerksamkeitsblöcke. Aufmerksamkeitsblöcke sind trainierbare Matrizen, die von einem Eingabevektor auf einen Ausgabevektor projizieren. Also eine sequentielle Datenstruktur in Vektorform in eine ebenfalls sequentielle Datenstruktur in Vektorform verarbeiten, indem eine Matrixmultiplikation auf dein Eingabevektor angewandt wird. Wie das funktioniert wird in [#Aufmerksamkeitsmechanismus] gezeigt.\n",
    "Diese Matrizen projizieren also im Idealfall von jedem Eintrag im Eingabevektor genau diejenige Information auf einen Eintrag im Ausgabevektor, die an der jeweiligen Stelle die Ausgabe beeinflussen soll. Da hierbei die Ausgabe i nicht von der Ausgabe i-1 abhängt, können wir parallel die komplette Ausgabe zu jeder Eingabe erzeugen und mit einem Gradient-Descent [Cite] Verfahren unser Modell trainieren\n",
    "Da wir während der Inferenz (Erklärung was Inferenz ist) von sequentiellen Daten aber meist nur die vorhergehende Daten nutzen möchten benutzen Transformer Architekturen eine Maske, die nach der Anwendung der Matrixmultiplikation alle Informationen, die nachfolgende Daten liefern herausfiltern, sodass schon während dem Training nur die Verbindung zwischen der aktuellen Position und den vorhergehenden gelernt wird.\n",
    "Durch wiederholen von Aufmerksamkeitsblöcken und einem nachfolgenden Feed-Forward Netzwerk entsteht so eine Architektur zur Verarbeitung sequenzieller Daten, die verglichen mit vorherigen Architekturen weniger Rechenkapazität benötigt und trotzdem eine wesentlich reduzierte Trainigszeit aufweist. [!Zitat]\n",
    "\n",
    "<span style=\"color:red\"> Hier eine grafische Darstellung von Eingabe- zu Ausgabevektor darstellen. Wie werden sequentielle Daten verarbeitet und wie macht das ein Transformer </span>\n",
    "\n",
    "### Ziel dieses interaktiven Artikels\n",
    "\n",
    "Ziel dieses interaktiven Artikels soll es sein die Architektur die [Vaswani2017] beschreiben in ihren einzelnen Komponenten darzustellen. Der Fokus liegt hierbei darauf die Prozesse, die während der Verarbeitung sequentieller Daten stattfinden, grafisch durch interaktive Anwendungen darzustellen der Nutzer den Einfluss unterschiedlicher Architekturbausteine auf verschiedene Eingaben anschaulich klar wird. Ziel ist es, die verschiedenen Bausteine so zu erklären, dass einem möglichen Anwender die Implementation einer Tranformer-Architektur, durch ein Verständnis des Nutzens der einzelnen Architekturbausteine, erleichtert wird.\n",
    "\n",
    "Der Artikel soll dazu dienen eine Implementierung ohne einschlägiges Vorwissen, z.B. im Kontext von KMUs die sich bisher noch nicht mit der Modellierung sequentieller Daten beschäftigt haben, zu ermöglichen und Programmierer in der Designentscheidungen in der Transformerarchitektur zu unterstützen.\n",
    "[Vaswani2017] und viele der auf ihrer Architektur aufbauenden wissenschaftlichen Arbeiten(Zitate), Erklärartikel oder -video (Zitate) beschränken sich darauf den Aufmerksamkeitsmechanismus ausführlich darzustellen. Dabei werden die Designentscheidungen für trainingsrelevante Elemente wie Dropout (Zitat), Residuale Verbindungen (Zitat) sowie die Beschreibung bereits etablierter Methoden wie Byte-Pair Encoding (zitat) als Einbettung oder Log-Softmax (Zitat) vernachlässigt. Diese Elemente sollen hier aber ebenfalls dargestellt werden.\n",
    "\n",
    "### Architekturübersicht\n",
    "\n",
    "In Abbildung 1 ist eine vollständige Darstellung aller Architekturelemente zu finden, die Teil der Transformerarchitektur sind. Dabei unterscheiden wir vier verschiedene Elemente. Prozesse und Methoden werden in Schwarz dargestellt. Die in diesen Prozessen generierte Daten werden in Blau dargestellt. Trainierbare Parameter werden in Gelb dargestellt. Hyperparameter, also festzulegende Eingabeparameter für das Modell werde in Grau dargestellt. Ebenfalls in Grau werden Eingabe- und Ausgabedaten dargestellt.\n",
    "Unsere Abbildung ist insofern komplett, als sie jeglichen Weg zeigen, den Daten durch das Modell nehmen können. Eine Darstellung in der Form eines Encode-Decoder Netzwerks [Encoder-Decoder], wie sie [Vaswani2017] und andere (Zitat) nutzen findet sich in Abbildung 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d914d5",
   "metadata": {},
   "source": [
    "\n",
    "##### Abbildung 1\n",
    "<a href=\"#fig:input\">Hier</a> sehen sie die komplette Darstellung der Architektur eines Transformermodells.\n",
    "\n",
    "<figure id=\"fig:input\" style=\"height: 700px;\">\n",
    "  <img src=\"./img/tf_arch_full.jpg\" style=\"height: 1400px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 1: Transformerarchitektur</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a6b568b",
   "metadata": {},
   "source": [
    "\n",
    "#### Encoder-Decoder\n",
    "\n",
    "Eine Encoder-Decoder Architektur besteht aus zwei voneinander getrennten neuronalen Netzen dem Encoder und dem Decoder, die aber zusammen trainiert werden. Diese Architektur wurde von [Cho 2014 Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation] vorgeschlagen und dann vor allem im Kontext von Neural Machine Translation (NMT) verwendet. Der Vorteil von Encoder-Decoder liegt darin, dass man sowohl den Encoder als auch den Decoder ersetzen kann. Im Idealfall könnte man im Bereich von NMT pro Sprache einen Encoder und einen Decoder trainieren, um dann beliebige Übersetzungen zwischen allen Sprachen zu generieren. Da auch [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE], die zuerst Aufmerksamkeitsmechanismen für Rekurrente Neuronale Netze (RNN) vorgeschlagen haben, eine Encoder-Decoder Architektur verwenden, verwundert es nicht, dass auch [Attention Is All You Need] eine solche Architektur wählten. Insbesondere da [Attention is All You Need] Transformer ebenfalls für NMT benutzen.\n",
    "Transformer zur Textvervollständigung können auch ausschließlich auf Decodern beruhen. In unserem Beispiel wird der Encoder zum enkodieren der Eingabe verwendet, um dann im Decoders die Eingabe als Anfang der Ausgabe zu verwenden. In Abbildung 2 sieht man wie Encoder und Decoder zusammenarbeiten. Der Decoder benutzt Selbstaufmerksamkeit, um seinen Input zu verarbeiten, nutzt dann <span style=\"color:red\">Source-Attention</span>, für den nächsten Schritt, um final erneut Selbstaufmerksamkeit zu benutzen, wohingegen der Encoder ausschließlich über Selbstaufmerksamkeit funktioniert. Näheres siehe im Kapitel Attention.\n",
    "\n",
    "##### Abbildung 2\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Darstellung des Encoder-Decoder Prinzips\n",
    "Darstellung der Layer im Encoder und Decoder\n",
    "</span>\n",
    "Hier können Sie nun einen Beispielsatz zuerst vom Encoder kodieren lassen, um ihn dann im nächsten Schritt vom Decoder dekodieren zu lassen und damit eine Ausgabe zu erhalten.\n",
    "<span style=\"color:red\">Interaktive Anwendung Encoder dann Decoder</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4798807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d138526bd2694718b165e8ed290ea8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Encoder-Decoder Test', continuous_update=False, description='Ihre Eingabe:', layout…"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "transformer = model.model\n",
    "\n",
    "input_widget_enc_dec = widgets.Text(\n",
    "    value='Encoder-Decoder Test',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_enc = widgets.Button(description='Enkodiere die Eingabe',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "button_widget_dec = widgets.Button(description='Dekodiere die Eingabe',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_enc = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "output_widget_dec = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click_enc(b):\n",
    "  with output_widget_enc:\n",
    "    output_widget_enc.clear_output()  # clear the previous output\n",
    "    #output_widget_dec.clear_output()\n",
    "\n",
    "    # Convert input to tensor if it is not already\n",
    "    # Create a dynamic tensor to store output\n",
    "    # Make sure tensor_input is 2-D\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)\n",
    "    if len(tensor_input.shape) == 0:\n",
    "      tensor_input = tensor_input[tf.newaxis]\n",
    "    # tokenize and encode input\n",
    "    # Identify end token of the input\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    context = transformer.encode(input_without_eos, None)\n",
    "\n",
    "    VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(context)\n",
    "\n",
    "def on_button_click_dec(b):\n",
    "  with output_widget_dec:\n",
    "    output_widget_dec.clear_output()\n",
    "\n",
    "    # Convert input to tensor if it is not already\n",
    "    # Create a dynamic tensor to store output\n",
    "    # Make sure tensor_input is 2-D\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    if len(tensor_input.shape) == 0:\n",
    "      tensor_input = tensor_input[tf.newaxis]\n",
    "    # tokenize and encode input\n",
    "    # Identify end token of the input\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    context = transformer.encode(input_without_eos, None)\n",
    "\n",
    "     # Write the input tokens (excluding the last one) to the output array\n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "      output_array = output_array.write(i, value)\n",
    "\n",
    "    dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "    dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "    VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(dec_out)\n",
    "\n",
    "button_widget_enc.on_click(on_button_click_enc)\n",
    "button_widget_dec.on_click(on_button_click_dec)\n",
    "\n",
    "widgets.VBox([input_widget_enc_dec, widgets.HBox([widgets.VBox([button_widget_enc, output_widget_enc]), widgets.VBox([button_widget_dec, output_widget_dec])])])\n",
    "#display(input_widget_enc_dec, button_widget_enc, output_widget_enc, button_widget_dec, output_widget_dec)\n",
    "\n",
    "\n",
    "#print('tok_out', tokenized_input)\n",
    "#print('enc_out', context)\n",
    "#print(\"dec_out\", dec_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028a3317",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Architekturblöcke\n",
    "\n",
    "Prinzipiell lassen sich Transformer in mehrere Blöcke einteilen.\n",
    "\n",
    "1. Eingabeblock\n",
    "\n",
    "Die Eingabepipeline verarbeitet die Eingabedaten in eine Form die für die Matrixtransformation im Aufmerksamkeitsblock genutzt werden kann. Diese unterscheidet sich für verschiedene Datentypen. In unserem Beispiel nutzen wir Textdaten, die wir durch Tokenisierung und ein Embedding in Tensoren verwandeln.\n",
    "\n",
    "2. Aufmerksamkeitsblock\n",
    "\n",
    "Der Aufmerksamkeitsblock verarbeitet Daten in Tensorform und liefert somit eine Abbildung von den Eingabetensoren auf die Ausgabetensoren, die jeweils von den Eingabe- und Ausgabeblöcken interpretiert wird.\n",
    "Man kann verschiedene Modulvarianten innerhalb eines Aufmerksamkeitsblocks unterscheiden. Aufmerksamkeitsmodule erhalten als Eingabedaten immer drei Tensoren. Diese werden Query, Key und Value genannt. Aus diesen berechnet ein Aufmerksamkeitsmodul eine Ausgabe.\n",
    "Typischerweise werden Aufmerksamkeitsmodule dabei unterschieden aus welcher Quelle Query, Key und Value stammen. Es gibt Self-Attention bei der Query, Key und Target alle aus einer Quelle stammen, Source-Attention (Hier ist der Query aus einer anderen Quelle als Key und Value, z.B. wenn der Query die Ausgabe eines Encoders ist, während Target die Ausgabe eines Decoderblocks ist).\n",
    "Desweiteren kann man nach Art des Maskings unterscheiden. Dieses geschieht um  das Aufmerksamkeitsmodul daran zu hindern aus einem Teil der Ausgabedaten zu lernen (z.B. möchte man bei sequentieller Datenverarbeitung verhindern, dass ein Decoderaufmerksamkeitsblock für die Vorhersage der Position i Daten aus den Positionen i+j nutzt.)\n",
    "\n",
    "3. Ausgabeblock\n",
    "\n",
    "Die Ausgabepipeline interpretiert die Ausgaben des Aufmerksamkeitsmoduls, sodass sie in eine für menschlichen Gebrauch nützlichen Form vorliegen (typischerweise z.B. Textdaten, Bilddaten, etc.)\n",
    "\n",
    "In Abbildung 1 entspricht das Aufmerksamkeitsmodul den Prozessen innerhalb der grauen Umrandung, während die Eingabe- und Ausgabepipeline darunter bzw. darüber zu finden sind.\n",
    "\n",
    "## Input\n",
    "\n",
    "Der erste Teil eines Transformermodells besteht aus der Eingabepipeline. Diese verarbeitet die Eingabe in das Modells, z.B. die Texteingabe eines Nutzers und bereitet sie darauf vor durch wiederholte Anwendung der Aufmerksamkeitsmodule in einem Transformermodell verarbeitet zu werden. Die Aufmerksamkeitsmodule arbeiten über eine Aufmerksamkeitsmatrix, die aus der jeweiligen Eingabe eine Ausgabe berechnet. Wir müssen also aus einer Eingabe in Textform eine Vektorrepräsentation erzeugen, die alle notwendigen Informationen für ein Modell enthalten, um nützliche Vorhersagen über eine Übersetzung oder eine Textvervollständigung machen zu können.\n",
    "Hier sehen Sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4caafaa5",
   "metadata": {},
   "source": [
    "\n",
    "##### Abbildung 3\n",
    "<a href=\"#fig:input\">Hier</a> sehen sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt.\n",
    "\n",
    "<figure id=\"fig:input\" style=\"height: 700px;\">\n",
    "  <img src=\"./img/tf_input_pipeline.jpg\" style=\"height: 700px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4578ca",
   "metadata": {},
   "source": [
    "\n",
    "Wie zu erkennen ist, werden während des Training eines Transformermodells ausschließlich die Gewichte für die Einbettung in einen der Modellgröße entsprechenden Vektor mittrainiert.\n",
    "Prinzipiell besteht die Eingabepipeline aus drei Modulen:\n",
    "\n",
    "1. Tokenisierung,\n",
    "2. Einbettung,\n",
    "3. Positionelle Kodierung.\n",
    "\n",
    "Dies entspricht den drei Schritten:\n",
    "\n",
    "1. Umwandlung von Text in eine Zahlenkodierung desselben Textes,\n",
    "2. Kodierung dieser\n",
    "<span style=\"color:red\">fortführen!</span>\n",
    "\n",
    "### Tokenisierung\n",
    "\n",
    "Bei der Tokenisierung wird der Satz in Textform z.B. \"Das ist ein Testsatz.\" in einen Zahlencode verwandelt. Hierfür kommen verschieden Methoden in Frage. Einer der simpelsten Methoden ist es z.B. jedem Buchstaben eine Zahl zuzuordnen. Das führt allerdings zu einer sehr langen Kodierung. Die entscheidenden Faktoren für eine gute Kodierung sind Vollständigkeit der Kodierung, Länge des kodierten Vektors und Größe des dafür nötigen Vokabulars.\n",
    "Die Kodierung mit einzelnen Buchstaben ist vollständig (man kann beliebige Zeichenkombinationen kodieren) und besitzt ein kurzes Vokabular (26 für alle Buchstaben plus alle Punktierungs und Sonderzeichen, die im Text vorkommen), allerdings ist die Länge der kodierten Vektoren groß. Andererseits könnte man ein Vokabular an Wörtern nehmen, die führt zu einer viel kürzeren Kodierung, allerdings besteht die Gefahr der Unvollständigkeit und für jedes Wort muss zur Kodierung in einem sehr großem Vokabular nachgeschlagen werden.\n",
    "Aktuelle Implementationen verwenden Optionen wie das Byte-Pair Encoding. {Cite Sennrich et al. 2016 and Gage 1994}.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b56024be",
   "metadata": {},
   "source": [
    "Den Tokenizer findet man in unserer <a href=\"#fig:transformer\">Abbildung 1</a> ganz unten und ist der erste Schritt, um eine Eingabe zu verarbeiten\n",
    "<figure id=\"fig:tokenizer\" style=\"height: 300px;\">\n",
    "  <img src=\"./img/tf_tokenizer.jpg\" style=\"height: 300px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a5e1d1",
   "metadata": {},
   "source": [
    "\n",
    "### Byte-Pair Encoding\n",
    "\n",
    "Byte-Pair Encoding nutzt ein Vokabular mit einer festgelegten Länge. In unserer Implementation des Tokenizer nutzt er ein Vokabular von 8000 Einheiten. Das Vokabular wird folgendermaßen erstellt:\n",
    "\n",
    "  1. Ein Text, der für die Erstellung des Vokabulars verwendet wird, wird in eine Sequence von Buchstaben und Symbole zerlegt. Wortenden werden mit einem zusätzlichen Symbol kodiert.\n",
    "  2. Alle Buchstaben und Symbole werden in das Vokabular aufgenommen.\n",
    "  3. Nun wird das häufigste 2-Gramm, also zwei aufeinander folgende Symbole, gesucht, das im Text zu finden ist.\n",
    "  4. Dieses wird ins Vokabular aufgenommen und im Text durch ein einzelnes Symbol ersetzt.\n",
    "  5. Dieser Prozess wird nun wiederholt, bis die vorgegebene Länge des Vokabulars erreicht ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60193c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c63ccae0d248108dec30d9c109b8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Tokenizer test', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a21910a2dc42c594c05d6bac6b70b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run tokenizer on input', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078aa3a3c2b64a8aae3a8fd341ccfa50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_tok = widgets.Text(\n",
    "    value='Tokenizer test',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_tok = widgets.Button(description='Run tokenizer on input',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_tok = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_tok:\n",
    "        output_widget_tok.clear_output()  # clear the previous output\n",
    "        tokens = tokenizer.tokenize(input_widget_tok.value)\n",
    "        lookup = tokenizer.lookup(tokens)\n",
    "\n",
    "        VisualWrapper.display_text('Tokens die aus der Eingabe mit Byte-Pair Encoding extrahiert werden:'.rjust(100) + ', '.join([token.decode('utf-8').rjust(10) for token in lookup.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "        VisualWrapper.display_text('Ihre Positionsnummer im Alphabet des Byte-Pair Encoding Algorithmus:'.rjust(100) + ', '.join([str(token).rjust(10) for token in tokens.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "\n",
    "button_widget_tok.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_tok, button_widget_tok, output_widget_tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f9693a5",
   "metadata": {},
   "source": [
    "In our test example you can see, how the input string is separated into tokens and then converted into numerical values, depending on the position the token has in our vocabulary.\n",
    "As you can see our byte-pair encoding vocabulary is extended by an [START] and [END] token, and it also contains elements of type 'abc##' or '##abc'. These represent a sequence at the start or end of a word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e5b84a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Abbildung 4\n",
    "\n",
    "\t<span style=\"color:red\">\n",
    "  Tokenizer\n",
    "  </span>\n",
    "\n",
    "Gage - A New Algorithm for Data Compression.pdf\n",
    "Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf\n",
    "<span style=\"color:red\">\n",
    "In our test example you can see, how the input string is separated into tokens and then converted into numerical values, depending on the position the token has in our vocabulary.\n",
    "As you can see our byte-pair encoding vocabulary is extended by an [START] and [END] token, and it also contains elements of type 'abc##' or '##abc'. These represent a sequence at the start or end of a word.\n",
    "</span>\n",
    "\n",
    "### Input Embedding\n",
    "\n",
    "Die Einbettung sorgt dafür, dass die beliebig lange Sequenzen die durch den Tokenizer entsteht in einen Vektor der Modellgröße d_model kodiert werden. Das heißt jedes Token, das zuvor durch eine Zahl kodiert wurde, die der Position entspricht, die das jeweilige Token in einem (in unserer Implementierung 8000 Wörter langen) Tokenwörterbuch zugewiesen bekommen hat, wird nun durch einen Vektor der Länge d_model kodiert. So entsteht ein Tensor der Dimension Anzahl enkodierte Tokens d_model. Die Einbettung ist Teil der vom Modell gelernten Parameter, wie man in der Übersichtsgrafik sehen kann.\n",
    "Wie man in <a href=\"#fig:embedding\">Abbildung 4</a> sehen kann besitzt die Einbettung trainierbare Gewichte und ist damit Teil der während des Training gelernten Parameter des Modells.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab2ec3f9",
   "metadata": {},
   "source": [
    "\n",
    "##### Abbildung 5\n",
    "\n",
    "Wie man in <a href=\"#fig:embedding\">Abbildung 4</a> sehen kann besitzt die Einbettung trainierbare Gewichte und ist damit Teil der während des Training gelernten Parameter des Modells.\n",
    "<figure id=\"fig:embedding\" style=\"height: 300px;\">\n",
    "  <img src=\"./img/tf_embedding.jpg\" style=\"height: 300px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 4: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "758a1d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f1dea6c1044e4d99ddea8de52404de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p style=\"font-size:18px; color:blue;\">Hier kannst du einen Text einbetten lassen. Wenn du die Ein…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b942d00456d4c7cb424877655fe368d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Einbettung Test', continuous_update=False, description='Ihre Eingabe:', layout=Layout(margin='0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a66f13e3a3344e1b141c273bfb885f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Einbettung erstellen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b98d5254584f96be79576ad795763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EmbeddingExample():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "        self.input_widget = widgets.Text(\n",
    "            value = 'Einbettung Test',\n",
    "            description = 'Ihre Eingabe:',\n",
    "            continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "            layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "        )\n",
    "\n",
    "        self.button_widget = widgets.Button(description='Einbettung erstellen',\n",
    "                                    layout = widgets.Layout(width='auto'))\n",
    "\n",
    "        self.output_widget = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "        self.old_context = None\n",
    "\n",
    "    def on_button_click(self, b):\n",
    "        with self.output_widget:\n",
    "            self.output_widget.clear_output()  # clear the previous output\n",
    "            VisualWrapper.reset_visualiser()\n",
    "            tokens = self.tokenizer.tokenize(self.input_widget.value)\n",
    "            input_without_eos = tokens[tf.newaxis, :, :-1]\n",
    "            context = model.model.enc_embed(input_without_eos)\n",
    "            VisualWrapper.display_text('So sieht die Einbettung der Eingabe aus.')\n",
    "            VisualWrapper.color_bar(context.to_tensor())\n",
    "\n",
    "            if self.old_context is not None:\n",
    "                padded_context, padded_old_context = self.pad_tensors(context, self.old_context)\n",
    "\n",
    "                VisualWrapper.display_text('So unterscheiden sich die alte und die neue Einbettung voneinander.')\n",
    "                context_diff = padded_context - padded_old_context\n",
    "                VisualWrapper.color_bar(context_diff)\n",
    "\n",
    "            self.old_context = context\n",
    "    \n",
    "    def pad_tensors(self, ragged_tensor1, ragged_tensor2):\n",
    "        # Convert ragged tensors to normal tensors, padding with zeros\n",
    "        tensor1 = ragged_tensor1.to_tensor()\n",
    "        tensor2 = ragged_tensor2.to_tensor()\n",
    "\n",
    "        # Calculate the shapes of the tensors\n",
    "        shape1 = tf.shape(tensor1)\n",
    "        shape2 = tf.shape(tensor2)\n",
    "\n",
    "        # Initialize a list for the target shape\n",
    "        target_shape = []\n",
    "\n",
    "        # Iterate over the dimensions of the tensors\n",
    "        for i in range(shape1.shape[0]):\n",
    "            # Append the maximum size of the dimension to the target shape\n",
    "            target_shape.append(tf.maximum(shape1[i], shape2[i]))\n",
    "\n",
    "        # Convert the target shape to a tensor\n",
    "        target_shape = tf.stack(target_shape)\n",
    "\n",
    "        # Initialize lists for the paddings of the tensors\n",
    "        paddings1 = []\n",
    "        paddings2 = []\n",
    "\n",
    "        # Iterate over the dimensions of the tensors\n",
    "        for i in range(shape1.shape[0]):\n",
    "            # Append the required padding for the dimension to the paddings\n",
    "            paddings1.append([0, target_shape[i] - shape1[i]])\n",
    "            paddings2.append([0, target_shape[i] - shape2[i]])\n",
    "\n",
    "        # Convert the paddings to tensors\n",
    "        paddings1 = tf.stack(paddings1)\n",
    "        paddings2 = tf.stack(paddings2)\n",
    "\n",
    "        # Pad the tensors to the target shape\n",
    "        tensor1_padded = tf.pad(tensor1, paddings1)\n",
    "        tensor2_padded = tf.pad(tensor2, paddings2)\n",
    "\n",
    "        return tensor1_padded, tensor2_padded\n",
    "\n",
    "emb_ex = EmbeddingExample()\n",
    "\n",
    "VisualWrapper.display_text('Hier kannst du einen Text einbetten lassen. Wenn du die Eingabe veränderst wird außerdem gezeigt, wie sich die Einbettung geändert hat.')\n",
    "\n",
    "emb_ex.button_widget.on_click(emb_ex.on_button_click)\n",
    "display(emb_ex.input_widget, emb_ex.button_widget, emb_ex.output_widget)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e70583",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Positionale Kodierung\n",
    "\n",
    "Da in der Einbettung keine Informationen über die Position der verschiedenen Worte kodiert wird, muss diese zusätzlich kodiert werden. Hierfür benutzt die Transformerarchitektur für jede Position der Einbettung (also jedes enkodierte Wort) eine Sinuskurve mit anderer Frequenz und Phase. Hier ist zu sehen, wie die positionale Kodierung für eine 2048 Vektoren lange und 512 Einträge tiefe Einbettung aussieht.\n",
    "\n",
    "Wie man erkennen kann, basiert die Kodierung darauf, dass Sinuskurven mit kurzer Frequenz eine Unterscheidung von Positionen ermöglichen, die nahe beieinander liegen, da ihre Werte für benachbarte ganze Zahlen sehr verschiedene Werte liefern. Sinuskurven mit langer Frequenz unterscheiden sich erst, wenn zwei Positionen, bzw. die ganze Zahlen, die sie repräsentieren weit voneinander entfernt liegen, dadurch können auch weiter voneinander entfernt liegende Positionen in Relation gesetzt werden. In die Einbettung werden diese verschiedenen Sinuskurven eingefügt, indem die Frequenz der hinzugefügten Sinuskurve von der Tiefe der Einbettung bestimmt wird. In unserer Grafik oben wird also jeweils eine Zeile der Einbettung an der jeweiligen Position hinzugefügt.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Interaktive Anwendung Pos Enc\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d875186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3a2462f6ba41d9bcba54a4746bf0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1025, description='length', max=2048, min=2), IntSlider(value=257, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(length=(2,2048,1), depth=(2,512,1))\n",
    "def print_pos_enc(length, depth):\n",
    "    VisualWrapper.color_bar(positional_encoding(length, depth))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389fa0d1",
   "metadata": {},
   "source": [
    "\n",
    "## Trainingsmethoden\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout ist eine Methode, die während des Trainingsprozesses eines neuronalen Netzes genutzt wird, um zu verhindern, dass die gelernte Gewichtung eines Modells in einem der Module des Modells zu sehr auf einen einzelnen Prädikator stützt. Dafür werden zwischen zwei Schritten desselben Modells, die trainierbare Gewichte enthalten eine Dropout-Layer eingefügt. Diese setzt zufällig einige der vom Modell generierten Werte auf einen vordefinierten Wert (meistens 0), um den nachfolgenden Schichten diese Informationen vorzuenthalten. Da diese Operation zufällig erfolgt, müssen die nachfolgenden Teile des Modells auf eine möglichst breite Kombination aus Merkmalen setzen, um seine Vorhersagen zu treffen. Somit kann man verhindern, dass Vorhersagen nur aufgrund eines einzigen Merkmals der vorherigen Ausgabe gemacht werden.\n",
    "Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Interaktive Anwendung Dropout\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2b358029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345b345c02ea4fec9a6efd9a7a676f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=30, continuous_update=False, description='Länge des Tensors:', max=2048, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953b83b6b22c4d3d893d9d9c497273bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=512, continuous_update=False, description='Tiefe des Tensors:', max=512, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f94e8d97794f51b19b0ea004ca6809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.1, continuous_update=False, description='Dropoutrate:', max=0.9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672e96b057e24dbe9f9273e095f8b079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Tes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93a5a1b91a244e9ae9be41a7527438b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_drop = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_drop = widgets.Text(value = 'Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test',\n",
    "                                 description = 'Ihre Eingabe:',\n",
    "                                 continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                                 layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "                                 )\n",
    "\n",
    "length_widget_drop = widgets.IntSlider(value=30,\n",
    "                           min=2,\n",
    "                           max=2048,\n",
    "                           description='Länge des Tensors:',\n",
    "                           continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                           )\n",
    "depth_widget_drop = widgets.IntSlider(value=512,\n",
    "                          min=2,\n",
    "                          max=512,\n",
    "                          description='Tiefe des Tensors:',\n",
    "                          continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                          )\n",
    "dropout_widget = widgets.FloatSlider(value=0.1,\n",
    "                              min=0,\n",
    "                              max=0.9,\n",
    "                              step=0.1,\n",
    "                              description='Dropoutrate:',\n",
    "                              continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                              )\n",
    "\n",
    "def out(length, depth, dropout, input):\n",
    "    VisualWrapper.reset_visualiser()\n",
    "    dropout_layer = layers.Dropout(dropout)\n",
    "    one_tensor = tf.ones([length, depth])\n",
    "    dropout_tensor = dropout_layer(one_tensor, training=True)\n",
    "    VisualWrapper.color_bar(dropout_tensor)\n",
    "\n",
    "    tokens = tokenizer_drop.tokenize(input)\n",
    "    input_without_eos = tokens[tf.newaxis, :, 1:-1]\n",
    "    context = model.model.enc_embed(input_without_eos)\n",
    "    context_drop = dropout_layer(context, training=True)\n",
    "    VisualWrapper.display_text('Für normale Dropoutwerte zwischen 0 und 0.3 sieht man die Veränderungen an tatsächlichen Vektoren nur schlecht, da viele Werte eines Tensors schon nahe bei 0 liegen.')\n",
    "    VisualWrapper.color_bar(context.to_tensor())\n",
    "    VisualWrapper.color_bar(context_drop.to_tensor())\n",
    "    \n",
    "\n",
    "output_widget_dropout = widgets.interactive_output(out,\n",
    "                                                   {'length': length_widget_drop, 'depth': depth_widget_drop, 'dropout': dropout_widget, 'input': input_widget_drop}, \n",
    "                                                   )\n",
    "\n",
    "display(length_widget_drop, depth_widget_drop, dropout_widget, input_widget_drop, output_widget_dropout)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2d169f9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Normalisierung\n",
    "\n",
    "Normalisierung ist eine Technik, die von [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift] eingeführt wurde. In tiefen neuronalen Netzen, mit nicht-linearen Aktivierungsfunktionen wie der Sigmoid-Funktion g(x) = 1/(1+exp(-x)) trainiert werden gilt, das g’(x) -> 0 für |x|-> inf. Das führt dazu, dass diese Funktionen in einen Bereich geraten können, in der der Gradient für Stochastic Gradient Descent (SGD) minimal wird und man spricht vom Vanishing Gradient Problem. In tiefen neuronalen Netzen ergibt sich hierbei das Problem, dass eine Layer z = g(Wx + b) mit der Sigmoid-Funktion g versucht den Output des gesamten vorherigen Netzes x zu gewichten. Dabei hängen sowohl W als auch b von x ab. Da sich während des Trainings alle Layers des Netzes fortwährend aktualisieren, ändert sich auch der Input x fortwährend. Die Tiefe eines neuronalen Netzes erhöht die Wahrscheinlichkeit für einen Vanishing Gradient. Dieser Effekt wird von [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift] Internal Covariate Shift genannt. \n",
    "\n",
    "Ba et al. - 2016 - Layer Normalization.pdf\n",
    "1502.03167.pdf\n",
    "<span style=\"color:red\">Interactive Application - In- Output of LayerNorm Comparison</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39a9d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a445e4d117148789c07aed9468d9c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Button(description='Click to proceed', style=ButtonStyle())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VisualWrapper.reset_visualiser()\n",
    "tokenizer_norm = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input=\"Test\"\n",
    "\n",
    "tensor_input = tf.convert_to_tensor(input)\n",
    "if len(tensor_input.shape) == 0:\n",
    "    tensor_input = tensor_input[tf.newaxis]\n",
    "# tokenize and encode input\n",
    "# Identify end token of the input\n",
    "tokenized_input = tokenizer_norm.tokenize(tensor_input).to_tensor()\n",
    "input_without_eos = tokenized_input[:, :-1]\n",
    "context = model.model.encode(input_without_eos, None)\n",
    "\n",
    "VisualWrapper.visualize_data(id='layer')\n",
    "# Hier gibt es ein Problem, weil man das Modell nicht ohne die Layernorm laufen lassen kann. Hierfür muss erst eine Lösung implementiert werden.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95cda063",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Skalierung mit sqr(d_k)\n",
    "\n",
    "Dasselbe Problem des Vanishing Gradient könnte innerhalb der Aufmerksamkeitsfunktion auftauchen, da hier softmax(QK^T)V berechnet wird, das Skalarprodukt QK^T mit d_model skaliert und die Softmaxfunktion\n",
    "    \n",
    "    sigma(z)_i = exp(z_i)/sum_j=1^N(exp(z_j)) für j=1,...,N\n",
    "\n",
    "somit leicht in einen saturierten Bereich mit extrem kleinen Gradienten gerät. Deshalb wird QK^T mit sqr(d_model) skaliert: softmax(QK^T/sqr(d_model)), um die Skalierung mit d_model zu minimieren.\n",
    "\n",
    "### Residuale Verbindung\n",
    "\n",
    "Die Idee für das Nutzen von Residualen Verbindungen kommt von [He et al. - 2015 - Deep Residual Learning for Image Recognition]. Die Autoren stellten fest, dass bei tiefen neuronalen Netzen (Tiefe meint hier die Anzahl an Schichten des neuronalen Netzes) sowohl die Genauigkeit während des Trainings als auch die Genauigkeit auf dem Testdatensatz mit zunehmender Tiefe schlechter wird.\n",
    "Da durch Normalisierung bereits sichergestellt ist, dass schwindende oder explodierende Gradienten kein Problem darstellen, scheitert die Optimierung der neuronalen Netze aus anderen Gründen.\n",
    "Einer der Gründe hierfür liegt vermutlich darin, dass die tieferen Schichten eines Modells zu Beginn des Trainings sehr viel stärker zur Ausgabe beitragen, als die vorhergehenden Schichten. Sie werden somit zuerst trainiert. Die weniger tiefen Schichten werden erst ausreichend trainiert, wenn in den tiefen Schichten keine Optimierung mehr möglich ist.\n",
    "Residuale Verbindungen ersetzen eine Schicht F(x) durch ihre residuale Verbindung H(x) = F(x)+x. In das Ergebnis von H(x) geht also sowohl der Output, als auch der Input von F direkt mit ein. Wendet man dieses Prinzip auf die Schichten tiefer neuronaler Netze an, sorgt das dafür, dass gleich zu Beginn, der Output der wenig tiefen Schichten relevant in den Output des gesamten Netzes einfließt, denn es gilt für das gesamte Netz N:\n",
    "N(x) = H_n(H_n-1(x)) + H_n-1(x) = H_n(H_n-1(x)) + H_n-1(H_n-2(x)) + … + H_2(H_1(x)) + H_1(x) \n",
    "Wie man in Abbildung 1 sehen kann, haben alle Aufmerksamkeitsmodule, sowie alle Feed-Forward Schichten eine residuale Verbindung.\n",
    "Hier können Sie ausprobieren, wie groß die Veränderung ist, die eine residuale Verbindung einer Feed-Forward Layer aus unserem Transformermodell hinzufügt:\n",
    "\n",
    "<span style=\"color:red\">Interaktiver Vergleich mit/ohne residuale Verbindung</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "afb96ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier soll ein interaktiver Vergleich zwischen einer Schicht ohne/mit residualen Verbindungen stehen. Das geht allerdings nur, wenn man diese deaktivieren kann. Das muss implementiert werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa40a6df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Modellschichten\n",
    "\n",
    "### Aufmerksamkeit\n",
    "\n",
    "Die Neuerung von Transformern im Vergleich zu vorangegangenen Lösungen für NMT ist es, allein auf den Mechanismus als Architektur für das Verarbeiten von Sprache zu setzen. Aufmerksamkeit wird auch schon von [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE] zur Verbesserung von RNNs zur Übersetzung von Texten verwendet.\n",
    "Der Aufmerksamkeitsmechanismus, den [Attention is all you need] beschreiben orientiert sich von der Idee dabei an einer Suchanfrage. [Buch zitieren] Die Aufmerksamkeitsschicht bekommt dabei zwei oder eigentlich drei Eingaben: den Query (Q), den Key (K) und den Value (V). In der Praxis erhalten aber Key und Value in Transformern immer dieselbe Eingabe und häufig sind Query, Key und Value sogar identisch. Aus welcher Quelle Query, Key und Value kommen unterscheidet unterschiedliche Formen von Aufmerksamkeit. So nennen wir Selbstaufmerksamkeit denjenigen Fall indem Q=K=V gilt und Kreuzaufmerksamkeit denjenigen Fall indem der Query aus der Ausgabe des Encoder besteht und Key und Value beide aus der Ausgabe eines Decoderblocks.\n",
    "\n",
    "Um zu erklären, wie Aufmerksamkeit funktioniert, sollten wir aber zunächst davon ausgehen, dass Query-, Key- und Value-Eingabe verschieden sind. Ich schreibe bewusst von der Eingabe, da in jeder Aufmerksamkeitsschicht zunächst Query-, Key- und Value-Eingabe Q_in, K_in, V_in mit Hilfe von gewichteten Matrix W^Q, W^K und W^V in Query, Key und Value Q= Q_in W^Q, K = K_in W^K, V = V_in W^V umgewandelt werden. Diese gewichteten Matrizen W^Q, W^K, W^V sind die trainierbaren Gewichte einer Aufmerksamkeitsschicht. In ihnen wird das Ergebnis der Aufmerksamkeitsschicht festgelegt, da alle nachfolgenden Prozesse deterministisch sind.\n",
    "Betrachten wir aber, was passiert, wenn Query, Key und Value durch diese Matrizen gewichtet werden.\n",
    "\n",
    "#### Aufmerksamkeitsfunktion\n",
    "\n",
    "Die Aufmerksamkeitsfunktion hat die Eingaben Q, K, V und lautet:\n",
    "\n",
    "\tAttention(Q, K, V) = softmax(QK^T/sqr(d_k)) V\n",
    "\n",
    "Es wird also zuerst das Kreuzprodukt aus Q und K gebildet. Dieses Produkt wird skaliert, wie in Skalierung mit sqr(d_k) zu lesen ist, und dann die Softmax-Funktion\n",
    "\n",
    "\tsigma(x)_i = exp(x_i)/sum_j=1^n(exp(x_j)) für i=1,...,n\n",
    "positionsweise berechnet. Nennen wir \n",
    "\n",
    "\tsoftmax(QK^T/sqr(d_k)) = Score(Q,K),\n",
    "dann ist \n",
    "\n",
    "\tAttention(Q, K, V) = Score(Q,K)*V \n",
    "eine Funktion, die V mit einem Vektor multipliziert, wobei |Score(Q,K)| = 1.\n",
    "\n",
    "<span style=\"color:red\">Hier gilt vermutlich, das softmax komponentenweise angewandt wird, was aber überprüft werden sollte, bevor wir es so aufnehmen. </span>\n",
    "Score(Q,K) gibt also an, mit welchem Anteil jeder Eintrag von V in den Ausgabevektor Attention(Q,K,V) eingehen soll. Eine gute grafische Erklärung dieser Methode findet man in [https://jalammar.github.io/illustrated-transformer/].\n",
    "Da mit Score(Q,K) nun eine Gewichtung besteht, wie stark die Ausgabe Attention(Q,K,V)_i von V_j abhängt, kann man diese als Aufmerksamkeitsgewichtung in Matrixform sehr gut darstellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afaf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79faa9cf1c94e19bbc9dd96e8988788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.', conti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1287c861f6e342a19d2b649afe4374ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Aufmerksamkeit berechnen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac3515ba6974eb68f9af3b93f082793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_attn = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "attn_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_attn = widgets.Text(\n",
    "    value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_attn = widgets.Button(description='Aufmerksamkeit berechnen',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_attn = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_attn:\n",
    "        output_widget_attn.clear_output()  # clear the previous output\n",
    "        VisualWrapper.\n",
    "        attn_model(input_widget_attn.value, interactive=True) # replace this with your function call\n",
    "        VisualWrapper.visualize_data(id='attention')\n",
    "\n",
    "        if False:\n",
    "            # Convert input to tensor if it is not already\n",
    "            # Create a dynamic tensor to store output\n",
    "            # Make sure tensor_input is 2-D\n",
    "            tensor_input = tf.convert_to_tensor(input_widget_attn.value)\n",
    "            output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "            if len(tensor_input.shape) == 0:\n",
    "            tensor_input = tensor_input[tf.newaxis]\n",
    "            # tokenize and encode input\n",
    "            # Identify end token of the input\n",
    "            tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()\n",
    "            input_without_eos = tokenized_input[:, :-1]\n",
    "            context = transformer.encode(input_without_eos, None)\n",
    "\n",
    "            # Write the input tokens (excluding the last one) to the output array\n",
    "            for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "            output_array = output_array.write(i, value)\n",
    "\n",
    "            dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "            dec_out = transformer.decode(context, None, dec_input, None)\n",
    "            \n",
    "\n",
    "button_widget_attn.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_attn, button_widget_attn, output_widget_attn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "162b450c",
   "metadata": {},
   "source": [
    "\n",
    "#### Multi-headed Aufmerksamkeit\n",
    "\n",
    "In der Praxis hat sich bewährt parallel mehrere dieser Aufmerksamkeitsmechanismen durchzuführen. Dazu werden zu Beginn h gewichteten Matrizen W_i^Q, W_i^K, W_i^V i= 1,...,h eingeführt. Diese erzeugen also h verschiedene Matrixtripel Q_i, K_i, V_i und somit ergeben sich h verschiedene Ausgaben H_i = Attention(Q_i, K_i, V_i) i=1,...,h.\n",
    "Diese werden nun zu einer einzigen Ausgabe zusammengeführt, indem wir MultiHeadAttention(Q_in,K_in,V_in) = Concat(H_1,..., H_h)W^O berechnen. Dabei ist Concat(A_1,...,A_n) das hintereinanderschreiben mehrerer Matrizen und W^O eine weitere trainierbare Matrix, die die verschiedenen Ausgaben H_1,..., H,h gewichtet.\n",
    "Deshalb sehen wir in der obigen Ausgabe auch x verschiedene Matrizen, die Score(Q_i,K_i) darstellen.\n",
    "\n",
    "#### Verschiedene Aufmerksamkeitstypen\n",
    "\n",
    "In der Architektur werden verschiedene Aufmerksamkeitstypen unterschieden. Es gibt dabei zwei Variablen die beeinflussen, welche Art von Aufmerksamkeit wir verwenden. Die erste Variable ist woher die Eingaben Q_in, K_in und V_in kommen. Die zweite Variable ist die Maskierung, die wir auf Score(Q,K) anwenden.\n",
    "\n",
    "##### Selbst-Aufmerksamkeit\n",
    "\n",
    "Die Aufmerksamkeit nennen wir Selbst-Aufmerksamkeit, wenn gilt Q_in=K_in=V_in. Wenn sich Score(Q,K) also bildlich gesprochen daraus ergibt, welche Aufmerksamkeit jede Position der Eingabe auf eine andere Position derselben Eingabe legt und diese Aufmerksamkeit auf die Eingabe selbst angewandt wird.\n",
    "\n",
    "##### Kreuz-Aufmerksamkeit\n",
    "\n",
    "Wie nennen die Aufmerksamkeit Kreuz-Aufmerksamkeit, wenn gilt Q_in |= K_in = V_in. Wenn sich Score(Q,K) also daraus ergibt, welche Aufmerksamkeit jede Position einer Eingabe Q_in auf die Positionen einer zweiten Eingabe K_in gibt und dieser Aufmerksamkeitsscore auf die zweite Eingabe angewandt wird. Dies ist zum Beispiel in der Encoder-Decoder der Fall, wenn Q_in sich aus der Ausgabe des Encoder ergibt und K_in = V_in ein Zwischenergebnis des Decoders ist.\n",
    "\n",
    "##### Maskierte Aufmerksamkeit\n",
    "\n",
    "Ein Fall von maskierter Aufmerksamkeit liegt dann vor, wenn bestimmte Werte von Score(Q,K) maskiert werden. Das ist zum Beispiel beim Subsequent Masking der Fall, hier wird Score(Q,K)i,j = -inf gesetzt für alle Eintrage j>i. Dadurch wird verhindert, dass die Ausgabe Attention(Q,K,V)_i sich auf die Werte V_j, j>i stützt. Z.B. wird während des Trainings im Decoder dadurch verhindert, dass das Model lernt Informationen aus den zukünftigen Einträgen V_j, j>i zu benutzen, um V_i vorherzusagen. Man sieht gut in der Darstellung von Score(Q,K), dass die Werte für j>i dadurch meistens nahe bei 0 liegen.\n",
    "\n",
    "### Feed-Forward\n",
    "\n",
    "<span style=\"color:red\">Evtl. Zitat zu einer Quelle über Standard FFN einfügen.</span>\n",
    "Feed-Forward Netzwerken sind die Standard Implementations eines neronalen Netzes, in der die Neuronen einer Schicht mit jedem Neuron der nachfolgenden Schicht verbunden sind. In der hier realisierten Implementation bestehen sie aus einer Inputschicht und einer Outputschicht der Größe d_model sowie einer Hidden-Layer der Größe 2048. Dies ist die Standardimplementation von Transformermodellen.\n",
    "In [FNet] wird gezeigt, dass man die Aufmerksamkeitslayers vollständig durch die lineare Fast-Fourier Transformation ersetzen kann, was zeigt, dass die Feed-Forward Layers durchaus einen erheblichen Anteil an der Interpretation des Inputs eines Transformermodells haben.\n",
    "\n",
    "### Maskieren\n",
    "\n",
    "Das Maskieren des Inputs ist eine wichtige funktionale Komponente der Transformerarchitektur. Beim Maskieren handelt es sich in Wirklichkeit um zwei Mechanismen, die zwar dieselbe Funktionsweise besitzen, aber sehr unterschiedliche Aufgaben in der Architektur besitzen. Einerseits das Subsequent Masking und das Padding Masking. Das Padding Masking stellt jediglich sicher, dass nur Positionen mit Inhalt vom Transformer verarbeitet werden, während das Subsequent Masking dafür sorgt, dass der Decoder des Transformers autoregressiv [Zitat Autoregression] ist. Das bedeutet, bei einer Vorhersage für eine Ausgabe an der Position i, soll das Modell nur Informationen aus den Eingabepositionen 1, …,  i-1 nutzen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae0c164",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\"> Interaktive Anwendung vgl. eines Tensors vor und nach dem Maskieren<span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "296d2bdb",
   "metadata": {},
   "source": [
    "\n",
    "#### Padding Masking\n",
    "\n",
    "Das Padding Masking ist notwendig, da Transformer sequentielle Daten so verarbeiten, als ob sie eine fixe Länge d_model hätten. Das geschieht deshalb, weil Transformer so lernen können jede Position der Ausgabelänge d_model parallel vorherzusagen. <span style=\"color:red\">Evtl. genauere Ausführung.</span>\n",
    "Um während des Trainings auch Daten mit einer Länge größer oder kleiner d_model zu nutzen, werden längere Sequenzen abgeschnitten und kürzere mit Nullen aufgefüllt. Diese Nullen müssen dann mit Hilfe von Padding Masking für das Training irrelevant gemacht werden. Das geschieht indem man alle Positionen die eine Null enthalten für das Modell auf -inf setzt, sodass sie beim Gradient Descent nicht berücksichtigt werden. [Zitat zu genaueren Erklärung] <span style=\"color:red\">Überprüfen ob das so stimmt!</span>\n",
    "\n",
    "#### Subsequent Masking\n",
    "\n",
    "Das Subsequent Masking benutzt die gleich Technik und setzt bestimmte Einträge innerhalb der Aufmerksamkeitslayers auf den Wert -inf. Subsequent Masking und Padding Masking werden dabei gleichzeitig angewandt.\n",
    "Einfügen einer mathematischen Beschreibung."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b154960",
   "metadata": {},
   "source": [
    "\n",
    "## Output\n",
    "\n",
    "### Log-Softmax\n",
    "\n",
    "<span style=\"color:red\">Hier fehlt noch der Inhalt</span>\n",
    "\n",
    "http://arxiv.org/abs/1608.05859\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d3cf257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9e10c0f8bd4bfd92568924751cbaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test sentence', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5720157c27164657bdc7392073c5ffa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run interactive inference', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a431febbf74280adaf25576208c7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_inf = widgets.Text(\n",
    "    value='Test sentence',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_inf = widgets.Button(description='Run interactive inference',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_inf = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_inf:\n",
    "        output_widget_inf.clear_output()  # clear the previous output\n",
    "        inference_model(input_widget_inf.value, interactive=True) # replace this with your function call\n",
    "        inference_model.print_results(visualisation=True)\n",
    "\n",
    "button_widget_inf.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_inf, button_widget_inf, output_widget_inf)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tf_simu_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
