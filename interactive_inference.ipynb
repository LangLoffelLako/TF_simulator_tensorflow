{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab5bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:21.815663900Z",
     "start_time": "2024-01-16T09:57:16.327504900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logging und Decorators\n",
    "import logging as log\n",
    "\n",
    "# Tensorflow Module\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Visualisierung und Eingabe\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact_manual, interactive, interact, VBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Backend Module\n",
    "from interactive_inference_backend import ModelLoader, StoryTokenizer, WordComplete, VisualWrapper, positional_encoding\n",
    "from interactive_inference_backend import reserved_tokens, vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff4da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.461354500Z",
     "start_time": "2024-01-16T09:57:23.186207800Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ModelLoader(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                            d_model=512,\n",
    "                            n_stacks=2,\n",
    "                            h_att=4,\n",
    "                            load_model=True,\n",
    "                            model_load_path=\"model_N2_h4_d512_t20230831-134344\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d75f32",
   "metadata": {},
   "source": [
    "<!-- Deckblatt -->\n",
    "<div style=\"text-align: center; line-height: 1.6; font-size: 16px; margin: 0 auto; display: block; max-width: 90%;\">\n",
    "\n",
    "  <img src=\"logo.svg\" alt=\"Hochschule Osnabrück Logo\" width=\"350\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "  <!--**Hochschule Osnabrück** -->\n",
    "  <p style=\"font-style: italic;\">Fakultät Management, Kultur und Technik</p>\n",
    "\n",
    "  <hr style=\"solid #000; margin: 20px 0;\">\n",
    "\n",
    "  <h1>Interaktive Erklärung der Transformerarchitektur</h1>\n",
    "  <p style=\"font-style: italic;\">Ein visuelles und interaktives Paper zur Veranschaulichung von Aufbau und Funktionsweise der Transformerarchitektur</p>\n",
    "\n",
    "  <hr style=\"solid #000; margin: 20px 0;\">\n",
    "\n",
    "  <p><strong>Autoren:</strong><br>  \n",
    "    Luis Mienhardt, Prof. Dr. Ralf Buschermöhle, Christian Minich, Thomas Adelt\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94fa9c-ba0a-4b73-92ed-2f9d2965a7ac",
   "metadata": {},
   "source": [
    "## Inhaltsverzeichnis\n",
    "- [Einleitung](#einleitung)\n",
    "    - [Disclaimer](#disclaimer)\n",
    "    - [Ziel des Artikels](#ziel-dieses-interaktiven-artikels)\n",
    "    - [Transformer: Motivation & Kernkomponenten](#kurzübersicht-transformer)\n",
    "    - [Architekturübersicht](#architekturübersicht)\n",
    "        - [Encoder-Decoder](#encoder-decoder)\n",
    "        - [Architekturvarianten](#architekturvarianten)\n",
    "        - [Architekturblöcke](#architekturblöcke)\n",
    "- [Input](#input)\n",
    "    - [Tokenization](#Tokenization)\n",
    "    - [Byte-Pair Encoding](#byte-pair-encoding)\n",
    "    - [Embedding](#embedding)\n",
    "    - [Positional Encoding](#positional-encoding)\n",
    "- [Trainingsmethoden](#trainingsmethoden)\n",
    "    - [Dropout](#dropout)\n",
    "    - [Normalisierung](#normalization)\n",
    "    - [Residual Connection](#residual-connection)\n",
    "- [Layers](#layers)\n",
    "    - [Attention](#attention)\n",
    "        - [Vorteile von Transformern](#vorteile-von-transformern)\n",
    "        - [Attention als Funktion](#attention-function)\n",
    "        - [Skalierung mit $\\sqrt{d_{model}}$](#skalierung-mit-sqrd_k)\n",
    "        - [Multi-Headed Attention](#multi-headed-attention)\n",
    "    - [Masking](#masking)\n",
    "        - [Padding Masking](#padding-masking)\n",
    "        - [Subsequent Masking](#subsequent-masking)\n",
    "    - [Attention-Mechanismen](#attention-mechanismen)\n",
    "        - [Self-Attention](#self-attention)\n",
    "        - [Cross-Attention](#cross-attention)\n",
    "        - [Masked Attention](#masked-attention)\n",
    "- [Simulation](#simulation)\n",
    "- [Bibliographie](#bibliographie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634588dd",
   "metadata": {},
   "source": [
    "## <a id=\"einleitung\"></a>Einleitung\n",
    "\n",
    "### <a id=\"disclaimer\"></a>Disclaimer\n",
    "\n",
    "Wir haben diesen Artikel für eine deutschsprachige Leserschaft verfasst. Da viele Begriffe aus dem Bereich des Machine Learning kein präzises deutsches Pendant besitzen, haben wir uns bewusst dafür entschieden, die Fachterminologie möglichst konsistent in englischer Sprache zu verwenden.\n",
    "\n",
    "Jeder Abschnitt beginnt mit einer kompakten, verständlich formulierten Einführung in einer grau hinterlegten Box, die sich an Leser:innen mit Abiturniveau richtet. Im Anschluss daran tauchen wir jeweils tiefer in die zugrunde liegenden Konzepte und Funktionsweisen ein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a793e0",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"ziel-dieses-interaktiven-artikels\"></a>Ziel des Artikels \n",
    "\n",
    "Das Ziel dieses Artikels besteht darin, die Transformerarchitektur aus Vaswani et al. [1] durch die Simulation ihrer Verarbeitungsschritte und Komponenten zu erläutern. Diese Simulation erlaubt es, interaktiv die Auswirkungen verschiedener Eingaben auf die Verarbeitungsschritte zu untersuchen und so ein schrittweises Verständnis der Gesamtverarbeitung zu entwickeln. Während sich wissenschaftliche Literatur wie der ursprüngliche Artikel [1] und darauf aufbauende wissenschaftliche Arbeiten [7, 8], Erklärartikel oder -videos [9, 10] sich oft auf einzelne Komponenten wie z.B. Attention-Blöcke konzentrieren, werden andere Elemente, die technischen Feinheiten der Architektur, wie z.B. \"<a href=\"#dropout\">Dropout</a>\" [11], \"<a href=\"#residual-connection\">Residual Connections</a>\" [12], \"<a href=\"#byte-pair-encoding\">Byte-Pair Encoding</a>\" [13], Embedding oder des \"Log-Softmax Algorithmus\" [5] oft nicht oder nur rudimentär erklärt. Genau diese Begriffe möchten wir erklären und darüber hinaus durch unsere Simulation erfahrbar machen. Sollten Sie als Manager oder als Entwickler in Betracht ziehen eine Transformerarchitektur zu nutzen, möchten wir, dass Sie am Ende dieses Artikels verstehen, wie die Technologie funktioniert, um sie implementieren zu können oder verstehen, wie jemand anderes sie implementiert hat. Wir wünschen viel Spaß beim Lesen und Simulieren.\n",
    "\n",
    "#### Voraussetzungen\n",
    "\n",
    "Trotz unseres Ziels die Transformer möglichst umfänglich zu erklären setzen wir einige Informationen ihrerseits voraus. Sie müssen keinerlei Vorwissen über die Funktionsweise von Transformern mitbringen, allerdings sollten sie dazu in der Lage sein die mathematische Theorie, sowie die Logik, die hinter der Entwicklung von Machine Learning Algorithmen zu verstehen. Spezifisch setzen wir die Begriffe und Ideen der lineare Algebra und der Wahrscheinlichkeitstheorie, die dem Machine Learning zugrundeliegen voraus. Alternativ empfehlen wir sich die Grundlagen beider Theorien im Standardwerk des Deep Learning anzueignen [5]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d44de",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"kurzübersicht-transformer\"></a>Transformer: Motivation & Kernkomponenten\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Stellen wir uns eine einfache Aussage vor: \"Die Hauptstadt von Deutschland ist ...\"  Ein Mensch liest diese Aussage, erkennt, dass es sich um eine geografische Wissensaussage handelt, und antwortet sofort mit „Berlin“. Dabei wird intuitiv auf relevante Informationen zurückgegriffen, während irrelevante ignoriert werden. Transformer-Modelle funktionieren ähnlich: Sie verarbeiten Texte, erkennen dabei Muster und Zusammenhänge und geben eine Antwort auf Basis der ihnen zur Verfügung stehenden Informationen. Ihre besondere Stärke liegt darin, „Aufmerksamkeit“ gezielt auf wichtige Stellen im Text zu richten genau wie ein Mensch, der beim Lesen erkennt, welche Wörter für die Beantwortung der Frage besonders wichtig sind.\n",
    "</div>\n",
    "Transformer-Modelle sind eine von Vaswani et al. [1] vorgeschlagene Architektur zur Verarbeitung sequenzieller Daten, also solcher, die eine bestimmte Reihenfolge aufweisen etwa Zeitreihen, Texte oder Bildfolgen. Im Vergleich zu den zuvor genutzten Architekturen wie Recurrent Neural Networks (RNN) [2, 3] die sequentielle Daten verarbeiten können oder mit Convolutional Neural Networks (CNN) [4] die eine parallele Verabeitung ermöglichen, kombinieren Transformer das Verarbeiten sequentiellen Datenstruktur mit der Möglichkeit dies parallel zu tun. Dies führt bei der Verwendung von parallel arbeitender Hardware, wie etwa GPUs (Grafikkarten mit typischerweise tausenden parallelen Recheneinheiten), oft zu einer deutlich kürzeren Trainingszeit [1, 6]. Typischerweise werden diesen Modellen Aufgaben gestellt, bei denen es darum geht, das nächste Element einer Sequenz vorherzusagen, wie beispielsweise den nächsten Datenpunkt einer Zeitreihe, das nächste Wort in einem Textabschnitt oder das nächste Bild in einem Video. Zur Veranschaulichung konzentrieren wir uns im Folgenden auf die Verarbeitung von Textdaten.\n",
    "\n",
    "Das zentrale Element von Transformern ist der sogenannte „Attention“-Block. Er bestimmt, welchen Teilen der Eingabe das Modell beim Verarbeiten besondere Aufmerksamkeit schenkt. Denn nicht jedes Wort in einem Satz trägt gleich viel Bedeutung für die Vorhersage des nächsten Worts. Das Modell verarbeitet dabei nicht ganze Wörter, sondern kleinere Einheiten, die später noch genauer beschrieben werden. Die Attention-Blöcke bestehen mathematisch gesehen aus trainierbaren Matrizen, welche die Eingabe, etwa einen Vektor, der eine Frage an einen Chatbot kodiert und darstellt, in eine Ausgabe überführen, also einen Vektor, der die passende Antwort repräsentiert. Ziel ist es, durch diese Projektion möglichst präzise Antworten zu erzeugen. Das Vorgehen ähnelt dabei dem menschlichen Lesen. Auch hier werden gezielt relevante Informationen herausgefiltert, um einen Sachverhalt zu verstehen.\n",
    "\n",
    "Beim Training mit Texten nutzen Transformer eine sogenannte Maske, die verhindert, dass das Modell bereits beim Lesen eines Wortes Informationen über zukünftige Wörter erhält, die es eigentlich vorhersagen soll. Dadurch wird ein natürlicher Lesefluss simuliert. So sieht das Modell bei dem Satz „Der Himmel ist klar und blau“ zunächst nur „Der Himmel ist“ und soll daraus das nächste Wort „klar“ vorhersagen. Die folgenden Wörter „und blau“ werden dabei maskiert, da sie zu diesem Zeitpunkt im Lesefluss noch nicht bekannt sind. Schrittweise verschiebt sich die Maske über den Text, sodass das Modell lernt, jede Vorhersage nur auf bereits gelesene Informationen zu stützen.\n",
    "\n",
    "In der sogenannten Inferenzphase – also während das Modell zur Beantwortung von Fragen oder Generierung von Text verwendet wird – funktioniert der Prozess ähnlich: Zunächst wird das erste Token der Antwort erzeugt, das dann zur Eingabe hinzugefügt wird, um das nächste Token zu generieren. Sobald das sogenannte Context-Window, also die maximale Anzahl an Tokens in der Eingabe, erreicht ist, wird das jeweils älteste Token entfernt. Dieses Context-Window bildet gewissermaßen das Gedächtnis des Modells.\n",
    "\n",
    "Im Transformer werden sowohl Eingaben als auch Ausgaben intern als Vektoren dargestellt. Wie diese Vektoren aus Text erzeugt werden, welche Rolle Attention-Blöcke, Feedforward-Netze und weitere Komponenten spielen, und wie daraus wieder lesbare Antworten entstehen, wird in den folgenden Kapiteln erläutert. Dort werden auch zentrale Konzepte wie „Embedding“, „Positional Encoding“ und „Normalisierung“ näher erklärt. Ein interaktives Transformermodell im weiteren Verlauf des Artikels ermöglicht darüber hinaus eine schrittweise Simulation dieser Prozesse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a6b568b",
   "metadata": {},
   "source": [
    "### <a id=\"architekturübersicht\"></a>Architekturübersicht\n",
    "\n",
    "#### <a id=\"encoder-decoder\"></a>Encoder-Decoder\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Wenn man ein Sprachmodell fragt: „Die Hauptstadt von Deutschland ist “, muss es nicht nur die Wörter erkennen, sondern auch deren Bedeutung erfassen und die passende Antwort – „Berlin“ – generieren. Genau hier setzt die Encoder-Decoder-Architektur an: Sie trennt das Verstehen und das Antworten in zwei spezialisierte Teilmodelle. Der Encoder übernimmt das Verstehen, der Decoder das Generieren einer sinnvollen Antwort. Diese Struktur ist der Kern vieler moderner Sprachmodelle und bildet die Grundlage für das in diesem Abschnitt beschriebene Architekturkonzept.\n",
    "</div>\n",
    "Die Encoder-Decoder-Struktur wurde von Cho et al. [14] eingeführt und ist insbesondere in der maschinellen Übersetzung weit verbreitet. Sie besteht aus den zwei vorher erwähnten trainierten Modulen: dem <a href=\"#encoder-decoder\">Encoder</a>, der die Eingabe analysiert und abstrahiert, und dem <a href=\"#encoder-decoder\">Decoder</a>, der auf Basis dieser abstrahierten Information eine Ausgabe erzeugt. Der Vorteil dieser Struktur liegt in ihrer Flexibilität: Verschiedene Encoder und Decoder lassen sich für unterschiedliche Sprachen kombinieren, um so Übersetzungen zwischen beliebigen Sprachpaaren zu ermöglichen.\n",
    "\n",
    "<figure id=\"fig:fig2\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_model_architecture.jpg\" style=\"width: auto; height: 1000px;\" alt=\"Transformer als Encoder-Decoder\"/>\n",
    "    <figcaption>Abbildung 1: Transformer als Encoder-Decoder</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "In unserer konkreten Architektur wird die Eingabe – beispielsweise ein Satz zunächst vom <a href=\"#encoder-decoder\">Encoder</a> verarbeitet. Dabei wird sie durch mehrere Transformer-Layer geleitet, die jeweils auf <a href=\"#attention\">Self-Attention</a> basieren. Das Ergebnis ist eine Vektor-Repräsentation im sogenannten Latent Space, die den semantischen Gehalt der Eingabe zusammenfasst. Diese Repräsentation ist die einzige Information, die dem <a href=\"#encoder-decoder\">Decoder</a> übergeben wird. Der Decoder erhält nicht direkt Zugriff auf den ursprünglichen Text oder die Token der Eingabe, sondern nutzt ausschließlich diese verdichtete Darstellung. \n",
    "<!--\n",
    "<i><b style=\"color:red;\">Der Decoder nutzt nicht direkt die Eingaben des Encoders, sondern nur den Latent Space des Encoders. Die weitere Eingabe in den Decoder ist der selbst von Decoder generierte Kontext. Bitte einmal in Text und Grafik differenzieren.</b></i>\n",
    "-->\n",
    "Der <a href=\"#encoder-decoder\">Decoder</a> generiert seine Ausgabe sequenziell. \n",
    "Dabei greift er zum einen auf den vom Encoder erzeugten Latent Space zurück über sogenannte <a href=\"#attention\">Cross-Attention</a>-Mechanismen –, zum anderen aber auch auf den bereits erzeugten Text, den er selbst in vorherigen Schritten produziert hat. Dieser eigene Kontext wird intern mithilfe von <a href=\"#attention\">Self-Attention</a> verarbeitet. Der entscheidende Punkt ist: Der Decoder kombiniert seine eigenen bisherigen Ausgaben mit der kodierten Repräsentation des Encoders, ohne direkten Zugriff auf den ursprünglichen Input.\n",
    "\n",
    "Wie diese Prozesse ablaufen, zeigt <a href=\"#fig:fig1\">Abbildung 1</a>: Die hellblau markierten Bereiche umfassen den <a href=\"#encoder-decoder\">Encoder</a>, bestehend aus mehreren identischen Schichten, die alle auf <a href=\"#attention\">Self-Attention</a> basieren. Die dunkelblauen Bereiche zeigen den <a href=\"#encoder-decoder\">Decoder</a>, der pro Schicht zwei zentrale Komponenten enthält – <a href=\"#attention\">Self-Attention</a> und <a href=\"#attention\">Cross-Attention</a>. Die Cross-Attention-Module kombinieren jeweils den Latent Space aus dem Encoder (Source Input) mit der internen Repräsentation des Decoders (Target Input), also mit dem bereits generierten Text. Dadurch entsteht eine gezielte Informationsverschmelzung zwischen semantischem Kontext und sprachlicher Ausgabe.\n",
    "\n",
    "Die Nutzereingabe wird in der sogenannten Input-Pipeline vorbereitet. Diese besteht zunächst aus einem Tokenizer, der die Eingabe in einzelne Tokens zerlegt. Anschließend erfolgt ein trainierbares Input Embedding, welches die Tokens in Vektoren übersetzt. Diese werden durch das <a href=\"#positional-encoding\">Positional Encoding</a> um Positionsinformationen ergänzt, damit das Modell Wortreihenfolgen erfassen kann. Erst danach gelangen sie in die Transformer-Layer des Encoders oder Decoders. \n",
    "\n",
    "Alle Elemente dieser Architektur sind in <a href=\"#fig:fig1\">Abbildung 1</a> übersichtlich dargestellt. Die grauen Bereiche kennzeichnen Eingabe- und Ausgabekomponenten sowie Komponenten mit fixierten Hyperparametern, etwa den fest eingestellten <a href=\"#dropout\">Dropout</a>,kurzgesagt eine Wahrscheinlichkeit in der Neuronen ausfallen, um das Auswendiglernen zu vermeiden. Schwarze Elemente wie das <a href=\"#positional-encoding\">Positional Encoding</a> oder der logaritmische Softmax sind deterministisch und enthalten keine lernbaren Parameter. Gelb hinterlegte Module wie die Embedding Weights oder die Transformer-Layer enthalten trainierbare Parameter und werden im Training angepasst. Eine genauere Darstellung der Transformer-Layer findet sich in <a href=\"#fig:fig4\">Abbildung 4</a>. Die blauen Elemente repräsentieren Datenzustände innerhalb des Modells, die durch verschiedene Operationen transformiert werden. Sind zwei blaue Elemente durch eine Linie verbunden, handelt es sich um dieselben Daten in verschiedenen Verarbeitungsschritten.\n",
    "\n",
    "Ein weiterer zentraler Punkt ist die Wiederholung der Transformer-Layer. Diese Schichtenstruktur wird nicht nur einmal durchlaufen, sondern n-mal, wobei n ein Hyperparameter des Modells ist. In der Architektur-Grafik ist dies exemplarisch durch die Verkettung zweier Layer dargestellt – tatsächlich wiederholt sich dieses Prinzip mehrfach, um eine tiefere semantische Verarbeitung zu ermöglichen. Informationen fließen dabei stets in Pfeilrichtung durch das Modell – rückwärts gerichtete Pfade sind nicht vorgesehen.\n",
    "\n",
    "Durch diese klare funktionale Trennung und die Kombination von <a href=\"#attention\">Self-Attention</a> und <a href=\"#attention\">Cross-Attention</a> gelingt es dem Modell, komplexe Eingaben zu analysieren und passende Ausgaben zu generieren – selbst bei Aufgaben, die ein tiefes Verständnis sprachlicher Zusammenhänge erfordern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0d18b",
   "metadata": {},
   "source": [
    "#### <a id=\"architekturvarianten\"></a> Architekturvarianten und verwirklichte Modelle\n",
    "\n",
    "Wie schon beschrieben, können Transformer auch als reine Encoder oder reine Decoder-Architektur verwendet werden. Encoder verdichten Informationen (z.B. zur Klassifikation, Sentiment-Analyse, oder Clustering), während Decoder generativ (z.B. zur Textfortsetzung oder Bildgenerierung) eingesetzt werden, um aus einer verdichteten Repräsentation wieder Informationen zu generieren. Wie beschrieben, war die Encoder-Decoder Architektur vor allem für Aufgaben des maschinellen Übersetzens gedacht. In der aktuellen Umsetzung sieht man jedoch häufig reine Encoder oder Decoder Architekturen. Dies liegt daran, dass die Architekturkomplexität dabei geringer ist und die spezifische Encoder-Decoder Architektur in anderen Aufgabenfeldern bzgl. Anforderungen und Trainingseffizienz gleichauf liegt. In der folgenden Tabelle [23] findet sich eine Auflistung der bekanntesten Modelle nach Architekturtyp.\n",
    "\n",
    "| Encoder           | Encoder-Decoder | Decoder        |\n",
    "|-------------------|-----------------|----------------|\n",
    "| BERT, DistillBERT | T5              | GPT            |\n",
    "| RoBERTa           | BART            | GPT-2          |\n",
    "| XLM, XLM-R        | M2M-100         | GPT-3          |\n",
    "| ALBERT            | BugBird         | GPT-4          |\n",
    "| ELECTRA           |                 | GPT-Neo, GPT-J |\n",
    "| DeBERTa           |                 |                | "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "398698e4",
   "metadata": {},
   "source": [
    "Hier können Sie nun einen Beispielsatz zuerst vom Encoder kodieren lassen, um ihn dann im nächsten Schritt vom Decoder dekodieren zu lassen und damit eine Ausgabe zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798807b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.504807400Z",
     "start_time": "2024-01-16T09:57:25.462149900Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "transformer = model.model\n",
    "\n",
    "input_widget_enc_dec = widgets.Text(\n",
    "    value='Was ist die Hauptstadt von Deutschland?',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_enc = widgets.Button(description='Wende den Encoder auf die Eingabe an.',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "button_widget_dec = widgets.Button(description='Wende den Decoder auf die Eingabe an.',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_enc = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "output_widget_dec = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def encode():\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    if len(tensor_input.shape) == 0:                                           # Überprüft, ob der Eingabetensor im korrekten Format ist\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # Falls nicht, wird eine Dimension hinzufügt \n",
    "    \n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')\n",
    "    \n",
    "    context = transformer.encode(input_without_eos, None)                      # Kodierung des Inputsatzes von (Transformer-Modell)\n",
    "    return context, string_value, lookup\n",
    "\n",
    "\n",
    "def decode(): \n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)   # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "    if len(tensor_input.shape) == 0:                                           # wie bei der Encodierung\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # wie bei der Encodierung\n",
    "\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # wie bei der Encodierung\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    \n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')                      \n",
    "    context = transformer.encode(input_without_eos, None)                     \n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "                              \n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):                        # Schleife durch jedes Token des Satzes\n",
    "      output_array = output_array.write(i, value)                              # Speichern des Tokens im Output array\n",
    "\n",
    "    dec_input = output_array.concat()[tf.newaxis]                              # Output Array wird zu einem einzigen Tensor konkateniert \n",
    "                                                                               # und anschließend um eine zusätzliche Dimension erweitert\n",
    "\n",
    "    dec_out = transformer.decode(context, None, dec_input, None)               # Decoder des Transformer-Modells wird verwendet, um den dec_input-Tensor \n",
    "                                                                               # unter Verwendung des zuvor berechneten Kontexts zu decodieren.\n",
    "\n",
    "    return dec_out, string_value, lookup\n",
    "\n",
    "def on_button_click_enc(b):\n",
    "  with output_widget_enc:\n",
    "    output_widget_enc.clear_output()  # clear the previous output\n",
    "    context, tokens, lookup = encode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(context)\n",
    "\n",
    "\n",
    "def on_button_click_dec(b):\n",
    "  with output_widget_dec:\n",
    "    output_widget_dec.clear_output()\n",
    "    dec_out, tokens, lookup = decode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(dec_out)\n",
    "\n",
    "button_widget_enc.on_click(on_button_click_enc)\n",
    "button_widget_dec.on_click(on_button_click_dec)\n",
    "\n",
    "ui = widgets.VBox([\n",
    "  input_widget_enc_dec, \n",
    "  widgets.HBox([\n",
    "    widgets.VBox([\n",
    "      button_widget_enc, \n",
    "      output_widget_enc],\n",
    "      layout=widgets.Layout(width='50%')),\n",
    "    widgets.VBox([\n",
    "      button_widget_dec, \n",
    "      output_widget_dec],\n",
    "      layout=widgets.Layout(width='50%'))\n",
    "    ])\n",
    "  ])\n",
    "display(ui)\n",
    "\n",
    "\n",
    "#print('tok_out', tokenized_input)\n",
    "#print('enc_out', context)\n",
    "#print(\"dec_out\", dec_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa505ea8-2a43-43f6-936e-0d45b4c96f75",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In der obigen Beispielsimulation sind über der Grafik die verarbeiteten Tokens dargestellt. Innerhalb der Grafik sind die den Token entsprechenden Position entlang der y-Achse dargestellt. Wendet man den Encoder an, erhält man eine Enkodierung im Latent-Space, wendet man danach den Decoder an, erhalt man ebenfalls eine Kodierung im Latent-Space, allerdings stellt diese (kodierte) Wahrscheinlichkeiten für Wörter da, die mithilfe von Softmax und Generator aus dem Latent-Space extrahiert werden können. Die x-Achse repräsentiert die Tiefe der Token bzw. die Anzahl der Dimensionen des Vektors, der alle Token darstellt. In diesem Fall beträgt die Tiefe 512, was bedeutet, dass jedes Token durch 512 verschiedene Werte charakterisiert wird.\n",
    "\n",
    "Schritt-für-Schritt passiert Folgendes:\n",
    "\n",
    "Das Model erstellt Token und markiert dabei mit Hilfe der \"##\" Zeichen welche Token zusammen ein Wort bilden. Das Modell weiß also, dass ein Token das mit \"##\" beginnt zu dem vorherigen Token gehört. Im Beispiel gilt das für \"Encoder\", welches aus den Tokens 'e', '##n', '##co', '##der' besteht. Mehr hierzu gibt es im Abschnitt [Byte-Pair Encoding](#byte-pair-encoding).\n",
    "\n",
    "Schaut man sich die Wörter vor der Tokenisierung an, sieht man das alle Wörter nun kleingeschrieben werden. Durch derartige Zusammenfassungen wird die Größe des Vokabulars reduziert. Linguistische und grammatische Informationen, die aufgrund der Zusammenführung verloren zu gehen scheinen, bleiben i.d.R. durch die Position und den Kontext des Wortes im Satz erhalten, wie zum Beispiel Substantivierungen. Dies führt dazu, dass das Modell stärker auf die Position des Wortes im Satz achtet. Der Tokenizer fügt zudem ein Start- und Endtoken hinzu. Das Endtoken wird im Modell nicht weiterverwendet, während das Starttoken die Position 0 in der weiteren Verarbeitung und Visualisierung einnimmt.\n",
    "\n",
    "Diese Token werden in den Encoder gespeist, um daraus (die linke Grafik) eine abstrakte Repräsentation der Eingabe im Latent-Space des Encoders zu erzeugen.\n",
    "\n",
    "Hat mein eine Kodierung der verfügbaren Informationen im Latent-Space, kann man den Decoder nutzen, der seinerseits die Eingabe und die Encoder-Kodierung nutzt, um eine Vektor im Latent-Space zu erzeugen. Dieser stellt nun die Vorhersagen des Modells an der jeweiligen Position dar, man erhält also sowohl eine vorhersage für die (n+1)-te Position, die man nun der Eingabe anhängen kann, aber auch eine Vorhersage für die schon vorhandenen n Posititonen, die wir schon kennen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028a3317",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### <a id=\"architekturbloecke\"></a>Architekturblöcke\n",
    "\n",
    "Wir möchten nun einmal alle Blöcke der Transformerarchitektur vorstellen und zu den jeweiligen Kapiteln verlinken, die die jeweiligen Elemente genauer vorstellen.\n",
    "\n",
    "Prinzipiell lassen sich Transformer in mehrere Module einteilen: die Umwandlung der Eingabe in eine Vektorrepräsentation, die Abbildung der Eingaberepräsentation durch die Attention-Layers in einen Ausgabevektor, die Umwandlung des Ausgabevektor in eine menschenlesbare Datenform als Ausgabe.\n",
    "In <a href=\"#fig:fig1\">Abbildung 1</a> können wir die Ausgabe leicht von den anderen Elementen unterscheiden, sie besteht aus all denen Teilen die außerhalb der Umrandungen für Encoder und Decoder zu finden sind. Die Eingabe wiederum besteht aus dem sich für Encoder und Decoder gleichenden Teil der die Nutzer Input Daten in den Source und Target Input für die Transformer Layer umwandelt.\n",
    "\n",
    "1. [Eingabe](#input)\n",
    "\n",
    "Die Eingabe wandelt die Eingabedaten in eine Form, die für die Matrixtransformation genutzt werden kann. Wie diese Umwandlung aussieht unterscheidet sich für jeden Eingabedatentypen. In unserem Beispiel nutzen wir Textdaten, die wir durch <a href=\"#Tokenization\">Tokenization</a> [17] und <a href=\"#embedding\">Embedding</a> [18] in Tensoren verwandeln. \n",
    "\n",
    "- Ein Tensor ist eine mathematische Entität um multidimensionale Daten darzustellen. \n",
    "- Ein Skalar (eine einzelne Zahl) ist ein Tensor der 0. Ordnung, ein Vektor (eine eindimensionale Liste von Werten) ein Tensor der 1. Ordnung, eine Matrix ein Tensor der 2. Ordnung (eine zweidimensionale Tabelle von Zahlen) und multidimensionale Arrays von Zahlen ein Tensor höherer Ordnung. \n",
    "\n",
    "2. [Attention](#attention)\n",
    "\n",
    "Die Attention-Layers verarbeiten Daten in Tensorform und erzeugen dabei eine mathematische Abbildung, die relevante Zusammenhänge innerhalb der Eingabedaten identifiziert. Diese Abbildung legt fest, welchen Informationen bei der Weiterverarbeitung besondere Bedeutung beigemessen wird. Die resultierenden Ausgabetensoren werden anschließend von den jeweiligen Modulen genutzt, um zum Beispiel Entscheidungen über das nächste Element in einer Sequenz zu treffen.\n",
    "\n",
    "Man kann verschiedene Varianten von Attention-Layers dahingehend unterscheiden, wie sich ihre Eingabedaten zusammensetzen. Jede Attention-Layer erhält dabei drei Tensoren als Eingabe, die als Query, Key und Value bezeichnet werden. Aus diesen berechnet die Attention-Layer eine Wahrscheinlichkeitsverteilung für das nächste Token. \n",
    "\n",
    "Die Query fragt nach dem relevanten Kontext, das heißt nach Wörtern in der Umgebung, die für das aktuelle Token besonders wichtig sein könnten, und repräsentiert das Token, für das das Modell diese Kontextinformationen sucht. Der Key stellt den Zusammenhang zwischen der Query und allen anderen Token her. Der Query-Tensor wird mit allen Key-Tensoren multipliziert. Je größer das Ergebnis dieser Berechnung ist, desto relevanter ist der Zusammenhang zwischen den betreffenden Token. Der Value-Tensor gewichtet diese Zusammenhänge, um daraus die Attention Scores zu berechnen, also die Stärke der Aufmerksamkeit, die auf einzelne Token gelegt wird. Diese Gewichtung erfolgt meist durch eine Softmax-Funktion, welche die Relevanzwerte in Wahrscheinlichkeiten umwandelt.\n",
    "\n",
    "Typischerweise werden Attention-Module danach unterschieden, aus welcher Quelle Query, Key und Value stammen. Es gibt zwei zentrale Varianten:\n",
    "\n",
    "- Bei der **Self-Attention** stammen Query, Key und Value aus derselben Quelle. Jedes Token wird in Bezug zu allen anderen Token der Sequenz gesetzt, um kontextuelle Beziehungen innerhalb der Sequenz zu erfassen. Zum Beispiel kann im Satz „Die Katze jagt die Maus um das Haus“ das Wort „jagt“ für das Wort „Katze“ besonders relevant sein, weil es deren Handlung beschreibt.\n",
    "\n",
    "- Bei der **Source-Attention** oder **Cross-Attention** stammt die Query aus einer anderen Quelle als Key und Value. Zum Beispiel nutzt ein Decoder seine bisher generierte Ausgabe als Query, um das nächste Token zu berechnen. Diese Queries werden mit den Keys und Values der Encoder-Eingabe verglichen. Die Keys helfen dabei zu bestimmen, wie relevant die Informationen des Encoders für den aktuellen Query sind.\n",
    "\n",
    "Desweiteren wird nach Art des Maskings unterschieden. Masking verdeckt immer einen Teil der Daten. In Transformern gibt es folgende Arten von Masking:\n",
    "\n",
    "- \"Subsequent Masking\", verdeckt alle Token nach dem zu prognostizierenden Token im Decoder-Attention-Block, z.B. \"Diese Eingabe ist [...]\" eine maskierte Version von \"Diese Eingabe ist ab hier maskiert.\"\n",
    "- \"Padding Masking\", verdeckt sog. \"Padding Tokens\", das sind Platzhalter Tokens mit der eine Sequenz auf die Länge des Context Windows aufgefüllt wird, um eine einheitliche Länge von Token als Eingabe zu erzeugen. Wenn wir z.B. nur Sätze mit fünf Wörtern erlauben, dann ist \"Ein kurzer Satz. <i>pad</i> <i>pad</i>\" eine erlaubte Version des Satzes \"Ein kurzer Satz.\" und \"Ein kurzer Satz. [...]\" wiederum eine maskierte Variante des erlaubten Satzes.\n",
    "\n",
    "3. Ausgabe\n",
    "\n",
    "Die Ausgabe interpretiert die Daten, die das Attention-Modul erzeugt, und formt sie in eine für den menschlichen Gebrauch nützliche Form um, etwa in Textdaten oder Bilddaten. Dafür wird in Transformern oft ein logarithmischer Softmax verwendet. Diese Funktion wird auf die Tensorausgabe des letzten Attention-Blocks angewendet, um Wahrscheinlichkeitsverteilungen über mögliche Ausgabewerte zu erzeugen – zum Beispiel über alle im Transformer kodierten Token bei Texten oder über alle sogenannten Bildpatches, also Gruppen von Pixeln, bei Bilddaten. Ein Beispiel: Aus einem Rohvektor wie [–100, 800, 20] erzeugt der logarithmische Softmax eine Verteilung wie [0.001, 0.98, 0.019], die anzeigt, mit welcher Wahrscheinlichkeit jeder Ausgabewert gewählt wird.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f86373b1",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"input\"></a>Input\n",
    "\n",
    "Der erste Teil eines Transformermodells besteht aus der Eingabepipeline. Diese verarbeitet die Eingabe, z.B. die Texteingabe eines Nutzers, und bereitet sie auf die Verarbeitung in den Attention-Modulen vor. Die Attention-Module arbeiten über eine Attention-Matrix, die aus der jeweiligen Eingabe eine Ausgabe berechnet. Wir müssen also aus einer Eingabe in Textform eine Vektorrepräsentation erzeugen, die alle notwendigen Informationen für das Modell enthalten, um mithilfe von Matrixmanipulationen nützliche Vorhersagen zu machen.\n",
    "\n",
    "In <a href=\"#fig:fig2\">Abbildung 2</a> sehen sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt. \n",
    "Wie zu erkennen ist, werden in der Eingabepipeline während des Training eines Transformermodells ausschließlich die Weights für das Embedding verändert, alle anderen Funktionen sind rein deterministisch und bleiben damit vom Trainingsprozess unbeeinflusst.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4caafaa5",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig2\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_input_pipeline.jpg\" style=\"height: auto; width: 300px; height: 450px; \" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "    <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4578ca",
   "metadata": {},
   "source": [
    "Prinzipiell besteht die Eingabepipeline aus drei Modulen, die in den folgenden drei Abschnitten genauer erläutert werden:\n",
    "\n",
    "1. Die [Tokenization](#Tokenization), die den Text mithilfe eines Symbolalphabets in eine Zahlenkodierung umwandelt. So wird z.B. \"Transformer\" in die Zahlenfolge \"2 61 4334 93 6622 202 3\" umgewandelt.\n",
    "2. Das [Embedding](#embedding), welches diese Kodierung mithilfe eines trainierbaren Algorithmus in eine Vektordarstellung umwandelt. Das Embedding lernt die komplexe Struktur eines Textes so darzustellen, dass sie informativ für die nachfolgenden Module ist. Wie genau diese Umwandlung aussieht ist dabei aufgrund der stochastischen Natur von Deep Learning Modellen nur schlecht logisch nachzuvollziehen. Eine Kodierung könnte z.B. die obige Zahlenfolge \"2 61 4334 93 6622 202 3\" in eine zweidimensinalen Vektor der Form (7, 512) mit Einträgen zwischen -1 und 1 verwandeln.\n",
    "3. Das [Positional Encoding](#positional-encoding) ein mit der Transformer-Architektur eingeführter Mechanismus. Im Gegensatz zu RNNs, die die Eingabedaten sequenziell präsentiert bekommen [19], enthalten bei Transformermodellen die Eingaben keine Information zur relativen Position der Tokens. Diese fehlenden Informationen werden in diesem Schritt manuell hinzugefügt indem <a href=\"#pos-enc-formula\">Sinuskurven</a> mit verschiedener Frequenz und Phase über die Eingabedaten gelegt werden.\n",
    "\n",
    "### <a id=\"Tokenization\"></a>Tokenization\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\"> Um ein Sprachmodell mit Texten zu füttern, muss Sprache zuerst in eine maschinenlesbare Form überführt werden. Dabei geht es nicht darum, Wörter einfach zu erkennen, sondern sie so zu zerlegen, dass sie vom Modell als Zahlenfolge verarbeitet werden können. Diese Umwandlung wird als <a href=\"#Tokenization\">Tokenization</a> bezeichnet. Nehmen wir den Beispielsatz „Die Hauptstadt von Deutschland ist“. Dieser wird nicht als Ganzes, sondern schrittweise in sogenannte Tokens aufgeteilt und anschließend in Zahlen übersetzt. Ein typisches Ergebnis könnte dabei so aussehen: Die, Ha, ##u, ##pt, ##st, ##adt, von, Deut, ##sch, ##land, ist. Die Wahl der Methode, mit der das geschieht, beeinflusst Effizienz und Genauigkeit des Modells erheblich. \n",
    "</div>\n",
    "\n",
    "Eine der grundlegendsten Methoden ist die Zeichencodierung, bei der jedem Zeichen, zum Beispiel Buchstaben, Ziffern oder Satzzeichen, eine eindeutige Zahl zugeordnet wird. Dieses Verfahren ist vollständig, da sich damit beliebige Zeichenkombinationen darstellen lassen. Gleichzeitig bleibt das Vokabular klein, weil nur die einzelnen Zeichen erfasst werden müssen. Im Deutschen umfasst das zum Beispiel das Alphabet von A bis Z, Umlaute wie Ä, Ö und Ü, das Eszett, Ziffern, Satzzeichen wie Punkt, Komma und Fragezeichen sowie Sonderzeichen wie %, &, oder €.\n",
    "\n",
    "Der Nachteil: Die entstehenden Sequenzen sind sehr lang, weil jedes Zeichen einzeln kodiert wird. Dadurch steigt der Rechenaufwand erheblich. Die Berechnung innerhalb eines Transformer-Modells skaliert mit der Sequenzlänge quadratisch, also in der Größenordnung O(n²). Längere Sequenzen führen somit zu einem spürbaren Anstieg des Speicher- und Zeitbedarfs bei der Verarbeitung.\n",
    "\n",
    "Wählt man hingegen ein Vokabular auf Wortebene, werden die Sequenzen deutlich kürzer, da ganze Wörter in einem Schritt kodiert werden. Damit sinkt die Länge der Eingabesequenz, aber ein neues Problem entsteht: Die Methode ist potenziell unvollständig. Es ist nahezu unmöglich, ein Vokabular zu definieren, welches alle möglichen Wörter einer Sprache abdeckt. Um das Risiko zu minimieren, müsste das Vokabular enorm groß sein was wiederum die Modellgröße und den Speicherbedarf stark erhöht.\n",
    "\n",
    "Um beide Probleme lange Sequenzen und unvollständige Wortabdeckung – auszugleichen, haben sich gemischte Verfahren etabliert, die auf großen Korpora trainiert werden. Diese Ansätze kombinieren Zeichen- und Wortebene und lassen sich in zwei Richtungen unterteilen: Top-Down- und Bottom-Up-Verfahren. Top-Down-Methoden starten mit einem umfangreichen Wort-Vokabular, welches aus einem Korpus extrahiert wurde, und erweitern es um sinnvolle Teilworte, wenn unbekannte Wörter auftreten. Bottom-Up-Verfahren hingegen beginnen mit einem kleinen Zeichenvorrat und fügen häufig auftretende Zeichenfolgen als neue Einheiten hinzu.\n",
    "\n",
    "Die <a href=\"#encoder-decoder\">Transformer</a>-Architektur nach [1] setzt auf ein solches Bottom-Up-Verfahren, genauer gesagt auf Byte-Pair Encoding [13]. Dieses Verfahren hat sich als effizient erwiesen: Es erlaubt eine kompakte Repräsentation mit begrenztem Vokabular und gleichzeitig eine relativ kurze Sequenzlänge. So lassen sich Eingaben präzise, vollständig und effizient kodieren – ein entscheidender Schritt für jede weitere Verarbeitung im Modell.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a5e1d1",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"byte-pair-encoding\"></a>Byte-Pair Encoding\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Stellen Sie sich vor, Sie wollen ein großes Buch so speichern, dass es möglichst wenig Platz braucht, aber Sie wollen trotzdem jeden Satz später genau wieder zusammensetzen können. Das Byte Pair Encoding (kurz: BPE) hilft genau dabei. Nehmen wir als einfaches Beispiel den Satz: „Was ist die Hauptstadt von Deutschland?“ Dieser Satz wird zunächst in einzelne Buchstaben oder kleine Zeichenfolgen zerlegt. Dann schaut sich das System an, welche Zeichen besonders oft zusammen vorkommen, zum Beispiel vielleicht „st“ oder „de“. Diese Paare werden dann durch neue Symbole ersetzt, um Speicherplatz zu sparen. So entsteht Stück für Stück ein Vokabular aus oft genutzten Teilen. Am Ende können sowohl ganze Wörter als auch häufige Wortbestandteile wie „##land“ oder „##en“ im Vokabular vorkommen. Das System „lernt“ also, welche Kombinationen besonders nützlich sind, um Sprache kompakt und effizient darzustellen.\n",
    "</div>\n",
    "Das Byte-Pair Encoding Verfahren nutzt ein Vokabular mit einer festgelegten Länge, um festzulegen wann der Algorithmus terminiert. In unserer Implementation des Tokenizer nutzten wir ein Vokabular von der Länge 8000. Das Vokabular wird dabei folgendermaßen erstellt:\n",
    "\n",
    "  1. Ein Text, der für die Erstellung des Vokabulars verwendet wird, wird in eine Sequenz von Buchstaben zerlegt. Wortenden werden mit einem zusätzlichen Symbol kodiert. Z.B. wird \"Ein Satz\" in \"[start]\", \"e\", \"i\", \"n\", \"s\", \"a\", \"t\", \"z\", \"[ende]\" zerlegt.\n",
    "  2. Alle vorhandenen Symbole werden automatisch in das Vokabular aufgenommen.\n",
    "  3. Nun wird das häufigste 2-Gramm, also zwei aufeinander folgende Symbole, gesucht, das im Text zu finden ist.\n",
    "  4. Dieses wird ins Vokabular aufgenommen und im Text durch ein einzelnes Symbol ersetzt.\n",
    "  5. Dieser Prozess wird nun wiederholt bei Schritt 3 (d.h. wir suchen nun das häufigste 2-Gramm oder 3-Gramm, sobald wir 3-Gramme haben, ziehen wir die entsprechenden 4-Gramme in Betracht, u.s.w.) bis die vorgegebene Länge des Vokabulars erreicht ist.\n",
    "\n",
    "Dieser schrittweise Aufbau erlaubt es, aus einfachen Zeichen häufige Wortteile oder ganze Wörter zu lernen. Dadurch lassen sich Sprachbestandteile effizient kodieren, ohne dass die Sequenzen so lang werden wie bei der Zeichencodierung.\n",
    "\n",
    "Ein Beispiel für einen solchen Algorithmus findet sich unten und kann dort ausprobiert werden.\n",
    "\n",
    "Dabei können sowohl ganze Wörter ins Vokabular aufgenommen werden, wenn sie denn oft genug auftauchen (bespielweise werden die Worte \"a\", \"the\", \"and\" bei englischen Texte sicherlich mitaufgenommen werden), aber auch einzelne Wortteile wie z.B. \"en\", \"##ment\" oder \"##ed\" werden in diesem Vokabular sicherlich vorkommen, um seltene Kombinationen wie \"enablement\" in die Wortteile \"en\", \"##able\" und \"##ment\" zerlegen zu können oder grammatikalische Formen wie \"wanted\" zu bilden. Die Zeichenfolge \"##\" beschreibt dabei, dass hier ein anderer Wortteil anschließen muss und an dieser Stelle kein Wortende sein kann.\n",
    "\n",
    "In unserem Testbeispiel ist zu sehen, wie Ihre Eingabe in Tokens getrennt und dann in eine Kodierung umgewandelt wird, je nachdem, welche Position das Token in unserem Vokabular hat.\n",
    "Wie Sie sehen, enthält das Byte-Pair Encoding Vokabular zusätzlich auch ein [START]- und [END]-Token um den Anfang und das Ende der Ein- bzw. Ausgabe zu markieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60193c08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.516820100Z",
     "start_time": "2024-01-16T09:57:25.499286900Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_tok = widgets.Text(\n",
    "    value='Die Hauptstadt von Deutschland ist ',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_tok = widgets.Button(description='Tokenizer auf Input anwenden',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_tok = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def tokenize(input_widget_tok):\n",
    "    tokens = tokenizer.tokenize(input_widget_tok.value)                    # Erstellung der Tokens als Index für Vokabular\n",
    "    lookup = tokenizer.lookup(tokens)                                      # Abrufen der Zeichenkette des Index im Vokabular                    \n",
    "    \n",
    "    return tokens, lookup\n",
    "    \n",
    "def on_button_click(b):\n",
    "    with output_widget_tok:\n",
    "        output_widget_tok.clear_output()                                                        \n",
    "        tokens, lookup = tokenize(input_widget_tok)\n",
    "\n",
    "        VisualWrapper.display_text('Tokens die aus der Eingabe mit Byte-Pair Encoding extrahiert werden:'.rjust(100) + ', '.join([token.decode('utf-8').rjust(10) for token in lookup.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "        VisualWrapper.display_text('Ihre Positionsnummer im Alphabet des Byte-Pair Encoding Algorithmus:'.rjust(100) + ', '.join([str(token).rjust(10) for token in tokens.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "\n",
    "button_widget_tok.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_tok, button_widget_tok, output_widget_tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e5b84a",
   "metadata": {},
   "source": [
    "### <a id=\"embedding\"></a>Embedding\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Nach der <a href=\"#Tokenization\">Tokenization</a> des Satzes „Was ist die Hauptstadt von Deutschland?“ liegt dieser als Sequenz von Zahlen vor, deren Länge von der ursprünglichen Texteingabe sowie vom verwendeten Byte-Pair-Encoding abhängt. Da gleichlange Sätze je nach Tokenisierung unterschiedlich viele Tokens erzeugen können, variiert auch die Sequenzlänge. Für das Training von <a href=\"#encoder-decoder\">Transformer</a>-Modellen ist jedoch eine einheitliche Eingabelänge erforderlich, um parallele Verarbeitung zu ermöglichen.\n",
    "</div>\n",
    "Um das zu gewährleisten, werden sogenannte Padding Tokens eingeführt. Diese enthalten keine semantische Information, sondern dienen lediglich dazu, die Sequenz künstlich auf eine vordefinierte Länge zu bringen. Erst anschließend wird die tokenisierte Eingabe in das notwendige Vektorformat überführt. Dabei wird jedes numerisch kodierte Token mithilfe einer trainierbaren Gewichtsmatrix in einen Vektor fester Länge eingebettet.\n",
    "\n",
    "Diese Länge wird mit $d_{model}$ bezeichnet und entspricht der Modelldimension, also der Anzahl an Werten, mit denen ein Token im Modell intern repräsentiert wird. In vielen Standard-Implementierungen beträgt diese Dimension beispielsweise 512.\n",
    "\n",
    "Dieser Schritt wird als <a href=\"#embedding\">Embedding</a> bezeichnet. Dabei handelt es sich um eine lernbare Abbildung, die sicherstellt, dass jedes Token unabhängig von seiner ursprünglichen numerischen Form in denselben hochdimensionalen Raum projiziert wird. Der dabei entstehende Vektor bestehend aus $d_{model}$ Elementen enthält keine direkt ablesbare Struktur: Es ist nicht ersichtlich, welche Informationen in welchem Teil des Vektors gespeichert sind. Dennoch lassen sich die einzelnen Dimensionene vermutlich grob als Träger unterschiedlicher Merkmale interpretieren: etwa semantische Bedeutung, grammatikalische Rolle, etc. Meistens werden, wie auch in unserer Implementation, hier noch keine Informationen zwischen den Token ausgetauscht. Das Embedding jedes einzelnen Tokens steht also alleine für sich, z.B. kann das Embedding nicht die Position des Tokens mit enkodieren.\n",
    "\n",
    "Das <a href=\"#embedding\">Embedding</a> ist ein zentraler Bestandteil der Modellparameter und wird während des Trainings kontinuierlich angepasst. Es gehört damit zu den nicht-deterministischen Komponenten des Modells. In <a href=\"#fig:embedding\">Abbildung 3</a> ist dargestellt, wie die Zahlenfolge aus der Tokenization durch das trainierbare Embedding in Vektoren umgewandelt wird – ein essenzieller Schritt, um die Eingabe für die nachfolgenden Schichten des Modells nutzbar zu machen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab2ec3f9",
   "metadata": {},
   "source": [
    "<figure id=\"fig:embedding\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_embedding.jpg\" style=\"max-width: 25%; max-height: 150vh; height: auto;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "    <figcaption>Abbildung 3: Gewichte der Eingabepipeline</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b34b4d",
   "metadata": {},
   "source": [
    "Wie ein solches Embedding aussieht und wie es sich verändert, wenn man beispielsweise neue Teile an den Satz anfügt können Sie in der nachfolgenden Simulation ausprobieren. Der Eingabetext wird erst vom Tokenizer in Tokens umgewandelt und dann durch das Embedding in einen Tensor.\n",
    "\n",
    "An jeder Position (vertikal dargestellt) ist dann das Embedding des Tokens an dieser Stelle zu sehen (horizontal dargestellt). Die farbliche Kodierung stellt dabei das Zahlenspektrum dar, indem sich die Vektoreinträge bewegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a1d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.563139100Z",
     "start_time": "2024-01-16T09:57:25.524821600Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingExample():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "        self.input_widget = widgets.Text(\n",
    "            value = 'Die Hauptstadt von Deutschland ist ',\n",
    "            description = 'Ihre Eingabe:',\n",
    "            continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "            layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "        )\n",
    "\n",
    "        self.button_widget = widgets.Button(description='Neue Einbettung erstellen',\n",
    "                                    layout = widgets.Layout(width='auto'))\n",
    "\n",
    "        self.output_widget = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "        self.old_context = None\n",
    "\n",
    "    def create_tokenized_embeddings(self):\n",
    "        tokens = self.tokenizer.tokenize(self.input_widget.value)                                 # Tokenisierung der Eingabe\n",
    "        tokens_all = tokens[tf.newaxis, :, :]                                                     # Hinzufügen einer weiteren Dimension\n",
    "        input_without_eos = tokens[tf.newaxis, :, :-1]                                            # Auswahl der Tokens bis zum [END] Token\n",
    "        token_input = self.tokenizer.detokenize(tokens_all)                                       # Nur zur Ausgabe Zwecken\n",
    "        string_value = token_input.numpy()[0][0].decode('utf-8')                                  # Nur zur Ausgabe Zwecken\n",
    "        lookup = tokenizer.lookup(input_without_eos)                                              # Nur zur Ausgabe Zwecken\n",
    "        lookup = [item.decode('utf-8') for sublist in lookup.numpy()[0] for item in sublist]      # Nur zur Ausgabe Zwecken\n",
    "        print(\"Wörter: \", string_value)\n",
    "        print(\"Tokens: \", lookup)\n",
    "        context = model.model.enc_embed(input_without_eos)                                        # Erstellung des Kontext Embedding \n",
    "        VisualWrapper.display_text('So sieht die Einbettung der Eingabe aus.')\n",
    "        VisualWrapper.color_bar(context.to_tensor())\n",
    "        if self.old_context is not None:\n",
    "             padded_context, padded_old_context = self.pad_tensors(context, self.old_context)     # Erstellung des Padding Vektors der Eingaben\n",
    "             VisualWrapper.display_text('So unterscheiden sich die alte und die neue Einbettung voneinander.')\n",
    "             context_diff = padded_context - padded_old_context                                   # Berechnung der Unterschiede beider Vektoren\n",
    "             VisualWrapper.color_bar(context_diff)\n",
    "\n",
    "        self.old_context = context\n",
    "\n",
    "    \n",
    "    def on_button_click(self, b):\n",
    "        with self.output_widget:\n",
    "            self.output_widget.clear_output()  # clear the previous output\n",
    "            VisualWrapper.reset_visualiser()\n",
    "            self.create_tokenized_embeddings()\n",
    "    \n",
    "    def pad_tensors(self, ragged_tensor1, ragged_tensor2):\n",
    "        \"\"\"Funktion um die Tensoren der Eingabe auf die gleiche Länge zu transformieren\"\"\"\n",
    "        tensor1 = ragged_tensor1.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "        tensor2 = ragged_tensor2.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "\n",
    "        shape1 = tf.shape(tensor1)\n",
    "        shape2 = tf.shape(tensor2)\n",
    "\n",
    "        target_shape = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            target_shape.append(tf.maximum(shape1[i], shape2[i]))                                # Die maximale Größe der Dimension wird an die Zielform angehängt.\n",
    "\n",
    "        target_shape = tf.stack(target_shape)                                                    # Umwandlung der Zielform in einen Tensor\n",
    "\n",
    "\n",
    "        paddings1 = []\n",
    "        paddings2 = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            paddings1.append([0, target_shape[i] - shape1[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "            paddings2.append([0, target_shape[i] - shape2[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "\n",
    "        paddings1 = tf.stack(paddings1)                                                          # Konvertieren der Paddings in Tensoren\n",
    "        paddings2 = tf.stack(paddings2)                                                          # Konvertieren der Paddings in Tensoren\n",
    "\n",
    "        tensor1_padded = tf.pad(tensor1, paddings1)                                              # Tensoren an die Zielform anpassen\n",
    "        tensor2_padded = tf.pad(tensor2, paddings2)                                              # Tensoren an die Zielform anpassen\n",
    "\n",
    "        return tensor1_padded, tensor2_padded\n",
    "\n",
    "emb_ex = EmbeddingExample()\n",
    "\n",
    "VisualWrapper.display_text('Hier können Sie einen Text einbetten lassen. Wenn du die Eingabe veränderst wird außerdem gezeigt, wie sich die Einbettung geändert hat.')\n",
    "\n",
    "emb_ex.button_widget.on_click(emb_ex.on_button_click)\n",
    "display(emb_ex.input_widget, emb_ex.button_widget, emb_ex.output_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef7ac5-da74-40c0-be78-f0f866ca9fd9",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesen Beispiel ist zu sehen, wie die Werteverteilung eines Embeddings grafisch dargestellt werden kann.\n",
    "\n",
    "*Codeerläuterung:* Das Embedding wird in dem von uns implementierten [Code](https://github.com/LangLoffelLako/TF_simulator_tensorflow/blob/main/interactive_inference.ipynb) der Funktion *create_tokenized_embeddings()* erstellt. Dazu wird zu erst der Eingabetext vom Tokenizer in Tokens unterteilt (Zeile 20). Die Tokens können Sie über der Grafik sehen. In Zeile 29 werden diese dann vom Transformer Modell in die Embeddings umgewandelt.\n",
    "\n",
    "<p>Beim Vergleich der beiden Sätze „Die Hauptstadt von Deutschland ist“ und „Die Hauptstadt von Frankreich ist“ fällt auf, dass im zweiten Satz also dem mit „Frankreich“ drei zusätzliche Token entstehen. Diese Unterschiede verdeutlichen, wie sich bereits kleine Änderungen im Text auf die Tokenisierung und das Positional Encoding auswirken können.</p>\n",
    "\n",
    "<p>Die Tokenisierung des Satzes „Die Hauptstadt von Deutschland ist“ ergibt insgesamt 15 Tokens: <code>[START] die ha ##up ##ts ##ta ##d ##t von de ##uts ##ch ##land is ##t</code>. Hier sieht man gut, dass Wörter wie „Hauptstadt“ und „Deutschland“ in kleinere Bestandteile zerlegt werden. Dieses Vorgehen nennt man subword-basierte Tokenisierung.</p>\n",
    "\n",
    "<p>Im französischen Satz kommen zusätzlich die Token <span style=\"background-color: yellow;\">#n</span>, <span style=\"background-color: yellow;\">##e</span> und <span style=\"background-color: yellow;\">##u</span> hinzu diese entstehen bei der Zerlegung von „Frankreich“. Dadurch verschiebt sich das gesamte Token-Layout des Satzes, was sich direkt auf das Positional Encoding auswirkt, also auf die Art, wie das Modell die Positionen der Tokens im Satz verarbeitet.</p>\n",
    "\n",
    "<p>Ein interessantes Muster zeigt sich im Wertebereich des Positional Encodings: In den ersten 256 Positionen bewegt sich der Wertebereich vorwiegend zwischen –1 und 2. In den Positionen von 257 bis 512 hingegen liegen die Werte meist zwischen 0 und –3. Diese Unterschiede ergeben sich aus der mathematischen Grundlage des Positional Encodings – der Sinus- und Kosinus-Funktion –, die für jede Position periodische Werte erzeugt.</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e70583",
   "metadata": {},
   "source": [
    "### Positional Encoding <a id=\"positional-encoding\"></a>\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Ein Satz wie „Die Hauptstadt von Deutschland ist Berlin“ enthält nicht nur Informationen über die Wörter selbst, sondern auch über deren Reihenfolge. Der Satz „Ist Berlin die Hauptstadt von Deutschland“ hat dieselben Wörter, bedeutet aber etwas anderes, weil die Positionen der Wörter vertauscht wurden. Für ein Sprachmodell ist es daher wichtig zu wissen, an welcher Stelle ein Wort steht. Die eingebetteten Wortvektoren (Embeddings), die ein Transformer-Modell verarbeitet, enthalten jedoch von sich aus keine Angaben zur Wortposition. Um diese Information zu ergänzen, wird das sogenannte Positional Encoding eingesetzt eine Methode, mit der die Position jedes einzelnen Wortes im Satz mathematisch kodiert wird.\n",
    "</div>\n",
    "Da im Embedding keine Informationen über die relative Position der verschiedenen Worte kodiert werden, muss diese manuell hinzugefügt werden. Hierfür verwendet die Transformerarchitektur für jede Position des Embeddings, also jedes enkodierte Token, eine veränderte Sinuskurve. Es ändern sich sowohl die Frequenz, also die Abstände der Nulldurchgänge, als auch die Phase, also die x-Werte der Nulldurchgänge. [1] Der sich ergebende Wert wird dem Embedding an der jeweiligen Stelle hinzugefügt. Die für diese Verschiebungen verwendeten Formel ist deterministisch für Position und Tiefe des Embeddings und lautet:\n",
    "\n",
    "<a id=\"pos-enc-formula\"></a>\n",
    "\n",
    "$$ PE(\\text{pos}, i) = \\begin{cases} \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), & \\text{falls } i \\text{ gerade ist} \\\\ \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), & \\text{falls } i \\text{ ungerade ist}, \\end{cases} $$\n",
    "\n",
    "wobei\\\n",
    "$\\text{pos}$ die Position des Tokens in der Sequenz ist,\\\n",
    "$i$ der Index der Positionsverschlüsselung ist,\\\n",
    "$d_{\\text{model}}$ die Dimensionalität des Embeddings ist.\n",
    "\n",
    "Durch das Positional Encoding lassen sich die verschiedenen Worte sehr gut voneinander trennen. Die Idee dahinter ist, dass die grobe Position eines Wortes anhand der langfrequenten Sinuskurven bestimmt werden kann, da sie sich über die gesamte Länge der Eingabe nur allmählich verändern und die Werte des Embeddings insgesamt in eine bestimmte Richtung verschieben. Beispielsweise besitzen die Worte im hinteren Teil der Eingabe größere Werte als die im vorderen Teil der Eingabe. Dies ist in der untenstehenden Simulation an großflächigen Rot- und Grünverschiebungen zu erkennen. die genaue Position durch die hochfrequenten Sinuskurven bestimmt werden kann, da diese sich bereits für benachbarte Vektoren klar unterscheiden. Dadurch wird deutlich, welches Wort an welcher Stelle im Embedding kodiert wurde. Dies entspricht den sehr chaotisch wirkenden Bereichen in der untenstehenden Simulation.\n",
    "\n",
    "In der untenstehenden Simulation ist zu sehen, wie das Positional Encoding beispielhaft für ein 1024 x 257 langes und tiefes Embedding aussieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d875186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.823595Z",
     "start_time": "2024-01-16T09:57:25.551970Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact(\n",
    "    length=widgets.IntSlider(\n",
    "        value=1024,\n",
    "        min=2,\n",
    "        max=2048,\n",
    "        description='Länge:'\n",
    "    ),\n",
    "    depth=widgets.IntSlider(\n",
    "        value=256,\n",
    "        min=1,\n",
    "        max=512,\n",
    "        description='Tiefe:'\n",
    "    )\n",
    ")\n",
    "def print_pos_enc(length, depth):\n",
    "    \"\"\"Display positional encoding visualization for given length and depth\"\"\"\n",
    "    pos_enc = positional_encoding(length, depth)\n",
    "    VisualWrapper.color_bar(pos_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb626ffc-d9b6-4181-92d3-1bf3618660b4",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "An diesem Beispiel sieht man den Effekt des Positional Encodings auf verschieden lange bzw. tiefe Embeddings. Wenn man mit den zwei Reglern spielt, sieht man, wie sich das Positional Encoding abhängig von der Länge und der Tiefe (in unserer Formel $d_{model}$) des Embeddings ändert. Erhöht man den \\\"Länge\\\"-Regler sieht man, dass sich zwar die Skalierung der y-Achse ändert und unten neue Werte hinzukommen, allerdings das Positional Encoding für die kleineren Positionen gleich bleibt. Ändert man die Tiefe des Modells, ändern wir allerding $d_{model}$ und sehen, dass es die Berechnung an allen Stellen des Encodings verändert. Die Werte des Positional Encoding sind aber generell deterministisch und somit für gegebene Tripel $(d_{model}, \\, pos, \\, i)$ immer gleich.\n",
    "\n",
    "Es ist jedoch wichtig zu beachten, dass dieses sinusbasierte Positional Encoding nicht mehr dem aktuellen Stand der Technik entspricht. Neuere Modelle verwenden alternative Methoden wie learned positional embeddings oder relative Positionierungen, die in vielen Fällen bessere Ergebnisse erzielen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389fa0d1",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"trainingsmethoden\"></a>Trainingsmethoden\n",
    "\n",
    "### <a id=\"dropout\"></a>Dropout\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Damit ein Modell nicht nur auswendig lernt, sondern wirklich versteht, wie Sprache funktioniert, wird ihm beim Üben gezielt ein Teil der Informationen vorenthalten. Angenommen, das Modell soll den Satz „Die Hauptstadt von Deutschland ist Berlin“ vervollständigen, dann könnte es sich zu sehr auf das Wort „Deutschland“ verlassen, um „Berlin“ vorherzusagen. Beim Dropout wird zufällig entschieden, bestimmte Wörter oder Merkmale auszublenden, vielleicht zum Beispiel „Deutschland“. Das zwingt das Modell, sich zusätzlich auf andere Hinweise wie die Satzstruktur oder den restlichen Kontext zu stützen. So lernt es, stabilere Vorhersagen zu machen, auch wenn einzelne Informationen fehlen. Dieses Ausblenden passiert ausschließlich während des Trainings. Beim tatsächlichen Einsatz arbeitet das Modell wieder mit dem vollständigen Input.\n",
    "</div>\n",
    "\n",
    "Dropout [11] ist eine Methode, die während des Trainingsprozesses eines neuronalen Netzes eingesetzt wird, um zu verhindern, dass das Modell sich zu stark auf einzelne Merkmale in den Trainingsdaten verlässt. Statt zu generalisieren, würde es sonst dazu neigen, spezifische Muster auswendig zu lernen. Um dem entgegenzuwirken, werden bei jedem Trainingsdurchlauf zufällig bestimmte Neuronen „ausgeschaltet“, also ignoriert. Dadurch wird das Netzwerk gezwungen, verschiedene Kombinationen von Merkmalen zu nutzen, was seine Fähigkeit zur Verallgemeinerung verbessert.\n",
    "\n",
    "Dafür wird zwischen zwei Schritten desselben Modells, eine Dropout-Layer eingefügt. Diese setzt zufällig einige der vom ersten Modellteil generierten Ausgabe auf einen vordefinierten Wert (meistens -$\\infty$), um den nachfolgenden Schichten diese Information vorzuenthalten. Da diese Operation zufällig erfolgt, müssen die nachfolgenden Teile des Modells lernen ihre Ausgabe auch ohne diese Information zu erstellen. Somit lernt das Model seine Vorhersage auf eine möglichst breite Kombination an Merkmalen aufzubauen und man verhindert, dass Vorhersagen nur aufgrund eines einzigen Merkmals der vorherigen Ausgabe gemacht werden.\n",
    "\n",
    "Ein gutes Beispiel ist das Ende eines Satzes vorherzusagen. In europäischen Sprachen wird ein Satz fast immer mit einem Punkt beendet, also ist es eine gute Strategie zu lernen, dass ein Satz durch einen Punkt beendet wird. Doch ein Modell, dass einen Punkt als einziges Merkmal eines Satzendes nutzt ist wenig robust. Wenn man an falscher Stelle einen Punkt setzt oder ihn an einem Satzende durch ein anderes Zeichen ersetzt werden die Vorhersagen des Models schlecht sein. Dabei gibt es auch andere Hinweise auf ein Satzende, z.B. das Vorkommen eines Verbs in der deutschen Sprache oder von Ort und Zeitangaben im Englischen.\n",
    "\n",
    "Um dem Modell keine Informationen vorzuenthalten, wenn es tatsächlich eingesetzt wird, ist das Dropout immer nur während des Trainings aktiv und wird danach abgeschalten, sodass während der Inferenzphase keine Informationen gelöscht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b358029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.618429300Z",
     "start_time": "2024-01-16T09:57:25.825653400Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_drop = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_drop = widgets.Text(value = 'Was ist die Hauptstadt von Deutschland?',\n",
    "                                 description = 'Ihre Eingabe:',\n",
    "                                 continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                                 layout = widgets.Layout(width='auto', margin='0px 0px 50px 0px')\n",
    "                                 )\n",
    "\n",
    "length_widget_drop = widgets.IntSlider(value=16,\n",
    "                           min=2,\n",
    "                           max=2048,\n",
    "                           description='Tensorlänge:',\n",
    "                           style = {'description_width': '100px'},\n",
    "                           continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                           )\n",
    "depth_widget_drop = widgets.IntSlider(value=512,\n",
    "                          min=2,\n",
    "                          max=512,\n",
    "                          description='Tensortiefe:',\n",
    "                          style = {'description_width': '100px'},\n",
    "                          continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                          )\n",
    "dropout_widget = widgets.FloatSlider(value=0.5,\n",
    "                              min=0,\n",
    "                              max=0.9,\n",
    "                              step=0.1,\n",
    "                              description='Dropoutrate:',\n",
    "                              style = {'description_width': '100px'},\n",
    "                              continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "def dropout_function(length, depth, dropout, input):\n",
    "\n",
    "    # Erstellung der Dropout Layer\n",
    "    dropout_layer = layers.Dropout(dropout)                                                         # Anwendung des Dropout auf die Layer\n",
    "    one_tensor = tf.ones([length, depth])                                                           # Erstellung eines Arrays aus 1-en\n",
    "    dropout_tensor = dropout_layer(one_tensor, training=True)                                       # Anwendung des Dropout auf die Layer\n",
    "\n",
    "    # Erstellung der Kontext Layer\n",
    "    tokens = tokenizer_drop.tokenize(input)                                                         # Tokenisierung des Inputs\n",
    "    input_without_eos = tokens[tf.newaxis, :, 1:-1]                                                 # Auswahl der Tokens bis zum [END] Token\n",
    "    context = model.model.enc_embed(input_without_eos)                                              # Erstellung des Embedding durch das Modell\n",
    "    context_drop = dropout_layer(context, training=True)                                            # Anwendung des Dropout auf das Embedding\n",
    "\n",
    "    return dropout_tensor, context_drop, context\n",
    "\n",
    "def out(length, depth, dropout, input):\n",
    "    VisualWrapper.reset_visualiser()                                                   \n",
    "    dropout_tensor, context_drop, context = dropout_function(length, depth, dropout, input)\n",
    "    VisualWrapper.color_bar(dropout_tensor)\n",
    "                               \n",
    "    VisualWrapper.color_bar(context.to_tensor())                     \n",
    "    VisualWrapper.color_bar(context_drop.to_tensor())\n",
    "    \n",
    "\n",
    "output_widget_dropout = widgets.interactive_output(out,\n",
    "                                                   {'length': length_widget_drop, 'depth': depth_widget_drop, 'dropout': dropout_widget, 'input': input_widget_drop}, \n",
    "                                                   )\n",
    "\n",
    "display(length_widget_drop, depth_widget_drop, dropout_widget, input_widget_drop, output_widget_dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d176e24-7e39-4e92-95d5-3d24eef076ec",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesem Beispiel wird der Effekt von Dropout auf die Layer und das Embedding dargestellt.\n",
    "\n",
    "*Codeerläuterung:* Dafür werden im von uns implementierten [Code](https://github.com/LangLoffelLako/TF_simulator_tensorflow/blob/main/interactive_inference.ipynb) in der Funktion *dropout_function()* jeweils Dropout auf die Layer und auf das Embedding angewendet. Dafür wird in Zeile 36 und 42 jeweils die jeweilige Transformation auf den beiden Objekten angewendet.\n",
    "\n",
    "Damit wird also ein gewisser Teil, welche mit dem Parameter \"Dropoutrate\" bestimmt wird, der Werte Layer bzw. des Embeddings den weiteren Verarbeitungsschritten vorenthalten. Dieser Parameter ist ein prozentualer Wert, d.h. bei einem Wert von 0.2 werden 20% der Werte vorenthalten. \n",
    "Für das Beispiel können dieses Mal die Länge des Tensors (Eingabe) und die Tiefe des Tensors bestimmt werden. Ebenso kann die Dropoutrate verändert werden.\n",
    "\n",
    "In der ersten Grafik sieht man welche Werte in einem uniformen Vektor vom Dropout verändert werden. In den beiden darauffolgenden Grafiken wird das Dropout auf den im Textfeld eingegebenen Beispieltext angewandt, nachdem er durch den Tokenizer und ein Embedding in Vektorform gebracht wurde. Die erste Grafik zeigt den vollständigen Vektor und die zweite Grafik den Vektor, der vom Dropout verändert wurde.\n",
    "Hier sieht man die ausgelassenen Positionen sehr gut. Mit dem Erhöhen der Dropout Rate, werden diese mehr. Außerdem kann man erkennen, dass sich, wenn man den Dropout erhöht, der Wertebereich ebenfalls ausweitet. Dies geschieht, da in der Tensorflow-Implementation die durch das Dropout unveränderten Werte mit $1 / (1-\\text{Dropoutrate})$ skaliert werden, um die Summe aller Werte konstant zu halten.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2d169f9",
   "metadata": {},
   "source": [
    "### <a id=\"normalization\"></a>Normalisierung\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\"> Fragt man nach der Hauptstadt von Deutschland, erwartet man immer dieselbe Antwort: Berlin. Würde sich diese Antwort ständig ändern, wäre kein zuverlässiges Lernen möglich. Genauso ist es in neuronalen Netzen: Wenn sich die Verteilung der Eingabewerte in jeder Schicht ständig verändert, wird das Training instabil. Genau das verhindert Normalisierung. Sie sorgt dafür, dass jede Schicht konsistente Eingaben erhält und damit besser lernen kann. </div>\n",
    "\n",
    "Normalisierung ist eine Technik, die von [20] eingeführt wurde. In Deep Neural Networks, die mit nicht-linearen Aktivierungsfunktionen wie der Sigmoid-Funktion\n",
    "\n",
    "$$g(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "trainiert werden gilt, dass $g'(x) \\rightarrow 0$ für $|x| \\rightarrow \\infty$. \n",
    "\n",
    "Das führt dazu, dass diese Modelle in einen Bereich geraten können, in dem $g'(x)$ sehr klein wird. Dadurch wird auch das Training durch Stochastic Gradient Descent (SGD) minimal, sodass das Training des Modells stagniert. Man spricht vom Vanishing Gradient Problem.\n",
    "\n",
    "In neuronalen Netzen ist hierbei das Problem, dass die tieferen Layer des Modells, z.B. eine Layer $z = g(Wx + b)$ mit der Sigmoid-Funktion g versucht mithilfe seiner trainierbaren Werte $W$ und $b$ den Output des gesamten vorherigen Netzes zu gewichten. Dabei werden sowohl $W$ und $b$ abhängig von vorherigen Werten $x$ trainiert und hängen somit selbst auch von $x$ ab. Da sich während des Trainings alle Layers des Netzes fortwährend aktualisieren, ändert sich auch der Input $x$ fortwährend, sodass ein Training späterer Schichten erst möglich ist, wenn die vorhergehenden sich weitgehend stabilisiert haben. \n",
    "Dieser Effekt wird von [20] Internal Covariate Shift genannt. \n",
    "\n",
    "Je tiefer das neuronale Netz, umso größer sind diese Veränderungen, da es mehr Schichten gibt, die sich verändern können. Die Tiefe einer neuronalen Netzes erhöht also die Wahrscheinlichkeit das ein Vanishing Gradient Problem auftritt und ein effektives Training frühzeitig aufhört.\n",
    "\n",
    "Transformer wie sie in [1] beschrieben sind nutzen um diesem Problem entgegenzuwirken Layer Normalization, einen Normalisierungsalgorithmus den [21] entwickelt hat. Eine Normalisierung führt dazu, dass zumindest der Wertebereich indem sich der Input $x$ aufhält während des gesamten Trainings stabil bleibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9d2b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.709361500Z",
     "start_time": "2024-01-16T09:57:26.620436400Z"
    }
   },
   "outputs": [],
   "source": [
    "VisualWrapper.reset_visualiser()\n",
    "tokenizer_norm = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input=\"Die Hauptstadt von Deutschland ist \"\n",
    "\n",
    "tensor_input = tf.convert_to_tensor(input)\n",
    "if len(tensor_input.shape) == 0:\n",
    "    tensor_input = tensor_input[tf.newaxis]\n",
    "\n",
    "tokenized_input = tokenizer_norm.tokenize(tensor_input).to_tensor()                             # Anwendung eines Tokenizers mit Normalisierung\n",
    "input_without_eos = tokenized_input[:, :-1]\n",
    "context = model.model.encode(input_without_eos, None)\n",
    "\n",
    "VisualWrapper.visualize_data(id='layer')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57791aa1",
   "metadata": {},
   "source": [
    "### <a id=\"residual-connection\"></a> Residual Connection\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Wenn ein Modell aus vielen Schichten besteht, kann es passieren, dass die unteren Schichten beim Lernen vernachlässigt werden. Um das zu verhindern, wird bei Residual Connections der ursprüngliche Input zusätzlich zum Ergebnis der Schicht weitergegeben. Das heißt: Die Schicht verändert den Input, aber das Original bleibt erhalten und fließt mit in die nächste Schicht ein. So bleibt die Information über alle Ebenen hinweg erhalten und das Modell kann stabiler und besser lernen.\n",
    "</div>\n",
    "Die Idee für das Nutzen von Residual Connections stammt aus [22]. Die Autoren stellten fest, dass bei tiefen neuronalen Netzen sowohl die Genauigkeit während des Trainings als auch auf dem Testdatensatz mit zunehmender Tiefe schlechter wird.\n",
    "\n",
    "Da durch Normalisierung bereits verhindert wird, dass das Vanishing-Gradient-Problem auftritt, liegt die Ursache für diese Verschlechterung vermutlich woanders. Eine Erklärung ist, dass zu Beginn des Trainings vor allem die oberen Schichten zur Ausgabe beitragen und daher zuerst optimiert werden. Die tieferen Schichten werden dagegen erst später ausreichend trainiert.\n",
    "\n",
    "Um sicherzustellen, dass alle Teile des Modells von Anfang an gleichermaßen zum Training beitragen, werden Residual Connections eingeführt. Dabei wird das Ergebnis einer Schicht nicht allein auf Basis ihrer Berechnung \\( F(x) \\) weitergegeben, sondern um den ursprünglichen Input \\( x \\) ergänzt. Es entsteht also eine neue Funktion:\n",
    "\n",
    "$$H(x) = F(x) + x.$$\n",
    "\n",
    "In das Ergebnis von \\( H(x) \\) geht somit sowohl der Output der Schicht als auch ihr ursprünglicher Input ein. Wird dieses Prinzip über mehrere Schichten hinweg angewendet, ergibt sich zum Beispiel für ein Netzwerk \\( N \\):\n",
    "\n",
    "$$N(x) = H_n(H_{n-1}(x)) + H_{n-1}(x) = H_n(H_{n-1}(x)) + H_{n-1}(H_{n-2}(x)) + \\dots + H_2(H_1(x)) + H_1(x)$$\n",
    "\n",
    "Das bedeutet: Auch die frühen Schichten tragen weiterhin direkt zur Gesamtausgabe des Modells bei. Dadurch verbessert sich der Informationsfluss durch das gesamte Netz und das Training wird stabiler und effizienter.\n",
    "\n",
    "Wie man in <a href=\"#fig:fig4\">Abbildung 4</a> sehen kann, haben alle Attention-Module sowie alle Feed Forward Layer in einem Transformermodell eine residuale Verbindung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO:\n",
    "# Hier fehlt noch die Implementierung der Residual Connection als Simulation.\n",
    "# In der nachfolgenden Simulation können Sie sehen, wie sich die Ausgabe einer neuronalen Schicht verändert, wenn man ihr eine Residual Connection beifügt. Der Effekt auf den gesamten Trainingsprozess lässt sich dabei natürlich nur schwer darstellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf4718db1a7166",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <a id=\"layers\"></a> Layers\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Ein Transformer besteht aus mehreren Verarbeitungsschritten, die sogenannten Layern. Jeder dieser Layer hilft dem Modell dabei, den Input besser zu verstehen. Man kann sich das vorstellen wie eine Kette von Stationen, an denen ein Satz wie „Die Hauptstadt von Deutschland ist Berlin“ immer weiter verarbeitet wird, um die Bedeutung der einzelnen Wörter und ihre Beziehungen zu erkennen. In einem Layer achtet das Modell zum Beispiel darauf, welche Wörter im Satz sich gegenseitig beeinflussen. Am Ende jedes Layers wird die gewonnene Information zusammengefasst und an die nächste Schicht weitergegeben. So baut der Transformer sein Verständnis Schritt für Schritt auf.\n",
    "</div>\n",
    "Der größte Teil der Verarbeitung findet in den Transformer Layer statt. Diese werden in <a href=\"fig:fig4\">Abbildung 4</a> detailliert dargestellt. Innerhalb der Transformer kann man die Attention Layer, einige dazwischen liegende Schritte und zuletzt eine Feed Forward Layer unterscheiden.\n",
    "\n",
    "#### Erklärung der Grafik\n",
    "\n",
    "In unserer Grafik werden die einzelnen Elemente nach ihrer Funktion und Eigenschaft farblich unterschieden:\n",
    "\n",
    "- **Blau**: Datenelemente, zum Beispiel der Source- und Target-Input  \n",
    "- **Schwarz**: Deterministische Prozesse, zum Beispiel die Matrixmultiplikation verschiedener Matrizen  \n",
    "- **Gelb**: Prozesse mit trainierbaren Parametern, zum Beispiel die Normalisierung des Outputs  \n",
    "- **Grau**: Prozesse mit Hyperparametern, zum Beispiel Dropout  \n",
    "\n",
    "Die Daten durchlaufen die Grafik entlang der Pfeile von den beiden Input-Optionen am unteren Rand bis zum Output am oberen Ende. Die Inputs können dabei auch äquivalent sein, was durch einen gestrichelten Pfeil mit Gleichheitszeichen dargestellt ist.\n",
    "\n",
    "Gestrichelte Pfeile tauchen nur in bestimmten [Attention-Mechanismen](attention-mechanismen) auf, wie weiter unten erläutert wird. So ist zum Beispiel das Masking optional.\n",
    "\n",
    "In der Grafik ist zu sehen, wie der Source- und der Target-Input, also die Eingabe und die erwartete Ausgabe des Modells, parallel verarbeitet werden. Beide Eingaben werden in Query, Key und Value umgewandelt. Query und Key werden gegebenenfalls maskiert, bevor sie mit dem Value kombiniert werden. Die Ausgaben der \\( n \\) verschiedenen Attention-Köpfe, wobei \\( n \\) ein Hyperparameter des Modells ist, werden aneinandergehängt und zu einem gemeinsamen Vektorembedding zusammengeführt. Dieses wird mit dem residualen Target-Embedding addiert und als Eingabe in die Feed-Forward-Layer übergeben. Dort wird der Output der Transformer-Layer berechnet. Das finale Ergebnis ergibt sich anschließend aus einer Kombination dieses Outputs mit dem ursprünglichen residualen Input.\n",
    "\n",
    "Wie diese Mechanismen im Detail funktionieren wird im folgenden Kapitel geklärt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e0f5d",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig4\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_layer_architecture.jpg\" style=\"width: 400px; height: 375px; height: auto; margin: auto;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "    <figcaption>\n",
    "      Abbildung 4: Transformerarchitektur <br>\n",
    "      In dieser Abbildung wird der Aufbau einer Attention Layer gezeigt. Dabei werden die verschiedenen Varianten (Self-, Cross-, Masked-Attention) parallel dargestellt (siehe gpunktierte Linien als Alternativen). Die beiden Input Embeddings werden von mehreren Attention-Köpfen parallel verarbeitet, um dann gemeinsam mit dem Target Embedding addiert von einer Feed Forward Layer zum Output Embedding der Attention Layer transformiert zu werden.\n",
    "    </figcaption>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c08fb1",
   "metadata": {},
   "source": [
    "### <a id=\"attention\"></a> Attention\n",
    "\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Fragt man nach der Hauptstadt von Deutschland, erwartet man die Antwort: Berlin. Um diese zu geben, muss man sich auf die relevanten Begriffe in der Frage konzentrieren, also zum Beispiel auf Hauptstadt und Deutschland. Genauso funktioniert der sogenannte Attention-Mechanismus in modernen Sprachmodellen wie dem Transformer. Er sorgt dafür, dass das Modell sich beim Verarbeiten eines Textes gezielt auf diejenigen Wörter konzentriert, die für die aktuelle Aufgabe besonders relevant sind. So richtet der Transformer den Fokus auf bestimmte Textstellen, um etwa eine passende Übersetzung oder Antwort zu erzeugen.\n",
    "</div>\n",
    "\n",
    "Die zentrale Neuerung von Transformern im Vergleich zu früheren Ansätzen in der Neural Machine Translation (NMT), dem ursprünglichen Einsatzgebiet dieser Architektur (siehe Abschnitt Encoder-Decoder), besteht darin, ausschließlich Attention als Mechanismus zur Sprachverarbeitung zu nutzen. Attention selbst wurde bereits zuvor von [15] zur Verbesserung rekursiver Modelle bei der maschinellen Übersetzung verwendet.\n",
    "\n",
    "Der von [1] eingeführte Attention-Mechanismus orientiert sich an der Idee, dass das Modell bei der Generierung von Ausgaben gezielt nach passenden Informationen im Eingabetext sucht. Diese Suche erfolgt über sogenannte Query-, Key- und Value-Vektoren:\n",
    "\n",
    "- den Query ($Q$)  \n",
    "- den Key ($K$)  \n",
    "- den Value ($V$)\n",
    "\n",
    "Dabei berechnet das Modell, wie ähnlich jeder Query-Vektor zu jedem Key-Vektor ist. Diese Ähnlichkeit wird meist mithilfe eines Skalarprodukts oder einer ähnlichen Funktion bestimmt. Je höher der errechnete Wert, desto größer die Relevanz des zugehörigen Value-Vektors für die aktuelle Berechnung.\n",
    "\n",
    "**Beispiel**:  \n",
    "Angenommen, wir wollen für das Token „ist“ im Satz „Die Hauptstadt von Deutschland ist Berlin“ passende Informationen finden.  \n",
    "Das Modell stellt für „ist“ einen Query-Vektor $Q$ auf und vergleicht diesen mit allen Key-Vektoren $K_1, K_2, \\dots, K_n$, die jeweils ein anderes Wort im Satz repräsentieren.  \n",
    "\n",
    "Wenn das Skalarprodukt zwischen $Q$ und $K_4$ (z. B. „Deutschland“) besonders hoch ist, bedeutet das: „Deutschland“ ist für die Interpretation von „ist“ besonders relevant. Der zugehörige Value-Vektor $V_4$ fließt entsprechend stark in das Ergebnis der Attention-Berechnung ein.\n",
    "\n",
    "Auf diese Weise lernt das Modell, sich kontextsensitiv auf die relevanten Stellen im Eingabetext zu konzentrieren – bei jedem Token neu und je nach Aufgabe unterschiedlich.\n",
    "\n",
    "In Transformern erhalten Key und Value immer dieselbe Eingabe. Häufig sind sogar Query, Key und Value identisch, was zur sogenannten **Self-Attention** führt. In diesem Fall stammen alle drei aus derselben Quelle ($Q = K = V$). Wenn dagegen der Query aus der Ausgabe des Encoders stammt und Key und Value aus dem Decoder, spricht man von **Cross-Attention** (siehe [Abbildung 4](fig:fig4)).\n",
    "\n",
    "Für das Verständnis der Funktionsweise von Attention ist es hilfreich, zunächst davon auszugehen, dass die Eingaben für Query, Key und Value unterschiedlich sind. Die tatsächlichen Eingaben, die die Attention-Layer erhält, sind zunächst Vektoren $Q'$, $K'$ und $V'$. Aus diesen werden durch Multiplikation mit jeweils trainierbaren Gewichtungsmatrizen die eigentlichen Query-, Key- und Value-Vektoren berechnet:\n",
    "\n",
    "$$Q = Q' \\times W^Q$$  \n",
    "$$K = K' \\times W^K$$  \n",
    "$$V = V' \\times W^V$$\n",
    "\n",
    "Die Matrizen $W^Q$, $W^K$, $W^V$ sind die trainierbaren Parameter der Attention-Layer. Sie bestimmen, wie die Eingaben transformiert werden. Alle weiteren Schritte innerhalb der Attention-Berechnung folgen einem deterministischen Ablauf.\n",
    "\n",
    "Eine visuelle und anschauliche Erklärung dieses Mechanismus bietet [9]. Falls verfügbar, kann hier direkt auf die entsprechende Darstellung verlinkt werden.\n",
    "\n",
    "Der Mechanismus der Attention lässt sich am ehesten mit der Datenstruktur eines **Dictionary** vergleichen, wie sie aus der Programmierung bekannt ist. Ein Dictionary besteht aus **Key-Value-Paaren**, wobei zu einem gegebenen Key der zugehörige Value abgefragt werden kann. Der Attention-Mechanismus funktioniert ähnlich, indem er anhand der Query entscheidet, auf welche Key-Value-Paare besonders geachtet werden soll. Die folgende Grafik veranschaulicht Gemeinsamkeiten und Unterschiede."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da880e3c13929c09",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<figure id=\"fig:fig_attention\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_attention.png\" style=\"max-width: 100%; max-height: 150vh; height: auto;\" alt=\"Abbildung 4: Vergleich von Attention mit Datenstruktur 'Dictionary'.\"/>\n",
    "    <figcaption>Abbildung 5: Vergleich von Attention mit einem klassischen Dictionary</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47d379a5e3fcd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Erklärung der Grafik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f91d9b-05cc-4af9-a709-a2d3dbcf785a",
   "metadata": {},
   "source": [
    "\n",
    "In beiden dargestellten Szenarien, \"Dictionary\" und \"Attention\", wird das Konzept von Key-Value-Paaren genutzt, wobei die Keys als Referenzindizes fungieren und die Values die eigentlichen zu verarbeitenden Informationen enthalten. Queries dienen in beiden Fällen dazu, relevante Daten aus diesen Paaren zu selektieren, und am Ende wird jeweils ein Output generiert, der aus den Informationen der Value-Komponenten resultiert.\n",
    "\n",
    "Jedoch gibt es markante Unterschiede zwischen den beiden Ansätzen. Im Dictionary findet eine direkte Zuordnung statt, bei der ein Query-Element einem Key zugeordnet und das zugehörige Value direkt als Output übernommen wird. Bei \"Attention\" wird hingegen eine gewichtete Kombination der Values vorgenommen, die von der Relevanz der Keys, bestimmt durch die Queries, abhängt. Dies spiegelt sich auch in der Art der Beziehungen wider: Während im \"Dictionary\" eine eindeutige 1:1-Beziehung herrscht, besteht im \"Attention\"-Mechanismus eine 1:n-Beziehung, bei der ein Query mehrere Keys beeinflusst. Dementsprechend ist die Ausgabe im \"Dictionary\" statisch und hängt ausschließlich von der direkten Übereinstimmung ab, während sie im \"Attention\"-Modell dynamisch ist und durch die berechneten Gewichtungen eine nuanciertere Informationszusammenstellung ermöglicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f0686",
   "metadata": {},
   "source": [
    "#### <a id=\"vorteile\"></a>Vorteile von Transformern\n",
    "\n",
    "Die Einführung des Attention-Mechanismus in Transformer-Architekturen hat zu bedeutenden Verbesserungen in der Verarbeitung natürlicher Sprache geführt, insbesondere im Vergleich zu den davor verwendeten rekurrenten neuronalen Netzwerken (RNNs). Einer der herausragenden Vorteile des Attention-Mechanismus ist seine Fähigkeit zur parallelen Verarbeitung von Daten. Im Gegensatz zu den sequenziellen Verarbeitungsgrenzen von RNNs ermöglicht diese Eigenschaft eine wesentlich effizientere Datenverarbeitung. Während für eine sequenzielle Verarbeitung die Komplexität von der Textlänge $n$ exponentiell abhängt $\\exp(n)$ gilt für die parallele Verarbeitung wie in Transformern nur eine lineare Abhängigkeit $a \\times n$. Das ist eine deutliche Verbesserung.\n",
    "\n",
    "Ein weiterer entscheidender Fortschritt, den der Attention-Mechanismus mit sich bringt, ist dass er bei der Verarbeitung der Daten an Position $n$ uneingeschränkt auf alle vorherige $n-1$ Daten zugreifen kann. Im Unterschied zu dem festen, oft begrenzten Gedächtnis der RNNs, das sich Daten von Position $1$ bis zur verarbeitung an Position $n$ bereits $n-1 \\text{-mal}$ merken musste, erlaubt der dynamische und kontextabhängige Speicher des Attention-Mechanismus eine umfassendere und flexiblere Berücksichtigung von Informationen. Dies ist besonders nützlich für das Verständnis und die Verarbeitung komplexer Sprachstrukturen.\n",
    "\n",
    "Besonders bemerkenswert ist auch, wie der Attention-Mechanismus die Handhabung von Langzeitabhängigkeiten verbessert. Durch die Fähigkeit, direkte Verbindungen zwischen weit auseinanderliegenden Elementen einer Sequenz herzustellen, können Transformer-Modelle effektiver mit Langzeitabhängigkeiten umgehen, was bei RNNs oft eine Herausforderung darstellt. Diese Fähigkeit verbessert das Verständnis und die Generierung von Sprache über längere Textabschnitte hinweg erheblich.\n",
    "\n",
    "Schließlich ermöglicht der Attention-Mechanismus eine verbesserte Kontextverarbeitung. Die Fähigkeit, die Bedeutung von Wörtern und Phrasen im Kontext ihres Auftretens zu erfassen, führt zu einem präziseren und tieferen Verständnis der Sprache. Diese kontextuelle Bewusstheit, die über die Fähigkeiten traditioneller RNNs hinausgeht, ist entscheidend für anspruchsvolle sprachverarbeitende Aufgaben wie z.B. Übersetzung oder Zusammenfassungen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa40a6df",
   "metadata": {},
   "source": [
    "#### <a id=\"attention-function\"></a> Attention als Funktion\n",
    "\n",
    "Die Funktion, die die Attention für uns berechnet, bekommt die Eingaben $Q$, $K$, $V$, also Query, Key und Value, die aus der Multiplikation der Eingabe mit den Gewichtsmatrizen entstanden sind. Sie lautet:\n",
    "\n",
    "$$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{\\text{model}}}}\\right)V$$\n",
    "\n",
    "Es wird also zuerst das Kreuzprodukt aus $Q$ und $K$ gebildet. Dieses Produkt wird mit $\\sqrt{d_{\\text{model}}}$ skaliert (den Grund dafür findest du im folgenden [Kapitel](#skalierung-mit-sqrd_k)), und auf dieses Ergebnis wird dann die Softmax-Funktion $\\sigma(x)$ angewandt.\n",
    "Diese Funktion lässt sich am besten positionsweise beschreiben:\n",
    "\n",
    "$$\\sigma(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^n \\exp(x_j)} \\text{ für } i=1, \\dots, n.$$\n",
    "\n",
    "Nennen wir also\n",
    "\n",
    "$$\\text{Score}(Q,K) \\coloneqq \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{model}}}\\right), $$\n",
    "\n",
    "dann ist\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{Score}(Q,K) \\times V$$\n",
    "\n",
    "eine Funktion, die $V$ mit einem Vektor multipliziert, wobei für den Vektor gilt $|\\text{Score}(Q,K)| = 1$.\n",
    "\n",
    "$\\text{Score}(Q,K)$ ist also ein Vektor exakt der Länge von $V$, der angibt, mit welchem Anteil jeder Eintrag von $V$ in den Ausgabevektor $\\text{Attention}(Q,K,V)$ eingehen soll. Dabei summiert sich $\\text{Score}(Q,K)$ zu $1$, es handelt sich also tatsächlich um eine Gewichtung der Einträge von $V$.\n",
    "\n",
    "Da mit $\\text{Score}(Q,K)$ nun also eine Gewichtung besteht, wie stark die Ausgabe $\\text{Attention}(Q,K,V)_i$ von $V_j$ abhängt, kann man dieses Verhältnis als Matrix grafisch darstellen. Dies geschieht in der Simulation unten, bei der für den vorgegebenen Satz eine Gewichtung aus einer der Attention-Layer dargestellt wird. Der Eintrag in der $i$-ten Zeile und $j$-ten Spalte gibt dabei den Einfluss von $V_j$ auf $\\text{Attention}(Q,K,V)_i$ an.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffd9c1",
   "metadata": {},
   "source": [
    "##### <a id=\"skalierung-mit-sqrd_k\"></a> Skalierung mit $\\sqrt{d_{model}}$\n",
    "\n",
    "Zuletzt sollte noch kurz erklärt werden, weshalb innerhalb der Aufmerksamkeitsfunktion $Attention$ mit dem Faktor $\\sqrt{d_{model}}$ skaliert wird. Das Problem des Vanishing Gradients kann auch innerhalb der Aufmerksamkeitsfunktion auftreten, da hier $\\text{softmax}(QK^T)V$ berechnet wird und das Skalarprodukt $QK^T$ mit $d_{model}$ skaliert.\n",
    "\n",
    "Somit gilt, dass die Softmax-Funktion leicht in einen saturierten Bereich mit extrem kleinen Gradienten gerät. Deshalb wird in der Umsetzung der Architektur $QK^T$ mit $sqr(d_{model})$ skaliert: \n",
    "\n",
    "$$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{model}}}\\right),$$\n",
    "\n",
    "und so die Skalierung der Attention mit $d_{model}$ minimiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afaf5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.743517300Z",
     "start_time": "2024-01-16T09:57:26.711359600Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_attn = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "attn_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_attn = widgets.Text(\n",
    "    value='Die Hauptstadt von Deutschland ist ',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_attn = widgets.Button(description='Embedding berechnen',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_attn = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def create_tokenized_embeddings():\n",
    "        tensor_input = tf.convert_to_tensor(input_widget_attn.value)                # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)    # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "        if len(tensor_input.shape) == 0:                                            # Überprüft, ob der Eingabetensor im korrekten Format ist                                     \n",
    "            tensor_input = tensor_input[tf.newaxis]                                 # Falls nicht, wird eine Dimension hinzufügt \n",
    "\n",
    "    \n",
    "        tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()              # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "        input_without_eos = tokenized_input[:, :-1]\n",
    "        context = transformer.encode(input_without_eos, None)                       # Erstellung der Kontext-Vektoren vom Transformer-Modell\n",
    "\n",
    "        # Write the input tokens (excluding the last one) to the output array\n",
    "        for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "            output_array = output_array.write(i, value)\n",
    "\n",
    "        dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "        dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_attn:\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 6\n",
    "        output_widget_attn.clear_output()  # clear the previous output\n",
    "        create_tokenized_embeddings()\n",
    "        VisualWrapper.visualize_data(id='attention')\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 1\n",
    "            \n",
    "\n",
    "button_widget_attn.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_attn, button_widget_attn, output_widget_attn)\n",
    "\n",
    "# TODO: In diesem Codeblock müssen noch einige Anpassungen am Text geschehen.\n",
    "# TODO: Die Aufmerksamkeitsmatrizen sind momentan 2x2 Matrizen. Hier gibt es einen Bug."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dca50aa-8c8c-49e4-955b-3c0791913f5d",
   "metadata": {},
   "source": [
    "#### <a id=\"multi-headed-attention\"></a> Multi-headed Attention\n",
    "\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Um den Mechanismus der Multi-headed Attention zu verstehen, kann man sich zunächst eine einfache Analogie vorstellen: Man stelle sich vor, eine Schülerin möchte die Hauptstadt von Deutschland herausfinden. Sie liest einen Text und richtet ihre Aufmerksamkeit gezielt auf den Satz: „Die Hauptstadt von Deutschland ist Berlin.“ Hierbei nutzt sie ihren Fokus, um genau diesen relevanten Informationskern herauszufiltern. Würde sie jedoch mehrere Fragen gleichzeitig beantworten wollen, etwa zur Geografie, Geschichte und Politik Deutschlands, müsste sie verschiedene Aspekte gleichzeitig betrachten. Genau das macht die Multi-headed Attention: Sie erlaubt einem Sprachmodell, verschiedene Informationsstränge parallel zu analysieren, so als hätte es mehrere \"Aufmerksamkeiten\" gleichzeitig im Einsatz.\n",
    "</div>\n",
    "\n",
    "Multi-headed Attention ist eine Erweiterung des Attention-Mechanismus, die darauf abzielt, die komplexen Strukturen von Texten besser zu erfassen. In herkömmlichen Modellen konkurrieren zahlreiche Beziehungen und Verbindungen innerhalb eines Textes um die Aufmerksamkeit eines einzigen Mechanismus, was zu einer Überlastung führen kann. Durch die Einführung von Multi-headed Attention wird diese Einschränkung überwunden, indem mehrere, parallel arbeitende Attention-Ströme geschaffen werden, von denen sich jeder auf unterschiedliche Aspekte des Textes konzentriert.\n",
    "\n",
    "Diese spezialisierten \"Köpfe\" können verschiedene Typen von Zusammenhängen innerhalb der Eingabedaten verarbeiten. Ein Kopf könnte sich auf die Beziehung zwischen Subjekten und Prädikaten konzentrieren, ein anderer auf die Kohärenz thematischer Elemente, und ein dritter könnte die Verbindung zwischen Haupt- und Nebensätzen analysieren. Ob das so passiert kann natürlich nicht nachgewiesen werden. Analysiert man allerdings die Aktivierungsmatrix der verschiedenen Köpfe, so kann man klare Unterschiede feststellen, sodass eine Spezialisierung anzunehmen ist. Durch diese Aufteilung wird vermieden, dass die Köpfe in Konkurrenz zueinander treten; stattdessen ergänzen sie sich, was zu einer umfassenderen Analyse führt.\n",
    "\n",
    "Die resultierenden, von jedem Kopf generierten Outputs werden anschließend zu einer einzigen, zusammenhängenden Darstellung kombiniert. Diese Synthese bietet eine reichhaltige, vielschichtige Perspektive auf die Eingabedaten, die weit über das hinausgeht, was mit einem einzigen Attention-Mechanismus möglich wäre. Multi-headed Attention ist somit ein Schlüsselelement, das die Fähigkeit von Modellen verbessert, subtile und komplexe Muster in Daten zu erkennen und darauf zu reagieren.\\\n",
    "\n",
    "Mathematisch betrachtet werden dazu zu Beginn $h$ gewichtete Matrizen \n",
    "\n",
    "$$W_i^Q, W_i^K, W_i^V \\quad i = 1, \\ldots, h$$ \n",
    "\n",
    "eingeführt. Diese erzeugen also $h$ verschiedene Matrixtripel \n",
    "\n",
    "$$Q_i = Q W_i^Q, \\quad K_i = K W_i^K, \\quad V_i = V W_i^V \\quad i = 1, \\ldots, h$$\n",
    "\n",
    "und somit ergeben sich $h$ verschiedene Ausgaben \n",
    "\n",
    "$$H_i = \\text{Attention}(Q_i, K_i, V_i) \\quad i = 1, \\ldots, h.$$\n",
    "\n",
    "Diese werden nun zu einer einzigen Ausgabe zusammengeführt, indem wir \n",
    "\n",
    "$$\\text{MultiHeadAttention}(Q,K,V) = \\text{Concat}(H_1, \\ldots, H_h) W^O$$\n",
    "\n",
    "berechnen. Dabei ist $\\text{Concat}(A_1, \\ldots, A_n)$ das Hintereinanderschreiben mehrerer Matrizen und $W^O$ eine weitere trainierbare Matrix, die die verschiedenen Ausgaben $H_1, \\ldots, H_h$ gewichtet. Deshalb sehen wir in der obigen Ausgabe auch mehrere Matrizen, die $\\text{Score}(Q_i, K_i)$ darstellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb503571-d491-4561-9e54-f245685c66fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<figure id=\"fig:fig_mhattention\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_mhattention.png\" style=\"max-width: 100%; max-height: 150vh; height: auto;\" alt=\"Abbildung x: Beispiel Multi-headed Attention.\"/>\n",
    "    <figcaption>Abbildung 6: Beispiel Multi-headed Attention</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96638e79-6c3f-4bdb-8ecd-34a293a64358",
   "metadata": {},
   "source": [
    "### Erklärung der Grafik\n",
    "\n",
    "Die Abbildung zeigt zwei Beispiele für die Visualisierung von Multi-headed Attention in einem Transformer-Modell. Jedes Diagramm repräsentiert die Aufmerksamkeitsverteilung eines eigenen \"Kopfes\" innerhalb des Attention-Mechanismus, und zwar für einen gegebenen Satz \"Ich besuchte den Kurs Digital Leadership und lernte\".\n",
    "In beiden Diagrammen sind die vertikalen Balken proportional zur Stärke der Aufmerksamkeit, die jedes Wort vom jeweiligen Kopf erhält. Ein höherer Balken bedeutet, dass das entsprechende Wort eine stärkere Aufmerksamkeit erhält, wenn das Modell versucht, die Bedeutung des Satzes zu interpretieren oder eine Aufgabe wie die Übersetzung durchzuführen.\n",
    "\n",
    "Die Wörter am unteren Rand jedes Diagramms sind die Eingabewörter, und die kleinen \"v\" und \"k\" Symbole repräsentieren die Values und Keys im Attention-Mechanismus. Das \"q\" steht für den Query-Vektor, der in diesem Fall auf das Wort \"lernte\" zeigt, was bedeutet, dass die Visualisierung die Aufmerksamkeit aus der Perspektive dieses spezifischen Wortes darstellt.\n",
    "\n",
    "Attention-Kopf 1 fokussiert sich auf die Entitäten. Hier sehen wir, dass dieser Kopf vor allem auf die Wörter \"Kurs\", \"Digital\", und \"Leadership\" Aufmerksamkeit legt. Diese Wörter sind als Entitäten (Namen von Personen, Orten oder spezifischen Objekten) identifiziert worden, was darauf hindeutet, dass dieser Kopf darauf trainiert ist, solche Entitäten im Text zu erkennen und hervorzuheben.\n",
    "Rechts: Attention-Kopf 2 fokussiert sich auf die syntaktisch relevanten Wörter\n",
    "\n",
    "Der zweite Kopf scheint die Aufmerksamkeit auf die Wörter \"Ich\", \"besuchte\" aber auch das query Wort \"lernte\" selbst zu richten. Die beiden letzten Wörter sind Verben und \"Ich\" ist das zugehörige Pronomen. Dieser Kopf ist somit auf die Identifizierung syntaktischer Strukturen ausgerichtet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebaa9f-8183-4a31-96e1-e4dbe5813a06",
   "metadata": {},
   "source": [
    "### <a id=\"masking\"></a> Masking\n",
    "\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Stellen Sie sich vor, Sie wollen einem Computer beibringen, einen Satz wie „Die Hauptstadt von Deutschland ist Berlin“ selbstständig zu vervollständigen. Damit er das korrekt lernen kann, muss er beim Üben so behandelt werden, dass er bei jedem Wort nur das sieht, was davor steht und nicht schon das, was danach kommt. Das nennt man Maskieren. Es gibt zwei Arten davon. Eine sorgt dafür, dass der Computer beim Lernen nicht auf leere Stellen achtet. Die andere stellt sicher, dass er beim Vervollständigen nicht schummelt, indem er schon zukünftige Wörter berücksichtigt.\n",
    "</div>\n",
    "\n",
    "Das Maskieren des Inputs ist eine wichtige  Komponente der Transformerarchitektur. Beim Masking handelt es sich in Wirklichkeit um zwei Mechanismen, die zwar dieselbe Funktionsweise besitzen, aber sehr unterschiedliche Aufgaben in der Architektur übernehmen. Einerseits das Subsequent Masking, andererseits das Padding Masking. Das Padding Masking stellt jediglich sicher, dass nur Positionen mit Inhalt vom Transformer verarbeitet werden, während das Subsequent Masking dafür sorgt, dass der Decoder des Transformers autoregressiv ist. Das bedeutet, bei einer Vorhersage für eine Ausgabe an der Position $i$, soll das Modell nur Informationen aus den Eingabepositionen $1,\\ldots,i-1$ nutzen. Das Zusammenspiel der Beiden sehen Sie in der folgenden Abbildung (<a href=\"#fig:fig_masking\">Abbildung 7</a>)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e465d9",
   "metadata": {},
   "source": [
    "#### <a id=\"padding-masking\"></a> Padding Masking\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Ein Satz wie „Die Hauptstadt von Deutschland ist Berlin“ ist kürzer als viele andere Sätze, die ein Transformer-Modell verarbeiten soll. Da das Modell aber Eingaben in gleicher Länge erwartet, wird dieser Satz künstlich verlängert, indem am Ende Platzhalter wie Nullen eingefügt werden. Damit das Modell diese Nullen nicht als echte Wörter interpretiert, sorgt das sogenannte Padding Masking dafür, dass sie bei der Berechnung ignoriert werden.\n",
    "</div>\n",
    "Das Padding Masking ist notwendig, da Transformer sequenzielle Daten so verarbeiten, als ob sie eine fixe Länge $d_{model}$ hätten. Das geschieht deshalb, weil Transformer so lernen können jede Position der Ausgabelänge $d_{model}$ parallel vorherzusagen. \n",
    "\n",
    "Um während des Trainings auch Daten mit einer Länge größer oder kleiner $d_{model}$ zu nutzen, werden längere Sequenzen abgeschnitten und kürzere mit Nullen aufgefüllt. Diese Nullen müssen dann mit Hilfe von Padding Masking für das Training irrelevant gemacht werden. Das geschieht indem man alle Positionen die eine Null enthalten für das Modell auf $-\\infty$ setzt, sodass sie beim Gradient Descent nicht berücksichtigt werden.\n",
    "\n",
    "#### <a id=\"subsequent-masking\"></a> Subsequent Masking\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Wenn ein Modell einen Satz wie „Die Hauptstadt von Deutschland ist Berlin“ Wort für Wort aufbauen soll, darf es bei jedem Schritt nur wissen, was davor schon gesagt wurde. Wenn es gerade das Wort „ist“ erzeugt, darf es zum Beispiel noch nicht wissen, dass später „Berlin“ folgt. Obwohl der ganze Satz im Speicher liegt, muss verhindert werden, dass das Modell zukünftige Wörter schon mit einplant. Genau das erreicht Subsequent Masking: Es blendet alle Informationen aus, die zu einem späteren Zeitpunkt im Satz liegen.\n",
    "</div>\n",
    "Subsequent Masking setzt ebenfalls bestimmte Werte auf $-\\infty$, verwendet dabei aber eine andere Strategie als Padding-Masken. In diesem Fall werden nicht die mit irrelevanten Informationen angefüllten Enden der Eingabesequenz maskiert. Stattdessen werden alle Werte von $Score(Q, V)$ auf $-\\infty$ gesetzt, die sich auf ein Token $V_j$ beziehen, wenn $j > i$ gilt. Das bedeutet, bei der Berechnung von $Attention(Q, K, V)_i$ darf das Modell nur solche Informationen aus $V$ berücksichtigen, die zum Zeitpunkt $i$ bereits bekannt sind.\n",
    "\n",
    "So wird verhindert, dass das Modell in die Zukunft schaut, also beim Vorhersagen eines Tokens bereits zukünftige Tokens mit einbezieht. Dies ist besonders wichtig für autoregressive Aufgaben wie die Textgenerierung.\n",
    "\n",
    "Ein Transformer bekommt einen ganzen Satz als Input. Um daraus mehrere Trainingsmöglichkeiten zu erzeugen, soll das Modell den Satz Wort für Wort aufbauen. Dabei darf es jeweils nur die bisherigen Wörter kennen und nicht die folgenden, obwohl der komplette Satz bereits im Speicher vorhanden ist. Das Subsequent Masking sorgt durch gezieltes Ausblenden dafür, dass das Modell diese zukünftigen Informationen beim Training nicht berücksichtigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9065dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wir sollten überlegen in einer zukünftigen Version eine Simulation zur Darstellung des Masking einzufügen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e3202-2841-4260-991f-e3919582cffa",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig_masking\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_masking.jpg\" style=\"width: 400px; height: 350px;\" alt=\"Abbildung 7: Zwei Varianten des Maskings im Transformer Modell.\"/>\n",
    "    <figcaption>Abbildung 7: Die zwei Varianten des Maskings im Transformer Modell</figcaption>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a0ea31",
   "metadata": {},
   "source": [
    "### <a id=\"attention-mechanismen\"></a> Verschiedene Attention-Mechanismen\n",
    "\n",
    "In der Architektur werden verschiedene Attentionstypen unterschieden. Es gibt dabei zwei Variablen die beeinflussen, welche Art von Attention wir verwenden. Die erste Variable ist, woher die Eingaben $Q', K'$ und $V'$ kommen. Die zweite Variable ist die Maskierung, die wir auf $\\text{Score}(Q,K)$ anwenden.\n",
    "\n",
    "\n",
    "#### <a id=\"self-attention\"></a> Self-Attention\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Ein Modell wie der Transformer versucht bei jedem Wort eines Satzes zu erkennen, welche anderen Wörter für dessen Bedeutung wichtig sind. Wenn der Satz „Die Hauptstadt von Deutschland ist Berlin“ lautet, erkennt das Modell zum Beispiel, dass „Deutschland“ für das Wort „Hauptstadt“ eine wichtige Rolle spielt. Diese Fähigkeit, die Aufmerksamkeit auf andere Stellen innerhalb desselben Satzes zu richten, nennt man Self-Attention.\n",
    "</div>\n",
    "Wir sprechen von Self-Attention, wenn gilt \n",
    "\n",
    "$$Q'=K'=V'.$$ \n",
    "\n",
    "Wenn sich $\\text{Score}(Q,K)$ also bildlich gesprochen daraus ergibt, welche Attention jede Position der Eingabe auf eine andere Position derselben Eingabe legt und diese Attention auf die Eingabe selbst angewandt wird.\n",
    "\n",
    "#### <a id=\"cross-attention\"></a> Cross-Attention\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Bei vielen Aufgaben arbeitet ein Sprachmodell nicht nur mit einem einzigen Text, sondern kombiniert Informationen aus zwei verschiedenen Quellen. Denkbar ist etwa der Fall, dass ein Modell auf der einen Seite eine Frage wie „Welche Stadt ist das politische Zentrum Deutschlands?“ erhält und auf der anderen Seite den Satz „Die Hauptstadt von Deutschland ist Berlin“. Das Modell richtet dann gezielt seine Aufmerksamkeit auf den zweiten Text, um die passende Information zu extrahieren. Diesen Vorgang, bei dem Informationen über zwei unterschiedliche Eingaben hinweg verknüpft werden, bezeichnet man als Cross-Attention.\n",
    "</div>\n",
    "Wie sprechen von Cross-Attention, wenn gilt \n",
    "\n",
    "$$K' = V'$$\n",
    "\n",
    "aber $Q'$ von diesen beiden Werten verschieden ist.\n",
    "Wenn sich $\\text{Score}(Q,K)$ also daraus ergibt, welche Attention jede Position einer Eingabe $Q'$ auf die Positionen einer zweiten Eingabe $K'$ gibt und dieser Attentionsscore auf die zweite Eingabe angewandt wird.\n",
    "\n",
    "Dies ist zum Beispiel in Transformern der Fall, wenn $Q'$ sich aus der Ausgabe des Encoder ergibt und $K' = V'$ ein Zwischenergebnis des Decoders ist.\n",
    "\n",
    "#### <a id=\"masked-attention\"></a> Masked Attention\n",
    "\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Beim automatischen Schreiben von Texten darf ein Modell zukünftige Wörter nicht kennen, bevor es sie selbst erzeugt. Wenn es etwa den Satz „Die Hauptstadt von Deutschland ist Berlin“ Wort für Wort vorhersagen soll, darf es beim Wort „ist“ noch nichts über das folgende „Berlin“ wissen. Damit es trotzdem sinnvoll vorhersagen kann, wird ihm gezielt der Blick in die Zukunft verwehrt. Dies geschieht durch Masked Attention: Das Modell darf bei der Vorhersage eines Wortes nur auf die Wörter schauen, die bereits vorher im Satz stehen.\n",
    "</div>\n",
    "\n",
    "Ein Fall von Masked-Attention liegt dann vor, wenn bestimmte Werte von $\\text{Score}(Q,K)$ maskiert werden. Das ist zum Beispiel beim Subsequent Masking der Fall, hier wird $\\text{Score}(Q,K)_{i,j} = -\\infty$ gesetzt für alle Einträge $j>i$. Dadurch wird verhindert, dass die Ausgabe $\\text{Attention}(Q,K,V)_i$ sich auf die Werte $V_j, j>i $ stützt. Zum Beispiel wird während des Trainings im Decoder dadurch verhindert, dass das Model lernt Informationen aus den zukünftigen Einträgen $V_j, j>i$ zu benutzen, um $V_i$ vorherzusagen. Man sieht das gut in der Darstellung von $\\text{Score}(Q,K)$ in unserer <a href=\"#simulation_attention\">Simulation der Attention-Matrizen</a>. Dort liegen die Werte für $j>i$ meistens nahe bei $0$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77e5aaf9",
   "metadata": {},
   "source": [
    "## <a id=\"simulation\"></a> Vollständige Simulation\n",
    "\n",
    "Zuletzt findet sich hier nun noch eine Simulation des kompletten Inferenzvorgangs innerhalb eines Transformermodells. Diese Simulation zeigt alle der vorher genannten Schritte in einem Prozess und liefert eine tatsächliche Vorhersage für den hier eingegebene Text.\n",
    "Da unser Modell im Vergleich zu großen in der Wirtschaft eingesetzen Modellen nur mit sehr wenig Traninigsdaten und -zeit trainiert wurde, ist seine Vorhersageleistung sehr beschränkt und es wird keinen vernünftigen Text liefern. Die Mechanismen die dabei implementiert wurdens, sind allerdings identisch zu denen sehr großer Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cf257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.762518500Z",
     "start_time": "2024-01-16T09:57:26.729423800Z"
    }
   },
   "outputs": [],
   "source": [
    "inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_inf = widgets.Text(\n",
    "    value='Was ist die Hauptstadt von Deutschland?',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  \n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_inf = widgets.Button(description='Run interactive inference',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_inf = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_inf:\n",
    "        output_widget_inf.clear_output()  \n",
    "        inference_model(input_widget_inf.value, interactive=True) \n",
    "        inference_model.print_results(visualisation=True)\n",
    "\n",
    "button_widget_inf.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_inf, button_widget_inf, output_widget_inf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82dc7ff0",
   "metadata": {},
   "source": [
    "# <a id=\"bibliographie\"></a> Bibliographie\n",
    "[1] Vaswani, A. et al. Attention Is All You Need. Preprint at http://arxiv.org/abs/1706.03762 (2017).\n",
    "\n",
    "[2] Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Computation 9, 1735–1780 (1997).\n",
    "\n",
    "[3] Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks 5, 157–166 (1994).\n",
    "\n",
    "[4] Cho, K., van Merrienboer, B., Bahdanau, D. & Bengio, Y. On the Properties of Neural Machine Translation: Encoder–Decoder Approaches. in Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation 103–111 (Association for Computational Linguistics, 2014). doi:10.3115/v1/W14-4012.\n",
    "\n",
    "[6] Kaplan, J. et al. Scaling Laws for Neural Language Models. Preprint at http://arxiv.org/abs/2001.08361 (2020).\n",
    "\n",
    "[5] Goodfellow, I., Bengio, Y. & Courville, A. Deep learning. (The MIT Press, 2016).\n",
    "\n",
    "[7] Radford, A. et al. Language Models are Unsupervised Multitask Learners. (2019).\n",
    "\n",
    "[8] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Preprint at http://arxiv.org/abs/1810.04805 (2019).\n",
    "\n",
    "[9] Alammar, J. The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. https://jalammar.github.io/illustrated-transformer/ (2018).\n",
    "\n",
    "[10] Encoder-Decoder. Understanding The Model Architecture | by Naoki | Medium. https://naokishibuya.medium.com/transformers-encoder-decoder-434603d19e1.\n",
    "\n",
    "[11] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. (2014).\n",
    "\n",
    "[12] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[13] Gage, P. A New Algorithm for Data Compression. (1994).\n",
    "\n",
    "[14] Cho, K. et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Preprint at http://arxiv.org/abs/1406.1078 (2014).\n",
    "\n",
    "[15] Bahdanau, D., Cho, K. & Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate. Preprint at http://arxiv.org/abs/1409.0473 (2016).\n",
    "\n",
    "[16] OpenAI. GPT-4 Technical Report. Preprint at http://arxiv.org/abs/2303.08774 (2023).\n",
    "\n",
    "[17] Grefenstette, G. & Tapanainen, P. What is a word, What is a sentence? Problems of Tokenization.\n",
    "\n",
    "[18] Lin, Z. et al. A Structured Self-attentive Sentence Embedding. Preprint at http://arxiv.org/abs/1703.03130 (2017).\n",
    "\n",
    "[19] Schmidt, R. M. Recurrent Neural Networks (RNNs): A gentle Introduction and Overview. Preprint at http://arxiv.org/abs/1912.05911 (2019).\n",
    "\n",
    "[20] Ioffe, S. & Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Preprint at http://arxiv.org/abs/1502.03167 (2015).\n",
    "\n",
    "[21] Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer Normalization. Preprint at http://arxiv.org/abs/1607.06450 (2016).\n",
    "\n",
    "[22] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[23] Tunstall, L., von Werra, L., Wolf, T. Natural Language Processing Mit Transformern. https://www.oreilly.com/library/view/natural-language-processing/9781098157081/.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "clean_voila_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
