{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ab5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging and decorators\n",
    "import logging as log\n",
    "\n",
    "# tensorflow modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# necessary for visualization and user input\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact_manual, interactive, interact, VBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# modules from backend\n",
    "from interactive_inference_backend import ModelLoader, StoryTokenizer, WordComplete, VisualWrapper, positional_encoding\n",
    "from interactive_inference_backend import reserved_tokens, vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cff4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelLoader(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                            d_model=512,\n",
    "                            n_stacks=2,\n",
    "                            h_att=4,\n",
    "                            load_model=True,\n",
    "                            model_load_path=\"model_N2_h4_d512_t20230831-134344\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589346f1",
   "metadata": {},
   "source": [
    "# Interaktive Erklärung der Transformerarchitektur\n",
    "## Einleitung\n",
    "### Kurzübersicht Transformer\n",
    "\n",
    "Transformer-Modelle sind eine von Vaswani et al. [1] vorgeschlagene Architektur für das Modellieren von sequentiellen Daten. Im Gegensatz zu den vorher genutzten Architekturen wie RNNs [2, 3] oder CNNs [4] ermöglichen Transformer allerdings das parallele Verarbeiten der sequentiellen Daten während des Trainings und ermöglichen dadurch mit erheblich reduzierter Trainingszeit und -rechenkapazität sequentielle Daten zu verarbeiten [Cite comparisson].\n",
    "\n",
    "Um diese parallele Verarbeitung zu erreichen nutzen Transformermodelle Aufmerksamkeitsblöcke. Aufmerksamkeitsblöcke sind trainierbare Matrizen, die von einem Eingabevektor auf einen Ausgabevektor projizieren. Also eine sequentielle Datenstruktur in Vektorform in eine ebenfalls sequentielle Datenstruktur in Vektorform verarbeiten, indem eine Matrixmultiplikation auf dein Eingabevektor angewandt wird. Wie das funktioniert wird in [einer eigenen Sektion behandelt](#aufmerksamkeit) gezeigt.\n",
    "Diese Matrizen projizieren also im Idealfall von jedem Eintrag im Eingabevektor genau diejenige Information auf einen Eintrag im Ausgabevektor, die an der jeweiligen Stelle die Ausgabe beeinflussen soll. Da hierbei die Ausgabe i nicht von der Ausgabe i-1 abhängt, können wir parallel die komplette Ausgabe zu jeder Eingabe erzeugen und mit einem Gradient-Descent [5] Verfahren unser Modell trainieren\n",
    "Da wir während der Inferenz (Erklärung was Inferenz ist) von sequentiellen Daten aber meist nur die vorhergehende Daten nutzen möchten benutzen Transformer Architekturen eine Maske, die nach der Anwendung der Matrixmultiplikation alle Informationen, die nachfolgende Daten liefern herausfiltern, sodass schon während dem Training nur die Verbindung zwischen der aktuellen Position und den vorhergehenden gelernt wird.\n",
    "Durch wiederholen von Aufmerksamkeitsblöcken und einem nachfolgenden Feed-Forward Netzwerk entsteht so eine Architektur zur Verarbeitung sequenzieller Daten, die verglichen mit vorherigen Architekturen weniger Rechenkapazität benötigt und trotzdem eine wesentlich reduzierte Trainigszeit aufweist. [1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0423185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier eine grafische Darstellung von Eingabe- zu Ausgabevektor darstellen. Wie werden sequentielle Daten verarbeitet und wie macht das ein Transformer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0012634",
   "metadata": {},
   "source": [
    "\n",
    "### Ziel dieses interaktiven Artikels\n",
    "\n",
    "Ziel dieses interaktiven Artikels soll es sein die Architektur die [1] beschreiben in ihren einzelnen Komponenten darzustellen. Der Fokus liegt hierbei darauf die Prozesse, die während der Verarbeitung sequentieller Daten stattfinden, grafisch durch interaktive Anwendungen darzustellen der Nutzer den Einfluss unterschiedlicher Architekturbausteine auf verschiedene Eingaben anschaulich klar wird. Ziel ist es, die verschiedenen Bausteine so zu erklären, dass einem möglichen Anwender die Implementation einer Tranformer-Architektur, durch ein Verständnis des Nutzens der einzelnen Architekturbausteine, erleichtert wird.\n",
    "\n",
    "Der Artikel soll dazu dienen eine Implementierung ohne einschlägiges Vorwissen, z.B. im Kontext von KMUs die sich bisher noch nicht mit der Modellierung sequentieller Daten beschäftigt haben, zu ermöglichen und Programmierer in der Designentscheidungen in der Transformerarchitektur zu unterstützen.\n",
    "[1] und viele der auf ihrer Architektur aufbauenden wissenschaftlichen Arbeiten [7, 8], Erklärartikel oder -videos [9, 10] beschränken sich, wenn überhaupt, darauf den Aufmerksamkeitsmechanismus ausführlich darzustellen. Dabei werden die Designentscheidungen für trainingsrelevante Elemente wie Dropout [11], Residuale Verbindungen [12] sowie die Beschreibung bereits etablierter Methoden wie Byte-Pair Encoding [13] als Einbettung oder Log-Softmax [5] vernachlässigt. Diese Elemente sollen hier aber ebenfalls dargestellt werden.\n",
    "\n",
    "### Architekturübersicht\n",
    "\n",
    "In <a href=\"#fig:input\">Abbildung 1</a> ist eine vollständige Darstellung aller Architekturelemente zu finden, die Teil der Transformerarchitektur sind. Dabei unterscheiden wir vier verschiedene Elemente. Prozesse und Methoden werden in Schwarz dargestellt. Die in diesen Prozessen generierte Daten werden in Blau dargestellt. Trainierbare Parameter werden in Gelb dargestellt. Hyperparameter, also festzulegende Eingabeparameter für das Modell werde in Grau dargestellt. Ebenfalls in Grau werden Eingabe- und Ausgabedaten dargestellt.\n",
    "Unsere Abbildung ist insofern komplett, als sie jeglichen Weg zeigen, den Daten durch das Modell nehmen können. Eine Darstellung, in der Form eines Encode-Decoder Netzwerks [14], wie sie [1] nutzen findet sich in Abbildung 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d914d5",
   "metadata": {},
   "source": [
    "<figure id=\"fig:input\" style=\"height: 700px;\">\n",
    "  <img src=\"./img/tf_arch_full.jpg\" style=\"height: 1400px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 1: Transformerarchitektur</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a6b568b",
   "metadata": {},
   "source": [
    "\n",
    "#### Encoder-Decoder\n",
    "\n",
    "Eine Encoder-Decoder Architektur besteht aus zwei voneinander getrennten neuronalen Netzen dem Encoder und dem Decoder, die aber zusammen trainiert werden. Diese Architektur wurde von [14] vorgeschlagen und dann vor allem im Kontext von Neural Machine Translation (NMT) verwendet. Der Vorteil von Encoder-Decoder liegt darin, dass man sowohl den Encoder als auch den Decoder ersetzen kann. Im Idealfall könnte man im Bereich von NMT pro Sprache einen Encoder und einen Decoder trainieren, um dann beliebige Übersetzungen zwischen allen Sprachen zu generieren. Da auch [15], die zuerst Aufmerksamkeitsmechanismen für Rekurrente Neuronale Netze (RNN) vorgeschlagen haben, eine Encoder-Decoder Architektur verwenden, verwundert es nicht, dass auch [1] eine solche Architektur wählten. Insbesondere da [1] Transformer ebenfalls für NMT benutzen.\n",
    "Transformer zur Textvervollständigung können auch ausschließlich auf Decodern beruhen [16]. In unserem Beispiel wird der Encoder zum enkodieren der Eingabe verwendet, um dann im Decoders die Eingabe als Anfang der Ausgabe zu verwenden. In Abbildung 2 sieht man wie Encoder und Decoder zusammenarbeiten. Der Decoder benutzt Selbstaufmerksamkeit, um seinen Input zu verarbeiten, nutzt dann Kreuz-Aufmerksamkeit, für den nächsten Schritt, um final erneut Selbstaufmerksamkeit zu benutzen, wohingegen der Encoder ausschließlich über Selbstaufmerksamkeit funktioniert. Näheres siehe im Kapitel [Aufmerksamkeit](#aufmerksamkeit)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "398698e4",
   "metadata": {},
   "source": [
    "Hier können Sie nun einen Beispielsatz zuerst vom Encoder kodieren lassen, um ihn dann im nächsten Schritt vom Decoder dekodieren zu lassen und damit eine Ausgabe zu erhalten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4798807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b9abb15c7d143a0ad8c0b48295ddb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Encoder-Decoder Test', continuous_update=False, description='Ihre Eingabe:', layout…"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "transformer = model.model\n",
    "\n",
    "input_widget_enc_dec = widgets.Text(\n",
    "    value='Encoder-Decoder Test',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_enc = widgets.Button(description='Enkodiere die Eingabe',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "button_widget_dec = widgets.Button(description='Dekodiere die Eingabe',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_enc = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "output_widget_dec = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click_enc(b):\n",
    "  with output_widget_enc:\n",
    "    output_widget_enc.clear_output()  # clear the previous output\n",
    "    #output_widget_dec.clear_output()\n",
    "\n",
    "    # Convert input to tensor if it is not already\n",
    "    # Create a dynamic tensor to store output\n",
    "    # Make sure tensor_input is 2-D\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)\n",
    "    if len(tensor_input.shape) == 0:\n",
    "      tensor_input = tensor_input[tf.newaxis]\n",
    "    # tokenize and encode input\n",
    "    # Identify end token of the input\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    context = transformer.encode(input_without_eos, None)\n",
    "\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(context)\n",
    "\n",
    "def on_button_click_dec(b):\n",
    "  with output_widget_dec:\n",
    "    output_widget_dec.clear_output()\n",
    "\n",
    "    # Convert input to tensor if it is not already\n",
    "    # Create a dynamic tensor to store output\n",
    "    # Make sure tensor_input is 2-D\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    if len(tensor_input.shape) == 0:\n",
    "      tensor_input = tensor_input[tf.newaxis]\n",
    "    # tokenize and encode input\n",
    "    # Identify end token of the input\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    context = transformer.encode(input_without_eos, None)\n",
    "\n",
    "     # Write the input tokens (excluding the last one) to the output array\n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "      output_array = output_array.write(i, value)\n",
    "\n",
    "    dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "    dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(dec_out)\n",
    "\n",
    "button_widget_enc.on_click(on_button_click_enc)\n",
    "button_widget_dec.on_click(on_button_click_dec)\n",
    "\n",
    "widgets.VBox([input_widget_enc_dec, widgets.HBox([widgets.VBox([button_widget_enc, output_widget_enc]), widgets.VBox([button_widget_dec, output_widget_dec])])])\n",
    "#display(input_widget_enc_dec, button_widget_enc, output_widget_enc, button_widget_dec, output_widget_dec)\n",
    "\n",
    "\n",
    "#print('tok_out', tokenized_input)\n",
    "#print('enc_out', context)\n",
    "#print(\"dec_out\", dec_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028a3317",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Architekturblöcke\n",
    "\n",
    "Prinzipiell lassen sich Transformer in mehrere Blöcke einteilen.\n",
    "\n",
    "1. Eingabeblock\n",
    "\n",
    "Die Eingabepipeline verarbeitet die Eingabedaten in eine Form die für die Matrixtransformation im Aufmerksamkeitsblock genutzt werden kann. Diese unterscheidet sich für verschiedene Datentypen. In unserem Beispiel nutzen wir Textdaten, die wir durch Tokenisierung [17] und ein Embedding [18] in Tensoren verwandeln.\n",
    "\n",
    "2. Aufmerksamkeitsblock\n",
    "\n",
    "Der Aufmerksamkeitsblock verarbeitet Daten in Tensorform und liefert somit eine Abbildung von den Eingabetensoren auf die Ausgabetensoren, die jeweils von den Eingabe- und Ausgabeblöcken interpretiert wird.\n",
    "Man kann verschiedene Modulvarianten innerhalb eines Aufmerksamkeitsblocks unterscheiden. Aufmerksamkeitsmodule erhalten als Eingabedaten immer drei Tensoren. Diese werden Query, Key und Value genannt. Aus diesen berechnet ein Aufmerksamkeitsmodul eine Ausgabe.\n",
    "Typischerweise werden Aufmerksamkeitsmodule dabei dadurch unterschieden aus welcher Quelle Query, Key und Value stammen.\n",
    "Es gibt \n",
    "\n",
    "- Self-Attention bei der Query, Key und Target alle aus einer Quelle stammen und\n",
    "- Source-Attention (Hier ist der Query aus einer anderen Quelle als Key und Value, z.B. wenn der Query die Ausgabe eines Encoders ist, während Target die Ausgabe eines Decoderblocks ist).\n",
    "\n",
    "Desweiteren kann man nach Art des Maskings unterscheiden. Masking wird verwendet, um das Aufmerksamkeitsmodul daran zu hindern aus einem Teil der Ausgabedaten zu lernen. In Transformern gibt es \n",
    "\n",
    "- Subsequent Masking, das ein Decoderaufmerksamkeitsblock daran hindert für die Vorhersage der Position i Daten aus den Positionen i+j nutzt, und\n",
    "- Padding Masking, das verhindert, dass Fehler verhindert, die aufgrund dessen entstehen, dass die Eingabe eines Transformers, unabhängig vom Inhalt, immer die selbe Anzahl an Tokens besitzen muss.\n",
    "\n",
    "3. Ausgabeblock\n",
    "\n",
    "Die Ausgabepipeline interpretiert die Ausgaben des Aufmerksamkeitsmoduls, sodass sie in eine für menschlichen Gebrauch nützlichen Form vorliegen (typischerweise z.B. Textdaten, Bilddaten, etc.). Dafür wird in Transfomern eine Log-Softmax Funktion genutzt, die auf die Tensorausgabe des letzten Aufmerksamkeitsblockes angewandt wird.\n",
    "\n",
    "In <a href=\"#fig:input\">Abbildung 1</a> entspricht das Aufmerksamkeitsmodul den Prozessen innerhalb der grauen Umrandung, während die Eingabe- und Ausgabepipeline darunter bzw. darüber zu finden sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f86373b1",
   "metadata": {},
   "source": [
    "\n",
    "## Input\n",
    "\n",
    "Der erste Teil eines Transformermodells besteht aus der Eingabepipeline. Diese verarbeitet die Eingabe in das Modells, z.B. die Texteingabe eines Nutzers und bereitet sie darauf vor durch wiederholte Anwendung der Aufmerksamkeitsmodule in einem Transformermodell verarbeitet zu werden. Die Aufmerksamkeitsmodule arbeiten über eine Aufmerksamkeitsmatrix, die aus der jeweiligen Eingabe eine Ausgabe berechnet. Wir müssen also aus einer Eingabe in Textform eine Vektorrepräsentation erzeugen, die alle notwendigen Informationen für ein Modell enthalten, um nützliche Vorhersagen über eine Übersetzung oder eine Textvervollständigung machen zu können.\n",
    "\n",
    "<a href=\"#fig:input\">Hier</a> sehen sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt. \n",
    "Wie zu erkennen ist, wird in der Eingabpipeline während des Training eines Transformermodells ausschließlich die Gewichte für die Einbettung trainiert, alles andere funktioniert deterministisch und ist damit unbeeinflusst vom Trainningsprozess.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4caafaa5",
   "metadata": {},
   "source": [
    "\n",
    "<figure id=\"fig:input\" style=\"height: 700px;\">\n",
    "  <img src=\"./img/tf_input_pipeline.jpg\" style=\"height: 700px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4578ca",
   "metadata": {},
   "source": [
    "Prinzipiell besteht die Eingabepipeline aus drei Modulen:\n",
    "\n",
    "1. [Tokenisierung](#tokenisierung),\n",
    "2. [Einbettung](#einbettung),\n",
    "3. [Positionale Kodierung](#positionale-kodierung).\n",
    "\n",
    "Dies entspricht drei Verarbeitungsschritten:\n",
    "\n",
    "1. Bei der Tokenisierung, wird der Text mithilfe eines Zeichenalphabets in eine Zahlenkodierung umgewandelt.\n",
    "2. Bei der Einettung wird diese mithilfe eines trainierbaren Algorithmus in eine Vektordarstellung mit sehr viel mehr Parametern umgewandelt. Diese Einbettung lernt die komplexe Struktur eines Textes so darzustellen, dass sie informativ für die nachfolgenden Module ist. Wie genau das passiert ist dabei aufgrund der stochastischen Natur von Deep Learning Verfahren nicht offen einsehbar.\n",
    "3. Die Positionelle Kodierung ist einzigartig für Transformermodelle. Im Gegensatz zu RNN, die die Eingabedaten sequentiell präsentiert bekommen [19], bekommen Transformer die Eingabe ohne Informationen zur relativen Position der Tokens. Diese fehlenden Informationen werden in diesem Schritt manuell hinzugefügt.\n",
    "\n",
    "### Tokenisierung\n",
    "\n",
    "Bei der Tokenisierung wird der Satz in Textform z.B. \"Das ist ein Testsatz.\" in einen Zahlencode verwandelt. Hierfür kommen verschieden Methoden in Frage. Einer der simpelsten Methoden ist es z.B. jedem Buchstaben eine Zahl zuzuordnen. Das führt allerdings zu einer sehr langen Kodierung.\n",
    "\n",
    "Die entscheidenden Faktoren für eine gute Kodierung sind\n",
    "- Vollständigkeit der Kodierung, \n",
    "- Länge des kodierten Vektors und \n",
    "- Größe des dafür nötigen Vokabulars.\n",
    "\n",
    "Die Kodierung mit einzelnen Buchstaben ist vollständig (man kann beliebige Zeichenkombinationen kodieren) und besitzt ein kurzes Vokabular (26 für alle Buchstaben plus alle Punktierungs und Sonderzeichen, die im Text vorkommen), allerdings ist die Länge der kodierten Vektoren groß.\n",
    "\n",
    "Benutzt man ein Vokabular an festgeleten Wörtern, wird die Länge der Kodierung verkürzt, allerdings besteht die Gefahr der Unvollständigkeit. Um das nach Möglichkeit zu verhindern, benötigt man ein Vokabular, dass den Raum der möglichen Wörter vollständig abdeckt. Diese Vokabular müsste daher sehr umfangreich sein.\n",
    "\n",
    "Die Probleme der obigen Methoden hat dazu geführt, dass sich gemischte Verfahren ergeben haben. Die vorher an einem möglichst großem Korpus trainiert werden. Zum einen Top-Down Verfahren, die von einem aus dem Korpus extrahierten Vokabular ausgehen und lernen unbekannte Worte in Teilworte zu zerlegen, die dann in das Vokabular aufgenommen werden. Zum anderen Bottom-Up Verfahren, die von einem Buchstabenvokabular ausgehen und häufig wiederkehrende Kombinationen in dieses explizit aufnehmen.\n",
    "Die Transformerarchitektur nach [1] verwendet ein solches Bottom-Up Verfahren namens Byte-Pair Encoding [13], dass sich als effizient erwiesen hat und ein relativ kompaktes Vokabular ermöglicht dabei aber die Länge der Tokenisierung klein hält. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b56024be",
   "metadata": {},
   "source": [
    "Den Tokenizer findet man in unserer <a href=\"#fig:transformer\">Abbildung 1</a> ganz unten und ist der erste Schritt, um eine Eingabe zu verarbeiten\n",
    "<figure id=\"fig:tokenizer\" style=\"height: 300px;\">\n",
    "  <img src=\"./img/tf_tokenizer.jpg\" style=\"height: 300px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a5e1d1",
   "metadata": {},
   "source": [
    "\n",
    "### Byte-Pair Encoding\n",
    "\n",
    "Byte-Pair Encoding nutzt ein Vokabular mit einer festgelegten Länge. In unserer Implementation des Tokenizer nutzt er ein Vokabular von 8000 Einheiten. Das Vokabular wird folgendermaßen erstellt:\n",
    "\n",
    "  1. Ein Text, der für die Erstellung des Vokabulars verwendet wird, wird in eine Sequenz von Buchstaben und Symbole zerlegt. Wortenden werden mit einem zusätzlichen Symbol kodiert.\n",
    "  2. Alle Buchstaben und Symbole werden in das Vokabular aufgenommen.\n",
    "  3. Nun wird das häufigste 2-Gramm, also zwei aufeinander folgende Symbole, gesucht, das im Text zu finden ist.\n",
    "  4. Dieses wird ins Vokabular aufgenommen und im Text durch ein einzelnes Symbol ersetzt.\n",
    "  5. Dieser Prozess wird nun wiederholt, bis die vorgegebene Länge des Vokabulars erreicht ist.\n",
    "\n",
    "Dabei werden sowohl ganze Wörter ins Vokabular aufgenommen, wenn sie oft genug auftauchen (bespielweise werden die Worte \"a\", \"the\", \"and\" bei englischen Texte sicher aufgenommen), aber auch einzelne Wortteile wie z.B. \"en##\", \"##ment\" oder \"##ed\" werden in diesem Vokabular sicher vorkommen, um seltene Kombinationen wie \"enablement\" in die Wortteile \"en##\", \"able\" und \"##ment\" zerlgen zu können oder grammatikalische Formen wie \"wanted\" zu bilden.\n",
    "\n",
    "In unserem Testbeispiel können Sie testen, wie Ihre Eingabe in Tokens getrennt und dann in numerische Werte umgewandelt wird, je nachdem, welche Position das Token in unserem Vokabular hat.\n",
    "Wie Sie sehen, wird unser Bytepaar-Kodierungsvokabular durch ein [START]- und [END]-Token erweitert und enthält auch Elemente vom Typ 'abc##' oder '##abc'. Diese stellen eine Sequenz am Anfang oder Ende eines Wortes dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60193c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a3de726a014a318a9d19fa2fe50f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Tokenizer test', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a569eae7aa4327aa69c4bb7c292806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run tokenizer on input', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8de25ddc8d4b6c8ed49c5c59103bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_tok = widgets.Text(\n",
    "    value='Tokenizer test',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_tok = widgets.Button(description='Run tokenizer on input',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_tok = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_tok:\n",
    "        output_widget_tok.clear_output()  # clear the previous output\n",
    "        tokens = tokenizer.tokenize(input_widget_tok.value)\n",
    "        lookup = tokenizer.lookup(tokens)\n",
    "\n",
    "        VisualWrapper.display_text('Tokens die aus der Eingabe mit Byte-Pair Encoding extrahiert werden:'.rjust(100) + ', '.join([token.decode('utf-8').rjust(10) for token in lookup.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "        VisualWrapper.display_text('Ihre Positionsnummer im Alphabet des Byte-Pair Encoding Algorithmus:'.rjust(100) + ', '.join([str(token).rjust(10) for token in tokens.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "\n",
    "button_widget_tok.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_tok, button_widget_tok, output_widget_tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e5b84a",
   "metadata": {},
   "source": [
    "### Einbettung\n",
    "\n",
    "Wo die Sequenzen die durch den Tokenizer entsteht in ihrer Länge, je nach Länge der Eingabe, aber auch durch die Eigenschaften des Byte-Pair-Encoding, variiert, ist es für Transformer notwendig eine gleichbleibendes Eingabeformat zu erhalten.\n",
    "\n",
    "Dies wird dadurch gelöst, dass Padding Tokens zum Einsatz kommen, also Tokens, die keine Information codieren, sondern der Eingabe beigefügt werden, um die erforderliche Länge zu erreichen.\n",
    "\n",
    "Außerdem wird die Ausgabe des Tokenizers mithilfe einer trainierbaren Einbettungsmatrix in das notwendige Format gebracht\n",
    "\n",
    "Die Einbettung sorgt dafür, dass die Ausgabe des Tokenizer in einen Vektor der Modellgröße *d_model* kodiert wird. Das heißt jedes Token, das zuvor durch eine Zahl kodiert wurde wird nun durch einen Vektor der Länge *d_model* kodiert. \n",
    "Die Einbettung ist Teil der vom Modell gelernten Parameter (siehe <a href=\"#fig:embedding\">Abbildung 5</a>) und somit nicht deterministisch gegeben. Welche Informationen aus der Ausgabe des Tokenizers wo gespeichert wird ist also nicht nachvollziehbar.\n",
    "Vorstellen kann man sich aber, dass in jedem der *d_model* Parametern einige der Informationen gespeichert werden, die für das jeweilige Token wichtig sind. Beispielsweise die Bedeutung des Tokens, seine grammatikalische Form, in welchem Kontext es benutzt wurde, steht es am Anfang eines Satzes oder am Ende, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab2ec3f9",
   "metadata": {},
   "source": [
    "\n",
    "<figure id=\"fig:embedding\" style=\"height: 300px;\">\n",
    "  <img src=\"./img/tf_embedding.jpg\" style=\"height: 300px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 4: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b34b4d",
   "metadata": {},
   "source": [
    "Wie eine solche Einbettung aussieht und wie sie sich verändert, wenn man beispielsweise neue Teile an den Satz anfügt können Sie in der nachfolgenden Simulation ausprobieren. Der Eingabetext wird erst vom Tokeniser in Tokens umgewandelt und dann durch die Einbettung in einen Tensor.\n",
    "\n",
    "An jeder Position (vertikal dargestellt) ist dann die Einbettung des Tokens an dieser Stelle zu sehen (horizontal dargestellt). Die farbliche Kodierung stellt dabei das Zahlenspektrum dar, indem sich die Vektoreinträge bewegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758a1d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc199b15cbc14faf83651509e75d20da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p style=\"font-size:18px; color:blue;\">Hier kannst du einen Text einbetten lassen. Wenn du die Ein…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d00ab96c3f4b4f8f545dfb43bbe1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Einbettung Test', continuous_update=False, description='Ihre Eingabe:', layout=Layout(margin='0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b267d4da7a054f1aa173ed5206c8beb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Einbettung erstellen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6ba6b34299441ab11ee040bebd8c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EmbeddingExample():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "        self.input_widget = widgets.Text(\n",
    "            value = 'Einbettung Test',\n",
    "            description = 'Ihre Eingabe:',\n",
    "            continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "            layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "        )\n",
    "\n",
    "        self.button_widget = widgets.Button(description='Einbettung erstellen',\n",
    "                                    layout = widgets.Layout(width='auto'))\n",
    "\n",
    "        self.output_widget = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "        self.old_context = None\n",
    "\n",
    "    def on_button_click(self, b):\n",
    "        with self.output_widget:\n",
    "            self.output_widget.clear_output()  # clear the previous output\n",
    "            VisualWrapper.reset_visualiser()\n",
    "            tokens = self.tokenizer.tokenize(self.input_widget.value)\n",
    "            input_without_eos = tokens[tf.newaxis, :, :-1]\n",
    "            context = model.model.enc_embed(input_without_eos)\n",
    "            VisualWrapper.display_text('So sieht die Einbettung der Eingabe aus.')\n",
    "            VisualWrapper.color_bar(context.to_tensor())\n",
    "\n",
    "            if self.old_context is not None:\n",
    "                padded_context, padded_old_context = self.pad_tensors(context, self.old_context)\n",
    "\n",
    "                VisualWrapper.display_text('So unterscheiden sich die alte und die neue Einbettung voneinander.')\n",
    "                context_diff = padded_context - padded_old_context\n",
    "                VisualWrapper.color_bar(context_diff)\n",
    "\n",
    "            self.old_context = context\n",
    "    \n",
    "    def pad_tensors(self, ragged_tensor1, ragged_tensor2):\n",
    "        # Convert ragged tensors to normal tensors, padding with zeros\n",
    "        tensor1 = ragged_tensor1.to_tensor()\n",
    "        tensor2 = ragged_tensor2.to_tensor()\n",
    "\n",
    "        # Calculate the shapes of the tensors\n",
    "        shape1 = tf.shape(tensor1)\n",
    "        shape2 = tf.shape(tensor2)\n",
    "\n",
    "        # Initialize a list for the target shape\n",
    "        target_shape = []\n",
    "\n",
    "        # Iterate over the dimensions of the tensors\n",
    "        for i in range(shape1.shape[0]):\n",
    "            # Append the maximum size of the dimension to the target shape\n",
    "            target_shape.append(tf.maximum(shape1[i], shape2[i]))\n",
    "\n",
    "        # Convert the target shape to a tensor\n",
    "        target_shape = tf.stack(target_shape)\n",
    "\n",
    "        # Initialize lists for the paddings of the tensors\n",
    "        paddings1 = []\n",
    "        paddings2 = []\n",
    "\n",
    "        # Iterate over the dimensions of the tensors\n",
    "        for i in range(shape1.shape[0]):\n",
    "            # Append the required padding for the dimension to the paddings\n",
    "            paddings1.append([0, target_shape[i] - shape1[i]])\n",
    "            paddings2.append([0, target_shape[i] - shape2[i]])\n",
    "\n",
    "        # Convert the paddings to tensors\n",
    "        paddings1 = tf.stack(paddings1)\n",
    "        paddings2 = tf.stack(paddings2)\n",
    "\n",
    "        # Pad the tensors to the target shape\n",
    "        tensor1_padded = tf.pad(tensor1, paddings1)\n",
    "        tensor2_padded = tf.pad(tensor2, paddings2)\n",
    "\n",
    "        return tensor1_padded, tensor2_padded\n",
    "\n",
    "emb_ex = EmbeddingExample()\n",
    "\n",
    "VisualWrapper.display_text('Hier kannst du einen Text einbetten lassen. Wenn du die Eingabe veränderst wird außerdem gezeigt, wie sich die Einbettung geändert hat.')\n",
    "\n",
    "emb_ex.button_widget.on_click(emb_ex.on_button_click)\n",
    "display(emb_ex.input_widget, emb_ex.button_widget, emb_ex.output_widget)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e70583",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Positionale Kodierung\n",
    "\n",
    "Da in der Einbettung keine Informationen über die Position der verschiedenen Worte kodiert wird, muss diese manuell hinzugefügt werden. Hierfür benutzt die Transformerarchitektur für jede Position der Einbettung (also jedes enkodierte Wort) eine Sinuskurve mit anderer Frequenz und Phase. [1] Diese wird der Einbettung an der jeweiligen Stelle hinzugefügt.\n",
    "\n",
    "Dadurch lassen sich die verschiedenen Worte sehr gut voneinander trennen. Die Idee dahinter ist, dass sich:\n",
    "1. die grobe Position der Worte durch die langfrquenten Sinuskurven bestimmen lässt, da sie sich über die gesamte Länge der Eingabe nur allmählich verändert und die Werte der Einbettung durch diese insgesamt in eine bestimmte Richtung verschoben werden, z.B. die Worte im hinteren Teil der Eingabe größere Werte besitzen als im vorderen Teil der Eingabe,\n",
    "2. die genaue Position durch die hochfrequentere Sinuskurven bestimmen lässt, da diese sich schon für benachbarte Vektoren unterscheidet und somit klar wird, welches Wort an welcher Stelle in der Einbettung kodiert wurde.\n",
    "\n",
    "In der unten stehenden Simulation ist zu sehen, wie die positionale Kodierung beipielhaft für eine 2048 x 512 lange und tiefe Einbettung aussieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d875186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7569a73906924a9289cdb32671889437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1025, description='length', max=2048, min=2), IntSlider(value=257, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(length=(2,2048,1), depth=(2,512,1))\n",
    "def print_pos_enc(length, depth):\n",
    "    VisualWrapper.color_bar(positional_encoding(length, depth))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389fa0d1",
   "metadata": {},
   "source": [
    "\n",
    "## Trainingsmethoden\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout [11] ist eine Methode, die während des Trainingsprozesses eines neuronalen Netzes genutzt wird, um zu verhindern, dass die gelernte Ausgabe eines Modells sich zu sehr auf einen einzelnen Prädikator stützt.\n",
    "Dafür wird zwischen zwei Schritten desselben Modells, eine Dropout-Layer eingefügt. Diese setzt zufällig einige der vom ersten Modellteil generierten Ausgabe auf einen vordefinierten Wert (meistens -inf), um den nachfolgenden Schichten diese Information vorzuenthalten. Da diese Operation zufällig erfolgt, müssen die nachfolgenden Teile des Modells lernen ihre Ausgabe auch ohne diese Information zu erstellen. Somit lernt das Model seine Vorhersage auf eine möglichst breite Kombination an Merkmalen aufzubauen und man verhindert, dass Vorhersagen nur aufgrund eines einzigen Merkmals der vorherigen Ausgabe gemacht werden.\n",
    "\n",
    "Ein gutes Beispiel ist das Ende eines Satzes vorherzusagen. In europäischen Sprachen wird ein Satz fast immer mit einem Punkt beendet, also ist es eine gute Strategie zu lernen, dass ein Satz druch einen Punkt beendet wird. Doch ein Modell, dass einen Punkt als einziges Merkmal eines Satzendes nutzt ist wenig robust, wenn man an falscher Stelle einen Punkt setzt oder ihn an einem Satzende druch ein anderes Zeichen ersetzt. Dabei gibt es auch andere Hinweise auf ein Satzende, wie z.B. das vorkommen eines Verbs in der deutschen Sprache oder von Ort und Zeitangaben im Englischen.\n",
    "\n",
    "Um dem Modell keine Informationen vorzuenthalten, wenn es tatsächlich eingesetzt wird, ist Dropout immer so konzipiert, dass es zwar während des Trainings aktiv ist, aber danach abgeschalten wird, sodass während der Inferenzphase keine Informationen gelöscht werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b5c7833",
   "metadata": {},
   "source": [
    "Wie sich eine Ausgabe eines Teils eines Transformermodells unterscheiden kann, wenn man das Dropout anwendet können Sie in der nachfolgenden Simulation testen.\n",
    "\n",
    "In der ersten Grafik sieht man welche Werte in einem uniformen Vektor vom Dropout verändert werden. In den beiden darauffolgenden Grafiken wird das Dropout auf den im Textfeld eingegebenen Beispieltext angewandt, nachdem er durch den Tokenizer und die Einbettung in Vektorform gebracht wurde. Die erste Grafik zeigt den vollständigen Vektor und die zweite Grafik den Vektor, der vom Dropout verändert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b358029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2049d620ee0f4c96ac43d3e002fb886a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=30, continuous_update=False, description='Länge des Tensors:', max=2048, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045a1d290871497d8878205825789c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=512, continuous_update=False, description='Tiefe des Tensors:', max=512, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef842ecea10a4efa93bac4b4695be14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.1, continuous_update=False, description='Dropoutrate:', max=0.9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3da0bb1862436890585e154f6289e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Tes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222ded8268c541468cc1a7b6ff0b6cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_drop = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_drop = widgets.Text(value = 'Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test',\n",
    "                                 description = 'Ihre Eingabe:',\n",
    "                                 continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                                 layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "                                 )\n",
    "\n",
    "length_widget_drop = widgets.IntSlider(value=30,\n",
    "                           min=2,\n",
    "                           max=2048,\n",
    "                           description='Länge des Tensors:',\n",
    "                           continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                           )\n",
    "depth_widget_drop = widgets.IntSlider(value=512,\n",
    "                          min=2,\n",
    "                          max=512,\n",
    "                          description='Tiefe des Tensors:',\n",
    "                          continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                          )\n",
    "dropout_widget = widgets.FloatSlider(value=0.1,\n",
    "                              min=0,\n",
    "                              max=0.9,\n",
    "                              step=0.1,\n",
    "                              description='Dropoutrate:',\n",
    "                              continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                              )\n",
    "\n",
    "def out(length, depth, dropout, input):\n",
    "    VisualWrapper.reset_visualiser()\n",
    "    dropout_layer = layers.Dropout(dropout)\n",
    "    one_tensor = tf.ones([length, depth])\n",
    "    dropout_tensor = dropout_layer(one_tensor, training=True)\n",
    "    VisualWrapper.color_bar(dropout_tensor)\n",
    "\n",
    "    tokens = tokenizer_drop.tokenize(input)\n",
    "    input_without_eos = tokens[tf.newaxis, :, 1:-1]\n",
    "    context = model.model.enc_embed(input_without_eos)\n",
    "    context_drop = dropout_layer(context, training=True)\n",
    "    VisualWrapper.color_bar(context.to_tensor())\n",
    "    VisualWrapper.color_bar(context_drop.to_tensor())\n",
    "    \n",
    "\n",
    "output_widget_dropout = widgets.interactive_output(out,\n",
    "                                                   {'length': length_widget_drop, 'depth': depth_widget_drop, 'dropout': dropout_widget, 'input': input_widget_drop}, \n",
    "                                                   )\n",
    "\n",
    "display(length_widget_drop, depth_widget_drop, dropout_widget, input_widget_drop, output_widget_dropout)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2d169f9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Normalisierung\n",
    "\n",
    "Normalisierung ist eine Technik, die von [20] eingeführt wurde. In tiefen neuronalen Netzen, mit nicht-linearen Aktivierungsfunktionen wie der Sigmoid-Funktion\n",
    "\n",
    "g(x) = 1/(1+exp(-x))\n",
    "\n",
    "trainiert werden gilt, dass g’(x) -> 0 für |x|-> inf. \n",
    "\n",
    "Das führt dazu, dass diese Funktionen in einen Bereich geraten können, in der der Gradient für Stochastic Gradient Descent (SGD) minimal wird und man spricht vom Vanishing Gradient Problem.\n",
    "\n",
    "In neuronalen Netzen ist hierbei das Problem, dass eine Layer z = g(Wx + b) mit der Sigmoid-Funktion g versucht den Output des gesamten vorherigen Netzes x zu gewichten. Dabei hängen sowohl W als auch b von x ab. Da sich während des Trainings alle Layers des Netzes fortwährend aktualisieren, ändert sich auch der Input x fortwährend. \n",
    "Dieser Effekt wird von [20] Internal Covariate Shift genannt. \n",
    "\n",
    "Je tiefer das neuronale Netz, umso größer sind diese Veränderungen, da es mehr Schichten gibt, die sich verändern können. Die Tiefe einer neuronalen Netzes erhöht also die Wahrscheinlichkeit das ein Vanishing Gradient Problem auftritt und ein effektives Training stoppt.\n",
    "\n",
    "Transformer wie sie [1] beschrieben nutzen hierfür Layer Normalisation ein Normalisierungsalgorithmus den [21] entwickelt hat.\n",
    "\n",
    "<span style=\"color:red\">Interactive Application - In- Output of LayerNorm Comparison funktioniert nicht richtig.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39a9d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd73d78f68b4e959baa768adfed1a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Button(description='Click to proceed', style=ButtonStyle())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VisualWrapper.reset_visualiser()\n",
    "tokenizer_norm = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input=\"Test\"\n",
    "\n",
    "tensor_input = tf.convert_to_tensor(input)\n",
    "if len(tensor_input.shape) == 0:\n",
    "    tensor_input = tensor_input[tf.newaxis]\n",
    "# tokenize and encode input\n",
    "# Identify end token of the input\n",
    "tokenized_input = tokenizer_norm.tokenize(tensor_input).to_tensor()\n",
    "input_without_eos = tokenized_input[:, :-1]\n",
    "context = model.model.encode(input_without_eos, None)\n",
    "\n",
    "VisualWrapper.visualize_data(id='layer')\n",
    "# Hier gibt es ein Problem, weil man das Modell nicht ohne die Layernorm laufen lassen kann. Hierfür muss erst eine Lösung implementiert werden.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57791aa1",
   "metadata": {},
   "source": [
    "\n",
    "### Residuale Verbindung\n",
    "\n",
    "Die Idee für das Nutzen von Residualen Verbindungen kommt von [22]. Die Autoren stellten fest, dass bei neuronalen Netzen sowohl die Genauigkeit während des Trainings als auch die Genauigkeit auf dem Testdatensatz mit zunehmender Tiefe schlechter wird.\n",
    "\n",
    "Da durch Normalisierung bereits sichergestellt ist, dass Vanishing Gradients kein Problem darstellen, scheitert die Optimierung der neuronalen Netze aus anderen Gründen.\n",
    "Einer der Gründe hierfür liegt vermutlich darin, dass die tieferen Schichten eines Modells zu Beginn des Trainings sehr viel stärker zur Ausgabe beitragen, als die vorhergehenden Schichten. Sie werden somit zuerst trainiert. Die weniger tiefen Schichten werden erst ausreichend trainiert, wenn in den tiefen Schichten keine Optimierung mehr möglich ist.\n",
    "\n",
    "Eine Lösung hierfür sind die residualen Verbindungen.\n",
    "Residuale Verbindungen ersetzen eine Schicht F(x) durch ihre residuale Verbindung\n",
    "\n",
    "H(x) = F(x)+x.\n",
    "\n",
    "In das Ergebnis von H(x) geht also sowohl der Output, als auch der Input von F direkt mit ein. Wendet man dieses Prinzip auf die Schichten tiefer neuronaler Netze an, sorgt das dafür, dass gleich zu Beginn, der Output der wenig tiefen Schichten relevant in den Output des gesamten Netzes einfließt, denn es gilt für das gesamte Netz N:\n",
    "\n",
    "N(x) = H_n(H_n-1(x)) + H_n-1(x) = H_n(H_n-1(x)) + H_n-1(H_n-2(x)) + … + H_2(H_1(x)) + H_1(x)\n",
    "\n",
    "Wie man in <a href=\"#fig:input\">Abbildung 1</a> sehen kann, haben alle Aufmerksamkeitsmodule, sowie alle Feed-Forward Schichten in einem Transformermodell eine residuale Verbindung.\n",
    "\n",
    "In der nachfolgenden Simulation können Sie sehen, wie sich die Ausgabe einer neuronalen Schicht verändert, wenn man ihr eine residuale Verbindung beifügt. Der Effekt auf den Trainingsprozess lässt sich dabei natürlich nur schwer darstellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb96ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaktiver Vergleich mit/ohne residuale Verbindung. Evtl. wäre es auch spannend den Trainingsprozess hier zu starten und Veränderungen dabei aufzuzeigen. Dies geht aber vermutlich interaktiv nicht, da es zu lange dauert und zu viele Ressourcen benötigt. Man könnte allerdings eine Aufzeichnung der zugehörigen Daten machen. Dies zu implementieren ist aber gerade nicht machbar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa40a6df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Modellschichten\n",
    "\n",
    "### Aufmerksamkeit\n",
    "\n",
    "Die Neuerung von Transformern im Vergleich zu vorangegangenen Lösungen für Neuronalen Maschinellen Übersetzen (NMT) ist es, allein auf die Aufmerksamkeit als Mechanismus für das Verarbeiten von Sprache zu setzen. Aufmerksamkeit wurde auch vorher schon von [15] zur Verbesserung von RNNs zur Übersetzung von Texten verwendet.\n",
    "\n",
    "Der Aufmerksamkeitsmechanismus, wie ihn [1] beschreiben, orientiert sich dabei an der Idee einer Suchanfrage des Ausgabetextes and sich selbst, bzw. den Eingabetext. Die Aufmerksamkeitsschicht bekommt dabei zwei oder eigentlich drei Eingaben: \n",
    "\n",
    "1. den Query (Q), \n",
    "2. den Key (K),\n",
    "3. den Value (V).\n",
    "\n",
    "In der Praxis erhalten aber Key und Value in Transformern immer dieselbe Eingabe und häufig sind Query, Key und Value sogar identisch. Aus welcher Quelle Query, Key und Value kommen unterscheidet unterschiedliche Formen von Aufmerksamkeit. So nennen wir Selbstaufmerksamkeit denjenigen Fall indem Q=K=V gilt und Kreuzaufmerksamkeit denjenigen Fall indem der Query aus der Ausgabe des Encoder besteht und Key und Value beide aus der Ausgabe eines Decoderblocks stammen.\n",
    "\n",
    "Um zu erklären, wie Aufmerksamkeit funktioniert, sollten wir aber zunächst davon ausgehen, dass Query-, Key- und Value-Eingabe verschieden sind. Ich schreibe bewusst von der Eingabe, da in jeder Aufmerksamkeitsschicht zunächst Query-, Key- und Value-Eingabe Q', K', V' mit Hilfe von gewichteten Matrix W^Q, W^K und W^V in Query, Key und Value \n",
    "\n",
    "Q= Q'\\*W^Q,  \n",
    "K = K'\\*W^K,  \n",
    "V = V'\\*W^V  \n",
    "\n",
    "umgewandelt werden. Diese gewichteten Matrizen W^Q, W^K, W^V sind die trainierbaren Gewichte der Aufmerksamkeitsschicht. Alle nachfolgenden Prozesse sind deterministisch. Das bedeutet, dass diese Umwandlungprozesse festelegen, welches Ergebnis die Aufmerksamkeitsschicht liefert.\n",
    "Betrachten wir aber, was passiert, wenn Query, Key und Value durch diese Matrizen gewichtet wurden. Eine grafisch aufbereitete Erklärung dessen, was ich beschreibe findet sich übrigens bei [9].\n",
    "\n",
    "#### Aufmerksamkeitsfunktion\n",
    "\n",
    "Die Aufmerksamkeitsfunktion bekommt die Eingaben Q, K, V aus der vorher beschriebenen Gewichtung und lautet:\n",
    "\n",
    "\tAttention(Q, K, V) = softmax(QK^T/sqr(d_k)) V\n",
    "\n",
    "Es wird also zuerst das Kreuzprodukt aus Q und K gebildet. Dieses Produkt wird mit sqr(*d_k*) skaliert, weshalb ist in folgendem [Kaptiel](#skalierung-mit-sqrd_k) nachzulesen, und auf dieses Ergebnis wird dann die Softmax-Funktion sigma(x) angewandt.\n",
    "Diese Funktion lässt sich am besten positionsweise beschreiben\n",
    "\n",
    "\tsigma(x)_i = exp(x_i)/sum_j=1^n(exp(x_j)) für i=1,...,n.\n",
    "\n",
    "Nennen wir also\n",
    "\n",
    "\tsoftmax(QK^T/sqr(d_k)) = Score(Q,K),\n",
    "\n",
    "dann ist \n",
    "\n",
    "\tAttention(Q, K, V) = Score(Q,K)*V \n",
    "\n",
    "eine Funktion, die V mit einem Vektor multipliziert, wobei für den Vektor gilt |Score(Q,K)| = 1.\n",
    "\n",
    "Score(Q,K) ist also ein Vektor exakt der Länge von V, der angibt, mit welchem Anteil jeder Eintrag von V in den Ausgabevektor Attention(Q,K,V) eingehen soll. Dabei summiert sich Score(Q,K) zu 1, ist also tatsächlich eine Gewichtung der Einträge von V.\n",
    "\n",
    "Da mit Score(Q,K) nun also eine Gewichtung besteht, wie stark die Ausgabe Attention(Q,K,V)_i von V_j abhängt, kann man dieses Verhältnis als Matrix grafisch darstellen. Dies geschieht in der Simulation unten, bei der für den vorgegebenen Satz eine Aufmerksamkeitsgewichtung aus einer der Aufmerksamkeitsschichten dargestellt wird. Der Eintrag in der i-ten Zeile und j-ten Spalte gibt dabei den einfluss von V_j auf Attention(Q,K,V)_i an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3afaf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729c289544a94c619f1774b38dcafb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.', conti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f6bd4046614c029ae67e23f6649a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Aufmerksamkeit berechnen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4d915d7bdf4a71a5930357b1b247c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_attn = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "attn_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_attn = widgets.Text(\n",
    "    value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_attn = widgets.Button(description='Aufmerksamkeit berechnen',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_attn = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_attn:\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 6\n",
    "        #output_widget_attn.clear_output()  # clear the previous output\n",
    "\n",
    "        # Convert input to tensor if it is not already\n",
    "        # Create a dynamic tensor to store output\n",
    "        # Make sure tensor_input is 2-D\n",
    "        tensor_input = tf.convert_to_tensor(input_widget_attn.value)\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        if len(tensor_input.shape) == 0:\n",
    "            tensor_input = tensor_input[tf.newaxis]\n",
    "        # tokenize and encode input\n",
    "        # Identify end token of the input\n",
    "        tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()\n",
    "        input_without_eos = tokenized_input[:, :-1]\n",
    "        context = transformer.encode(input_without_eos, None)\n",
    "\n",
    "        # Write the input tokens (excluding the last one) to the output array\n",
    "        for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "            output_array = output_array.write(i, value)\n",
    "\n",
    "        dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "        dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "        VisualWrapper.visualize_data(id='attention')\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 1\n",
    "            \n",
    "\n",
    "button_widget_attn.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_attn, button_widget_attn, output_widget_attn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "162b450c",
   "metadata": {},
   "source": [
    "\n",
    "#### Multi-headed Aufmerksamkeit\n",
    "\n",
    "In der Praxis hat sich bewährt parallel mehrere dieser Aufmerksamkeitsmechanismen durchzuführen. Dazu werden zu Beginn h gewichteten Matrizen \n",
    "\n",
    "W_i^Q, W_i^K, W_i^V i= 1,...,h \n",
    "\n",
    "eingeführt. Diese erzeugen also h verschiedene Matrixtripel \n",
    "\n",
    "Q_i, K_i, V_i i = 1,...,h\n",
    "\n",
    "und somit ergeben sich h verschiedene Ausgaben \n",
    "\n",
    "H_i = Attention(Q_i, K_i, V_i) i=1,...,h.\n",
    "\n",
    "Diese werden nun zu einer einzigen Ausgabe zusammengeführt, indem wir \n",
    "\n",
    "MultiHeadAttention(Q_in,K_in,V_in) = Concat(H_1,..., H_h)W^O \n",
    "\n",
    "berechnen. Dabei ist Concat(A_1,...,A_n) das hintereinanderschreiben mehrerer Matrizen und W^O eine weitere trainierbare Matrix, die die verschiedenen Ausgaben H_1,..., H,h gewichtet.\n",
    "Deshalb sehen wir in der obigen Ausgabe auch x verschiedene Matrizen, die Score(Q_i,K_i) darstellen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fec2a5af",
   "metadata": {},
   "source": [
    "\n",
    "### Maskieren\n",
    "\n",
    "Das Maskieren des Inputs ist eine wichtige funktionale Komponente der Transformerarchitektur. Beim Maskieren handelt es sich in Wirklichkeit um zwei Mechanismen, die zwar dieselbe Funktionsweise besitzen, aber sehr unterschiedliche Aufgaben in der Architektur besitzen. Einerseits das Subsequent Masking und das Padding Masking. Das Padding Masking stellt jediglich sicher, dass nur Positionen mit Inhalt vom Transformer verarbeitet werden, während das Subsequent Masking dafür sorgt, dass der Decoder des Transformers autoregressiv ist. Das bedeutet, bei einer Vorhersage für eine Ausgabe an der Position i, soll das Modell nur Informationen aus den Eingabepositionen 1,…,i-1 nutzen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e465d9",
   "metadata": {},
   "source": [
    "\n",
    "#### Padding Masking\n",
    "\n",
    "Das Padding Masking ist notwendig, da Transformer sequentielle Daten so verarbeiten, als ob sie eine fixe Länge *d_model* hätten. Das geschieht deshalb, weil Transformer so lernen können jede Position der Ausgabelänge *d_model* parallel vorherzusagen. \n",
    "\n",
    "Um während des Trainings auch Daten mit einer Länge größer oder kleiner *d_model* zu nutzen, werden längere Sequenzen abgeschnitten und kürzere mit Nullen aufgefüllt. Diese Nullen müssen dann mit Hilfe von Padding Masking für das Training irrelevant gemacht werden. Das geschieht indem man alle Positionen die eine Null enthalten für das Modell auf *-inf* setzt, sodass sie beim Gradient Descent nicht berücksichtigt werden.\n",
    "\n",
    "#### Subsequent Masking\n",
    "\n",
    "Das Subsequent Masking benutzt die gleich Technik des Werte auf *-inf* setzen. Dabei wird aber nicht das mit irrelevanten Informationen angefüllte Ende des Eingabevektors auf Null gesetzt, sondern es werden diejenige Werte von Score(Q, V) auf *-inf* gesetzt, die einen Wert V_j, für die Ausgabe i Attention(Q,K,V)_i gewichten würden, obwohl j>i gilt. Also es wird beschränkt, dass Score(Q, V) nur diejenigen Werte von V einbeziehen darf, die bei der Vorhersage für den i-ten Wert schon bekannt sind.\n",
    "\n",
    "Dass das relevant ist liegt daran, dass beim Training von Transformern ja der komplette Input gleichzeitig verwertet wird. Ein Satz wird als ganzes vom Transformer verarbeitet und er erzeugt eine Vorhersage, welches Token an einer bestimmten Position in diesem Satz vorkommt. Dabei erhält er in der Eingabe aber schon die Information, welches Token an dieser Position tatsächlich steht, da er ja den kompletten Satz als Eingabe bekommen hat. Dies muss nun also innerhalb des Modells mit einer Maskierung wieder rückgängig gemacht."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a0ea31",
   "metadata": {},
   "source": [
    "\n",
    "#### Verschiedene Aufmerksamkeitstypen\n",
    "\n",
    "In der Architektur werden verschiedene Aufmerksamkeitstypen unterschieden. Es gibt dabei zwei Variablen die beeinflussen, welche Art von Aufmerksamkeit wir verwenden. Die erste Variable ist woher die Eingaben Q', K' und V' kommen. Die zweite Variable ist die Maskierung, die wir auf Score(Q,K) anwenden.\n",
    "\n",
    "##### Selbst-Aufmerksamkeit\n",
    "\n",
    "Die Aufmerksamkeit nennen wir Selbst-Aufmerksamkeit, wenn gilt \n",
    "\n",
    "Q'=K'=V'. \n",
    "\n",
    "Wenn sich Score(Q,K) also bildlich gesprochen daraus ergibt, welche Aufmerksamkeit jede Position der Eingabe auf eine andere Position derselben Eingabe legt und diese Aufmerksamkeit auf die Eingabe selbst angewandt wird.\n",
    "\n",
    "##### Kreuz-Aufmerksamkeit\n",
    "\n",
    "Wie nennen die Aufmerksamkeit Kreuz-Aufmerksamkeit, wenn gilt \n",
    "\n",
    "K' = V' \n",
    "\n",
    "aber Q' von diesen beiden Werten verschieden ist.\n",
    "Wenn sich Score(Q,K) also daraus ergibt, welche Aufmerksamkeit jede Position einer Eingabe Q' auf die Positionen einer zweiten Eingabe K' gibt und dieser Aufmerksamkeitsscore auf die zweite Eingabe angewandt wird.\n",
    "\n",
    "Dies ist zum Beispiel in Transformern der Fall, wenn Q' sich aus der Ausgabe des Encoder ergibt und K' = V' ein Zwischenergebnis des Decoders ist.\n",
    "\n",
    "##### Maskierte Aufmerksamkeit\n",
    "\n",
    "Ein Fall von maskierter Aufmerksamkeit liegt dann vor, wenn bestimmte Werte von Score(Q,K) maskiert werden. Das ist zum Beispiel beim Subsequent Masking der Fall, hier wird Score(Q,K)i,j = -inf gesetzt für alle Einträge j>i. Dadurch wird verhindert, dass die Ausgabe Attention(Q,K,V)_i sich auf die Werte V_j, j>i stützt. Z.B. wird während des Trainings im Decoder dadurch verhindert, dass das Model lernt Informationen aus den zukünftigen Einträgen V_j, j>i zu benutzen, um V_i vorherzusagen. Man sieht gut in der Darstellung von Score(Q,K), dass die Werte für j>i dadurch meistens nahe bei 0 liegen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b92c7bb2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Skalierung mit sqr(d_k)\n",
    "\n",
    "Dasselbe Problem des Vanishing Gradient könnte innerhalb der Aufmerksamkeitsfunktion auftauchen, da hier softmax(QK^T)V berechnet wird, das Skalarprodukt QK^T mit d_model skaliert und die Softmaxfunktion\n",
    "    \n",
    "    sigma(z)_i = exp(z_i)/sum_j=1^N(exp(z_j)) für j=1,...,N\n",
    "\n",
    "somit leicht in einen saturierten Bereich mit extrem kleinen Gradienten gerät. Deshalb wird QK^T mit sqr(d_model) skaliert: softmax(QK^T/sqr(d_model)), um die Skalierung mit d_model zu minimieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae0c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaktive Anwendung vgl. eines Tensors vor und nach dem Maskieren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77e5aaf9",
   "metadata": {},
   "source": [
    "Zuletzt findet sich hier nun noch eine Simulation des kompletten Inferenzvorgangs innerhalb eines Transformermodells. In dieser Simulation kann man noch einmal alle vorher beschriebenen Schritte nachvollziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d3cf257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2256c9cd63b49da915c315b7afc134c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test sentence', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108f0cf1ec804d26975c5141a49c505c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run interactive inference', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a58709a5d694f39b0c46a5d54432021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_inf = widgets.Text(\n",
    "    value='Test sentence',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_inf = widgets.Button(description='Run interactive inference',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_inf = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_inf:\n",
    "        output_widget_inf.clear_output()  # clear the previous output\n",
    "        inference_model(input_widget_inf.value, interactive=True) # replace this with your function call\n",
    "        inference_model.print_results(visualisation=True)\n",
    "\n",
    "button_widget_inf.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_inf, button_widget_inf, output_widget_inf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82dc7ff0",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "[1] Vaswani, A. et al. Attention Is All You Need. Preprint at http://arxiv.org/abs/1706.03762 (2017).\n",
    "\n",
    "[2] Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Computation 9, 1735–1780 (1997).\n",
    "\n",
    "[3] Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks 5, 157–166 (1994).\n",
    "\n",
    "[4] Cho, K., van Merrienboer, B., Bahdanau, D. & Bengio, Y. On the Properties of Neural Machine Translation: Encoder–Decoder Approaches. in Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation 103–111 (Association for Computational Linguistics, 2014). doi:10.3115/v1/W14-4012.\n",
    "\n",
    "[6] Kaplan, J. et al. Scaling Laws for Neural Language Models. Preprint at http://arxiv.org/abs/2001.08361 (2020).\n",
    "\n",
    "[5] Goodfellow, I., Bengio, Y. & Courville, A. Deep learning. (The MIT Press, 2016).\n",
    "\n",
    "[7] Radford, A. et al. Language Models are Unsupervised Multitask Learners. (2019).\n",
    "\n",
    "[8] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Preprint at http://arxiv.org/abs/1810.04805 (2019).\n",
    "\n",
    "[9] Alammar, J. The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. https://jalammar.github.io/illustrated-transformer/ (2018).\n",
    "\n",
    "[10] Encoder-Decoder. Understanding The Model Architecture | by Naoki | Medium. https://naokishibuya.medium.com/transformers-encoder-decoder-434603d19e1.\n",
    "\n",
    "[11] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. (2014).\n",
    "\n",
    "[12] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[13] Gage, P. A New Algorithm for Data Compression. (1994).\n",
    "\n",
    "[14] Cho, K. et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Preprint at http://arxiv.org/abs/1406.1078 (2014).\n",
    "\n",
    "[15] Bahdanau, D., Cho, K. & Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate. Preprint at http://arxiv.org/abs/1409.0473 (2016).\n",
    "\n",
    "[16] OpenAI. GPT-4 Technical Report. Preprint at http://arxiv.org/abs/2303.08774 (2023).\n",
    "\n",
    "[17] Grefenstette, G. & Tapanainen, P. What is a word, What is a sentence? Problems of Tokenization.\n",
    "\n",
    "[18] Lin, Z. et al. A Structured Self-attentive Sentence Embedding. Preprint at http://arxiv.org/abs/1703.03130 (2017).\n",
    "\n",
    "[19] Schmidt, R. M. Recurrent Neural Networks (RNNs): A gentle Introduction and Overview. Preprint at http://arxiv.org/abs/1912.05911 (2019).\n",
    "\n",
    "[20] Ioffe, S. & Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Preprint at http://arxiv.org/abs/1502.03167 (2015).\n",
    "\n",
    "[21] Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer Normalization. Preprint at http://arxiv.org/abs/1607.06450 (2016).\n",
    "\n",
    "[22] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tf_simu_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
