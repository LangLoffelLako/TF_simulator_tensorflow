{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ab5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging and decorators\n",
    "import logging as log\n",
    "\n",
    "# tensorflow modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# necessary for visualization and user input\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact_manual, interactive, interact, VBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# modules from backend\n",
    "from interactive_inference_backend import ModelLoader, StoryTokenizer, \n",
    "from interactive_inference_backend import reserved_tokens, vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cff4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelLoader(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                            d_model=512,\n",
    "                            n_stacks=2,\n",
    "                            h_att=4,\n",
    "                            load_model=True,\n",
    "                            model_load_path=\"model_N2_h4_d512_t20230831-134344\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b25d199d",
   "metadata": {},
   "source": [
    "# Interaktive Erklärung der Transformerarchitektur\n",
    "## Einleitung\n",
    "### Kurzübersicht Transformer\n",
    "\n",
    "Transformer-Modelle sind eine von Vaswani et al. [^vaswani2017] vorgeschlagene Architektur für das Modellieren von sequentiellen Daten. Im Gegensatz zu den vorher genutzten Architekturen wie RNNs [Cite RNN] oder CNNs [Cite CNN] ermöglichen Transformer allerdings das parallele Verarbeiten der sequentiellen Daten während des Trainings und ermöglichen dadurch mit erheblich reduzierter Trainingszeit und -rechenkapazität sequentielle Daten zu verarbeiten [Cite comparisson].\n",
    "\n",
    "Um diese parallele Verarbeitung zu erreichen nutzen Transformermodelle Aufmerksamkeitsblöcke. Aufmerksamkeitsblöcke sind trainierbare Matrizen, die von einem Eingabevektor auf einen Ausgabevektor projizieren. Also eine sequentielle Datenstruktur in Vektorform in eine ebenfalls sequentielle Datenstruktur in Vektorform verarbeiten, indem eine Matrixmultiplikation auf dein Eingabevektor angewandt wird. Wie das funktioniert wird in [#Aufmerksamkeitsmechanismus] gezeigt.\n",
    "Diese Matrizen projizieren also im Idealfall von jedem Eintrag im Eingabevektor genau diejenige Information auf einen Eintrag im Ausgabevektor, die an der jeweiligen Stelle die Ausgabe beeinflussen soll. Da hierbei die Ausgabe i nicht von der Ausgabe i-1 abhängt, können wir parallel die komplette Ausgabe zu jeder Eingabe erzeugen und mit einem Gradient-Descent [Cite] Verfahren unser Modell trainieren\n",
    "Da wir während der Inferenz (Erklärung was Inferenz ist) von sequentiellen Daten aber meist nur die vorhergehende Daten nutzen möchten benutzen Transformer Architekturen eine Maske, die nach der Anwendung der Matrixmultiplikation alle Informationen, die nachfolgende Daten liefern herausfiltern, sodass schon während dem Training nur die Verbindung zwischen der aktuellen Position und den vorhergehenden gelernt wird.\n",
    "Durch wiederholen von Aufmerksamkeitsblöcken und einem nachfolgenden Feed-Forward Netzwerk entsteht so eine Architektur zur Verarbeitung sequenzieller Daten, die verglichen mit vorherigen Architekturen weniger Rechenkapazität benötigt und trotzdem eine wesentlich reduzierte Trainigszeit aufweist. [!Zitat]\n",
    "\n",
    "<span style=\"color:red\"> Hier eine grafische Darstellung von Eingabe- zu Ausgabevektor darstellen. Wie werden sequentielle Daten verarbeitet und wie macht das ein Transformer </span>\n",
    "\n",
    "### Ziel dieses interaktiven Artikels\n",
    "\n",
    "Ziel dieses interaktiven Artikels soll es sein die Architektur die [Vaswani2017] beschreiben in ihren einzelnen Komponenten darzustellen. Der Fokus liegt hierbei darauf die Prozesse, die während der Verarbeitung sequentieller Daten stattfinden, grafisch durch interaktive Anwendungen darzustellen der Nutzer den Einfluss unterschiedlicher Architekturbausteine auf verschiedene Eingaben anschaulich klar wird. Ziel ist es, die verschiedenen Bausteine so zu erklären, dass einem möglichen Anwender die Implementation einer Tranformer-Architektur, durch ein Verständnis des Nutzens der einzelnen Architekturbausteine, erleichtert wird.\n",
    "\n",
    "Der Artikel soll dazu dienen eine Implementierung ohne einschlägiges Vorwissen, z.B. im Kontext von KMUs die sich bisher noch nicht mit der Modellierung sequentieller Daten beschäftigt haben, zu ermöglichen und Programmierer in der Designentscheidungen in der Transformerarchitektur zu unterstützen.\n",
    "[Vaswani2017] und viele der auf ihrer Architektur aufbauenden wissenschaftlichen Arbeiten(Zitate), Erklärartikel oder -video (Zitate) beschränken sich darauf den Aufmerksamkeitsmechanismus ausführlich darzustellen. Dabei werden die Designentscheidungen für trainingsrelevante Elemente wie Dropout (Zitat), Residuale Verbindungen (Zitat) sowie die Beschreibung bereits etablierter Methoden wie Byte-Pair Encoding (zitat) als Einbettung oder Log-Softmax (Zitat) vernachlässigt. Diese Elemente sollen hier aber ebenfalls dargestellt werden.\n",
    "\n",
    "### Architekturübersicht\n",
    "\n",
    "In Abbildung 1 ist eine vollständige Darstellung aller Architekturelemente zu finden, die Teil der Transformerarchitektur sind. Dabei unterscheiden wir vier verschiedene Elemente. Prozesse und Methoden werden in Schwarz dargestellt. Die in diesen Prozessen generierte Daten werden in Blau dargestellt. Trainierbare Parameter werden in Gelb dargestellt. Hyperparameter, also festzulegende Eingabeparameter für das Modell werde in Grau dargestellt. Ebenfalls in Grau werden Eingabe- und Ausgabedaten dargestellt.\n",
    "Unsere Abbildung ist insofern komplett, als sie jeglichen Weg zeigen, den Daten durch das Modell nehmen können. Eine Darstellung in der Form eines Encode-Decoder Netzwerks [Encoder-Decoder], wie sie [Vaswani2017] und andere (Zitat) nutzen findet sich in Abbildung 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d914d5",
   "metadata": {},
   "source": [
    "\n",
    "##### Abbildung 1\n",
    "<a href=\"#fig:input\">Hier</a> sehen sie die komplette Darstellung der Architektur eines Transformermodells.\n",
    "\n",
    "<figure id=\"fig:input\" style=\"height: 700px;\">\n",
    "  <img src=\"./img/tf_arch_full.jpg\" style=\"height: 1400px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 1: Transformerarchitektur</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a6b568b",
   "metadata": {},
   "source": [
    "\n",
    "#### Encoder-Decoder\n",
    "\n",
    "Eine Encoder-Decoder Architektur besteht aus zwei voneinander getrennten neuronalen Netzen dem Encoder und dem Decoder, die aber zusammen trainiert werden. Diese Architektur wurde von [Cho 2014 Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation] vorgeschlagen und dann vor allem im Kontext von Neural Machine Translation (NMT) verwendet. Der Vorteil von Encoder-Decoder liegt darin, dass man sowohl den Encoder als auch den Decoder ersetzen kann. Im Idealfall könnte man im Bereich von NMT pro Sprache einen Encoder und einen Decoder trainieren, um dann beliebige Übersetzungen zwischen allen Sprachen zu generieren. Da auch [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE], die zuerst Aufmerksamkeitsmechanismen für Rekurrente Neuronale Netze (RNN) vorgeschlagen haben, eine Encoder-Decoder Architektur verwenden, verwundert es nicht, dass auch [Attention Is All You Need] eine solche Architektur wählten. Insbesondere da [Attention is All You Need] Transformer ebenfalls für NMT benutzen.\n",
    "Transformer zur Textvervollständigung können auch ausschließlich auf Decodern beruhen. In unserem Beispiel wird der Encoder zum enkodieren der Eingabe verwendet, um dann im Decoders die Eingabe als Anfang der Ausgabe zu verwenden. In Abbildung 2 sieht man wie Encoder und Decoder zusammenarbeiten. Der Decoder benutzt Selbstaufmerksamkeit, um seinen Input zu verarbeiten, nutzt dann <span style=\"color:red\">Source-Attention</span>, für den nächsten Schritt, um final erneut Selbstaufmerksamkeit zu benutzen, wohingegen der Encoder ausschließlich über Selbstaufmerksamkeit funktioniert. Näheres siehe im Kapitel Attention.\n",
    "\n",
    "##### Abbildung 2\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Darstellung des Encoder-Decoder Prinzips\n",
    "Darstellung der Layer im Encoder und Decoder\n",
    "</span>\n",
    "Hier können Sie nun einen Beispielsatz zuerst vom Encoder kodieren lassen, um ihn dann im nächsten Schritt vom Decoder dekodieren zu lassen und damit eine Ausgabe zu erhalten.\n",
    "<span style=\"color:red\">Interaktive Anwendung Encoder dann Decoder</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4798807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c4fd313ff84e9dae82b2cb14669780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Encoder-Decoder Test', continuous_update=False, description='Ihre Eingabe:', layout…"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "transformer = model.model\n",
    "\n",
    "input_widget_enc_dec = widgets.Text(\n",
    "    value='Encoder-Decoder Test',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_enc = widgets.Button(description='Enkodiere die Eingabe',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "button_widget_dec = widgets.Button(description='Dekodiere die Eingabe',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_enc = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "output_widget_dec = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click_enc(b):\n",
    "  with output_widget_enc:\n",
    "    output_widget_enc.clear_output()  # clear the previous output\n",
    "    #output_widget_dec.clear_output()\n",
    "\n",
    "    # Convert input to tensor if it is not already\n",
    "    # Create a dynamic tensor to store output\n",
    "    # Make sure tensor_input is 2-D\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)\n",
    "    if len(tensor_input.shape) == 0:\n",
    "      tensor_input = tensor_input[tf.newaxis]\n",
    "    # tokenize and encode input\n",
    "    # Identify end token of the input\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    context = transformer.encode(input_without_eos, None)\n",
    "\n",
    "    VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(context)\n",
    "\n",
    "def on_button_click_dec(b):\n",
    "  with output_widget_dec:\n",
    "    output_widget_dec.clear_output()\n",
    "\n",
    "    # Convert input to tensor if it is not already\n",
    "    # Create a dynamic tensor to store output\n",
    "    # Make sure tensor_input is 2-D\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    if len(tensor_input.shape) == 0:\n",
    "      tensor_input = tensor_input[tf.newaxis]\n",
    "    # tokenize and encode input\n",
    "    # Identify end token of the input\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    context = transformer.encode(input_without_eos, None)\n",
    "\n",
    "     # Write the input tokens (excluding the last one) to the output array\n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "      output_array = output_array.write(i, value)\n",
    "\n",
    "    dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "    dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "    VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(dec_out)\n",
    "\n",
    "button_widget_enc.on_click(on_button_click_enc)\n",
    "button_widget_dec.on_click(on_button_click_dec)\n",
    "\n",
    "widgets.VBox([input_widget_enc_dec, widgets.HBox([widgets.VBox([button_widget_enc, output_widget_enc]), widgets.VBox([button_widget_dec, output_widget_dec])])])\n",
    "#display(input_widget_enc_dec, button_widget_enc, output_widget_enc, button_widget_dec, output_widget_dec)\n",
    "\n",
    "\n",
    "#print('tok_out', tokenized_input)\n",
    "#print('enc_out', context)\n",
    "#print(\"dec_out\", dec_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028a3317",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Architekturblöcke\n",
    "\n",
    "Prinzipiell lassen sich Transformer in mehrere Blöcke einteilen.\n",
    "\n",
    "1. Eingabeblock\n",
    "\n",
    "Die Eingabepipeline verarbeitet die Eingabedaten in eine Form die für die Matrixtransformation im Aufmerksamkeitsblock genutzt werden kann. Diese unterscheidet sich für verschiedene Datentypen. In unserem Beispiel nutzen wir Textdaten, die wir durch Tokenisierung und ein Embedding in Tensoren verwandeln.\n",
    "\n",
    "2. Aufmerksamkeitsblock\n",
    "\n",
    "Der Aufmerksamkeitsblock verarbeitet Daten in Tensorform und liefert somit eine Abbildung von den Eingabetensoren auf die Ausgabetensoren, die jeweils von den Eingabe- und Ausgabeblöcken interpretiert wird.\n",
    "Man kann verschiedene Modulvarianten innerhalb eines Aufmerksamkeitsblocks unterscheiden. Aufmerksamkeitsmodule erhalten als Eingabedaten immer drei Tensoren. Diese werden Query, Key und Value genannt. Aus diesen berechnet ein Aufmerksamkeitsmodul eine Ausgabe.\n",
    "Typischerweise werden Aufmerksamkeitsmodule dabei unterschieden aus welcher Quelle Query, Key und Value stammen. Es gibt Self-Attention bei der Query, Key und Target alle aus einer Quelle stammen, Source-Attention (Hier ist der Query aus einer anderen Quelle als Key und Value, z.B. wenn der Query die Ausgabe eines Encoders ist, während Target die Ausgabe eines Decoderblocks ist).\n",
    "Desweiteren kann man nach Art des Maskings unterscheiden. Dieses geschieht um  das Aufmerksamkeitsmodul daran zu hindern aus einem Teil der Ausgabedaten zu lernen (z.B. möchte man bei sequentieller Datenverarbeitung verhindern, dass ein Decoderaufmerksamkeitsblock für die Vorhersage der Position i Daten aus den Positionen i+j nutzt.)\n",
    "\n",
    "3. Ausgabeblock\n",
    "\n",
    "Die Ausgabepipeline interpretiert die Ausgaben des Aufmerksamkeitsmoduls, sodass sie in eine für menschlichen Gebrauch nützlichen Form vorliegen (typischerweise z.B. Textdaten, Bilddaten, etc.)\n",
    "\n",
    "In Abbildung 1 entspricht das Aufmerksamkeitsmodul den Prozessen innerhalb der grauen Umrandung, während die Eingabe- und Ausgabepipeline darunter bzw. darüber zu finden sind.\n",
    "\n",
    "## Input\n",
    "\n",
    "Der erste Teil eines Transformermodells besteht aus der Eingabepipeline. Diese verarbeitet die Eingabe in das Modells, z.B. die Texteingabe eines Nutzers und bereitet sie darauf vor durch wiederholte Anwendung der Aufmerksamkeitsmodule in einem Transformermodell verarbeitet zu werden. Die Aufmerksamkeitsmodule arbeiten über eine Aufmerksamkeitsmatrix, die aus der jeweiligen Eingabe eine Ausgabe berechnet. Wir müssen also aus einer Eingabe in Textform eine Vektorrepräsentation erzeugen, die alle notwendigen Informationen für ein Modell enthalten, um nützliche Vorhersagen über eine Übersetzung oder eine Textvervollständigung machen zu können.\n",
    "Hier sehen Sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4caafaa5",
   "metadata": {},
   "source": [
    "\n",
    "##### Abbildung 3\n",
    "<a href=\"#fig:input\">Hier</a> sehen sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt.\n",
    "\n",
    "<figure id=\"fig:input\" style=\"height: 700px;\">\n",
    "  <img src=\"./img/tf_input_pipeline.jpg\" style=\"height: 700px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4578ca",
   "metadata": {},
   "source": [
    "\n",
    "Wie zu erkennen ist, werden während des Training eines Transformermodells ausschließlich die Gewichte für die Einbettung in einen der Modellgröße entsprechenden Vektor mittrainiert.\n",
    "Prinzipiell besteht die Eingabepipeline aus drei Modulen:\n",
    "\n",
    "1. Tokenisierung,\n",
    "2. Einbettung,\n",
    "3. Positionelle Kodierung.\n",
    "\n",
    "Dies entspricht den drei Schritten:\n",
    "\n",
    "1. Umwandlung von Text in eine Zahlenkodierung desselben Textes,\n",
    "2. Kodierung dieser\n",
    "<span style=\"color:red\">fortführen!</span>\n",
    "\n",
    "### Tokenisierung\n",
    "\n",
    "Bei der Tokenisierung wird der Satz in Textform z.B. \"Das ist ein Testsatz.\" in einen Zahlencode verwandelt. Hierfür kommen verschieden Methoden in Frage. Einer der simpelsten Methoden ist es z.B. jedem Buchstaben eine Zahl zuzuordnen. Das führt allerdings zu einer sehr langen Kodierung. Die entscheidenden Faktoren für eine gute Kodierung sind Vollständigkeit der Kodierung, Länge des kodierten Vektors und Größe des dafür nötigen Vokabulars.\n",
    "Die Kodierung mit einzelnen Buchstaben ist vollständig (man kann beliebige Zeichenkombinationen kodieren) und besitzt ein kurzes Vokabular (26 für alle Buchstaben plus alle Punktierungs und Sonderzeichen, die im Text vorkommen), allerdings ist die Länge der kodierten Vektoren groß. Andererseits könnte man ein Vokabular an Wörtern nehmen, die führt zu einer viel kürzeren Kodierung, allerdings besteht die Gefahr der Unvollständigkeit und für jedes Wort muss zur Kodierung in einem sehr großem Vokabular nachgeschlagen werden.\n",
    "Aktuelle Implementationen verwenden Optionen wie das Byte-Pair Encoding. {Cite Sennrich et al. 2016 and Gage 1994}.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b56024be",
   "metadata": {},
   "source": [
    "Den Tokenizer findet man in unserer <a href=\"#fig:transformer\">Abbildung 1</a> ganz unten und ist der erste Schritt, um eine Eingabe zu verarbeiten\n",
    "<figure id=\"fig:tokenizer\" style=\"height: 300px;\">\n",
    "  <img src=\"./img/tf_tokenizer.jpg\" style=\"height: 300px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a5e1d1",
   "metadata": {},
   "source": [
    "\n",
    "### Byte-Pair Encoding\n",
    "\n",
    "Byte-Pair Encoding nutzt ein Vokabular mit einer festgelegten Länge. In unserer Implementation des Tokenizer nutzt er ein Vokabular von 8000 Einheiten. Das Vokabular wird folgendermaßen erstellt:\n",
    "\n",
    "  1. Ein Text, der für die Erstellung des Vokabulars verwendet wird, wird in eine Sequence von Buchstaben und Symbole zerlegt. Wortenden werden mit einem zusätzlichen Symbol kodiert.\n",
    "  2. Alle Buchstaben und Symbole werden in das Vokabular aufgenommen.\n",
    "  3. Nun wird das häufigste 2-Gramm, also zwei aufeinander folgende Symbole, gesucht, das im Text zu finden ist.\n",
    "  4. Dieses wird ins Vokabular aufgenommen und im Text durch ein einzelnes Symbol ersetzt.\n",
    "  5. Dieser Prozess wird nun wiederholt, bis die vorgegebene Länge des Vokabulars erreicht ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60193c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42291628b094f1dbea84a0c4f455ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Tokenizer test', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cef96c56f0461f8b144a918d93e121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run tokenizer on input', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b87c49211fe48618b7a094a9732c8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_tok = widgets.Text(\n",
    "    value='Tokenizer test',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_tok = widgets.Button(description='Run tokenizer on input',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_tok = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_tok:\n",
    "        output_widget_tok.clear_output()  # clear the previous output\n",
    "        tokens = tokenizer.tokenize(input_widget_tok.value)\n",
    "        lookup = tokenizer.lookup(tokens)\n",
    "\n",
    "        VisualWrapper.display_text('Tokens die aus der Eingabe mit Byte-Pair Encoding extrahiert werden:'.rjust(100) + ', '.join([token.decode('utf-8').rjust(10) for token in lookup.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "        VisualWrapper.display_text('Ihre Positionsnummer im Alphabet des Byte-Pair Encoding Algorithmus:'.rjust(100) + ', '.join([str(token).rjust(10) for token in tokens.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "\n",
    "button_widget_tok.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_tok, button_widget_tok, output_widget_tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f9693a5",
   "metadata": {},
   "source": [
    "In our test example you can see, how the input string is separated into tokens and then converted into numerical values, depending on the position the token has in our vocabulary.\n",
    "As you can see our byte-pair encoding vocabulary is extended by an [START] and [END] token, and it also contains elements of type 'abc##' or '##abc'. These represent a sequence at the start or end of a word."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e5b84a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Abbildung 4\n",
    "\n",
    "\t<span style=\"color:red\">\n",
    "  Tokenizer\n",
    "  </span>\n",
    "\n",
    "Gage - A New Algorithm for Data Compression.pdf\n",
    "Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf\n",
    "<span style=\"color:red\">\n",
    "In our test example you can see, how the input string is separated into tokens and then converted into numerical values, depending on the position the token has in our vocabulary.\n",
    "As you can see our byte-pair encoding vocabulary is extended by an [START] and [END] token, and it also contains elements of type 'abc##' or '##abc'. These represent a sequence at the start or end of a word.\n",
    "</span>\n",
    "\n",
    "### Input Embedding\n",
    "\n",
    "Die Einbettung sorgt dafür, dass die beliebig lange Sequenzen die durch den Tokenizer entsteht in einen Vektor der Modellgröße d_model kodiert werden. Das heißt jedes Token, das zuvor durch eine Zahl kodiert wurde, die der Position entspricht, die das jeweilige Token in einem (in unserer Implementierung 8000 Wörter langen) Tokenwörterbuch zugewiesen bekommen hat, wird nun durch einen Vektor der Länge d_model kodiert. So entsteht ein Tensor der Dimension Anzahl enkodierte Tokens d_model. Die Einbettung ist Teil der vom Modell gelernten Parameter, wie man in der Übersichtsgrafik sehen kann.\n",
    "Wie man in <a href=\"#fig:embedding\">Abbildung 4</a> sehen kann besitzt die Einbettung trainierbare Gewichte und ist damit Teil der während des Training gelernten Parameter des Modells.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab2ec3f9",
   "metadata": {},
   "source": [
    "\n",
    "##### Abbildung 5\n",
    "\n",
    "Wie man in <a href=\"#fig:embedding\">Abbildung 4</a> sehen kann besitzt die Einbettung trainierbare Gewichte und ist damit Teil der während des Training gelernten Parameter des Modells.\n",
    "<figure id=\"fig:embedding\" style=\"height: 300px;\">\n",
    "  <img src=\"./img/tf_embedding.jpg\" style=\"height: 300px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 4: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "758a1d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9ac6dc4a9747679d78a9de185079b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p style=\"font-size:18px; color:blue;\">Hier kannst du einen Text einbetten lassen. Wenn du die Ein…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c7f2c7561e4da0803975bd042d53c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Einbettung Test', continuous_update=False, description='Ihre Eingabe:', layout=Layout(margin='0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b86a6b644443549628e0a871bc9926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Einbettung erstellen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34c705802c44190b1f0706ef2764d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EmbeddingExample():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "        self.input_widget = widgets.Text(\n",
    "            value = 'Einbettung Test',\n",
    "            description = 'Ihre Eingabe:',\n",
    "            continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "            layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "        )\n",
    "\n",
    "        self.button_widget = widgets.Button(description='Einbettung erstellen',\n",
    "                                    layout = widgets.Layout(width='auto'))\n",
    "\n",
    "        self.output_widget = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "        self.old_context = None\n",
    "\n",
    "    def on_button_click(self, b):\n",
    "        with self.output_widget:\n",
    "            self.output_widget.clear_output()  # clear the previous output\n",
    "            VisualWrapper.reset_visualiser()\n",
    "            tokens = self.tokenizer.tokenize(self.input_widget.value)\n",
    "            input_without_eos = tokens[tf.newaxis, :, :-1]\n",
    "            context = model.model.enc_embed(input_without_eos)\n",
    "            VisualWrapper.display_text('So sieht die Einbettung der Eingabe aus.')\n",
    "            VisualWrapper.color_bar(context.to_tensor())\n",
    "\n",
    "            if self.old_context is not None:\n",
    "                padded_context, padded_old_context = self.pad_tensors(context, self.old_context)\n",
    "\n",
    "                VisualWrapper.display_text('So unterscheiden sich die alte und die neue Einbettung voneinander.')\n",
    "                context_diff = padded_context - padded_old_context\n",
    "                VisualWrapper.color_bar(context_diff)\n",
    "\n",
    "            self.old_context = context\n",
    "    \n",
    "    def pad_tensors(self, ragged_tensor1, ragged_tensor2):\n",
    "        # Convert ragged tensors to normal tensors, padding with zeros\n",
    "        tensor1 = ragged_tensor1.to_tensor()\n",
    "        tensor2 = ragged_tensor2.to_tensor()\n",
    "\n",
    "        # Calculate the shapes of the tensors\n",
    "        shape1 = tf.shape(tensor1)\n",
    "        shape2 = tf.shape(tensor2)\n",
    "\n",
    "        # Initialize a list for the target shape\n",
    "        target_shape = []\n",
    "\n",
    "        # Iterate over the dimensions of the tensors\n",
    "        for i in range(shape1.shape[0]):\n",
    "            # Append the maximum size of the dimension to the target shape\n",
    "            target_shape.append(tf.maximum(shape1[i], shape2[i]))\n",
    "\n",
    "        # Convert the target shape to a tensor\n",
    "        target_shape = tf.stack(target_shape)\n",
    "\n",
    "        # Initialize lists for the paddings of the tensors\n",
    "        paddings1 = []\n",
    "        paddings2 = []\n",
    "\n",
    "        # Iterate over the dimensions of the tensors\n",
    "        for i in range(shape1.shape[0]):\n",
    "            # Append the required padding for the dimension to the paddings\n",
    "            paddings1.append([0, target_shape[i] - shape1[i]])\n",
    "            paddings2.append([0, target_shape[i] - shape2[i]])\n",
    "\n",
    "        # Convert the paddings to tensors\n",
    "        paddings1 = tf.stack(paddings1)\n",
    "        paddings2 = tf.stack(paddings2)\n",
    "\n",
    "        # Pad the tensors to the target shape\n",
    "        tensor1_padded = tf.pad(tensor1, paddings1)\n",
    "        tensor2_padded = tf.pad(tensor2, paddings2)\n",
    "\n",
    "        return tensor1_padded, tensor2_padded\n",
    "\n",
    "emb_ex = EmbeddingExample()\n",
    "\n",
    "VisualWrapper.display_text('Hier kannst du einen Text einbetten lassen. Wenn du die Eingabe veränderst wird außerdem gezeigt, wie sich die Einbettung geändert hat.')\n",
    "\n",
    "emb_ex.button_widget.on_click(emb_ex.on_button_click)\n",
    "display(emb_ex.input_widget, emb_ex.button_widget, emb_ex.output_widget)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e70583",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Positionale Kodierung\n",
    "\n",
    "Da in der Einbettung keine Informationen über die Position der verschiedenen Worte kodiert wird, muss diese zusätzlich kodiert werden. Hierfür benutzt die Transformerarchitektur für jede Position der Einbettung (also jedes enkodierte Wort) eine Sinuskurve mit anderer Frequenz und Phase. Hier ist zu sehen, wie die positionale Kodierung für eine 2048 Vektoren lange und 512 Einträge tiefe Einbettung aussieht.\n",
    "\n",
    "Wie man erkennen kann, basiert die Kodierung darauf, dass Sinuskurven mit kurzer Frequenz eine Unterscheidung von Positionen ermöglichen, die nahe beieinander liegen, da ihre Werte für benachbarte ganze Zahlen sehr verschiedene Werte liefern. Sinuskurven mit langer Frequenz unterscheiden sich erst, wenn zwei Positionen, bzw. die ganze Zahlen, die sie repräsentieren weit voneinander entfernt liegen, dadurch können auch weiter voneinander entfernt liegende Positionen in Relation gesetzt werden. In die Einbettung werden diese verschiedenen Sinuskurven eingefügt, indem die Frequenz der hinzugefügten Sinuskurve von der Tiefe der Einbettung bestimmt wird. In unserer Grafik oben wird also jeweils eine Zeile der Einbettung an der jeweiligen Position hinzugefügt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d875186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ab0d11ff804d82aff487fb126c1c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1025, description='length', max=2048, min=2), IntSlider(value=257, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(length=(2,2048,1), depth=(2,512,1))\n",
    "def print_pos_enc(length, depth):\n",
    "    VisualWrapper.color_bar(positional_encoding(length, depth))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389fa0d1",
   "metadata": {},
   "source": [
    "\n",
    "## Trainingsmethoden\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout ist eine Methode, die während des Trainingsprozesses eines neuronalen Netzes genutzt wird, um zu verhindern, dass die gelernte Gewichtung eines Modells in einem der Module des Modells zu sehr auf einen einzelnen Prädikator stützt. Dafür werden zwischen zwei Schritten desselben Modells, die trainierbare Gewichte enthalten eine Dropout-Layer eingefügt. Diese setzt zufällig einige der vom Modell generierten Werte auf einen vordefinierten Wert (meistens 0), um den nachfolgenden Schichten diese Informationen vorzuenthalten. Da diese Operation zufällig erfolgt, müssen die nachfolgenden Teile des Modells auf eine möglichst breite Kombination aus Merkmalen setzen, um seine Vorhersagen zu treffen. Somit kann man verhindern, dass Vorhersagen nur aufgrund eines einzigen Merkmals der vorherigen Ausgabe gemacht werden.\n",
    "Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b358029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4a18db61f94d30ab4e0efcabfe33f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=30, continuous_update=False, description='Länge des Tensors:', max=2048, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c410f28e64884efd98fa412be886b4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=512, continuous_update=False, description='Tiefe des Tensors:', max=512, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c54670f0da4a6abb5a2eca7bc48011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.1, continuous_update=False, description='Dropoutrate:', max=0.9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c9a6e9b4874aa287587493788ed27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Tes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f77d277bf7f4300a0debbaefedc1990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_drop = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_drop = widgets.Text(value = 'Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test',\n",
    "                                 description = 'Ihre Eingabe:',\n",
    "                                 continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                                 layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "                                 )\n",
    "\n",
    "length_widget_drop = widgets.IntSlider(value=30,\n",
    "                           min=2,\n",
    "                           max=2048,\n",
    "                           description='Länge des Tensors:',\n",
    "                           continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                           )\n",
    "depth_widget_drop = widgets.IntSlider(value=512,\n",
    "                          min=2,\n",
    "                          max=512,\n",
    "                          description='Tiefe des Tensors:',\n",
    "                          continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                          )\n",
    "dropout_widget = widgets.FloatSlider(value=0.1,\n",
    "                              min=0,\n",
    "                              max=0.9,\n",
    "                              step=0.1,\n",
    "                              description='Dropoutrate:',\n",
    "                              continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                              )\n",
    "\n",
    "def out(length, depth, dropout, input):\n",
    "    VisualWrapper.reset_visualiser()\n",
    "    dropout_layer = layers.Dropout(dropout)\n",
    "    one_tensor = tf.ones([length, depth])\n",
    "    dropout_tensor = dropout_layer(one_tensor, training=True)\n",
    "    VisualWrapper.color_bar(dropout_tensor)\n",
    "\n",
    "    tokens = tokenizer_drop.tokenize(input)\n",
    "    input_without_eos = tokens[tf.newaxis, :, 1:-1]\n",
    "    context = model.model.enc_embed(input_without_eos)\n",
    "    context_drop = dropout_layer(context, training=True)\n",
    "    VisualWrapper.display_text('Für normale Dropoutwerte zwischen 0 und 0.3 sieht man die Veränderungen an tatsächlichen Vektoren nur schlecht, da viele Werte eines Tensors schon nahe bei 0 liegen.')\n",
    "    VisualWrapper.color_bar(context.to_tensor())\n",
    "    VisualWrapper.color_bar(context_drop.to_tensor())\n",
    "    \n",
    "\n",
    "output_widget_dropout = widgets.interactive_output(out,\n",
    "                                                   {'length': length_widget_drop, 'depth': depth_widget_drop, 'dropout': dropout_widget, 'input': input_widget_drop}, \n",
    "                                                   )\n",
    "\n",
    "display(length_widget_drop, depth_widget_drop, dropout_widget, input_widget_drop, output_widget_dropout)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2d169f9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Normalisierung\n",
    "\n",
    "Normalisierung ist eine Technik, die von [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift] eingeführt wurde. In tiefen neuronalen Netzen, mit nicht-linearen Aktivierungsfunktionen wie der Sigmoid-Funktion g(x) = 1/(1+exp(-x)) trainiert werden gilt, das g’(x) -> 0 für |x|-> inf. Das führt dazu, dass diese Funktionen in einen Bereich geraten können, in der der Gradient für Stochastic Gradient Descent (SGD) minimal wird und man spricht vom Vanishing Gradient Problem. In tiefen neuronalen Netzen ergibt sich hierbei das Problem, dass eine Layer z = g(Wx + b) mit der Sigmoid-Funktion g versucht den Output des gesamten vorherigen Netzes x zu gewichten. Dabei hängen sowohl W als auch b von x ab. Da sich während des Trainings alle Layers des Netzes fortwährend aktualisieren, ändert sich auch der Input x fortwährend. Die Tiefe eines neuronalen Netzes erhöht die Wahrscheinlichkeit für einen Vanishing Gradient. Dieser Effekt wird von [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift] Internal Covariate Shift genannt. \n",
    "\n",
    "Ba et al. - 2016 - Layer Normalization.pdf\n",
    "1502.03167.pdf\n",
    "<span style=\"color:red\">Interactive Application - In- Output of LayerNorm Comparison</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39a9d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca781f84036840d1a771b6938b082b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Button(description='Click to proceed', style=ButtonStyle())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VisualWrapper.reset_visualiser()\n",
    "tokenizer_norm = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input=\"Test\"\n",
    "\n",
    "tensor_input = tf.convert_to_tensor(input)\n",
    "if len(tensor_input.shape) == 0:\n",
    "    tensor_input = tensor_input[tf.newaxis]\n",
    "# tokenize and encode input\n",
    "# Identify end token of the input\n",
    "tokenized_input = tokenizer_norm.tokenize(tensor_input).to_tensor()\n",
    "input_without_eos = tokenized_input[:, :-1]\n",
    "context = model.model.encode(input_without_eos, None)\n",
    "\n",
    "VisualWrapper.visualize_data(id='layer')\n",
    "# Hier gibt es ein Problem, weil man das Modell nicht ohne die Layernorm laufen lassen kann. Hierfür muss erst eine Lösung implementiert werden.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95cda063",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Skalierung mit sqr(d_k)\n",
    "\n",
    "Dasselbe Problem des Vanishing Gradient könnte innerhalb der Aufmerksamkeitsfunktion auftauchen, da hier softmax(QK^T)V berechnet wird, das Skalarprodukt QK^T mit d_model skaliert und die Softmaxfunktion\n",
    "    \n",
    "    sigma(z)_i = exp(z_i)/sum_j=1^N(exp(z_j)) für j=1,...,N\n",
    "\n",
    "somit leicht in einen saturierten Bereich mit extrem kleinen Gradienten gerät. Deshalb wird QK^T mit sqr(d_model) skaliert: softmax(QK^T/sqr(d_model)), um die Skalierung mit d_model zu minimieren.\n",
    "\n",
    "### Residuale Verbindung\n",
    "\n",
    "Die Idee für das Nutzen von Residualen Verbindungen kommt von [He et al. - 2015 - Deep Residual Learning for Image Recognition]. Die Autoren stellten fest, dass bei tiefen neuronalen Netzen (Tiefe meint hier die Anzahl an Schichten des neuronalen Netzes) sowohl die Genauigkeit während des Trainings als auch die Genauigkeit auf dem Testdatensatz mit zunehmender Tiefe schlechter wird.\n",
    "Da durch Normalisierung bereits sichergestellt ist, dass schwindende oder explodierende Gradienten kein Problem darstellen, scheitert die Optimierung der neuronalen Netze aus anderen Gründen.\n",
    "Einer der Gründe hierfür liegt vermutlich darin, dass die tieferen Schichten eines Modells zu Beginn des Trainings sehr viel stärker zur Ausgabe beitragen, als die vorhergehenden Schichten. Sie werden somit zuerst trainiert. Die weniger tiefen Schichten werden erst ausreichend trainiert, wenn in den tiefen Schichten keine Optimierung mehr möglich ist.\n",
    "Residuale Verbindungen ersetzen eine Schicht F(x) durch ihre residuale Verbindung H(x) = F(x)+x. In das Ergebnis von H(x) geht also sowohl der Output, als auch der Input von F direkt mit ein. Wendet man dieses Prinzip auf die Schichten tiefer neuronaler Netze an, sorgt das dafür, dass gleich zu Beginn, der Output der wenig tiefen Schichten relevant in den Output des gesamten Netzes einfließt, denn es gilt für das gesamte Netz N:\n",
    "N(x) = H_n(H_n-1(x)) + H_n-1(x) = H_n(H_n-1(x)) + H_n-1(H_n-2(x)) + … + H_2(H_1(x)) + H_1(x) \n",
    "Wie man in Abbildung 1 sehen kann, haben alle Aufmerksamkeitsmodule, sowie alle Feed-Forward Schichten eine residuale Verbindung.\n",
    "Hier können Sie ausprobieren, wie groß die Veränderung ist, die eine residuale Verbindung einer Feed-Forward Layer aus unserem Transformermodell hinzufügt:\n",
    "\n",
    "<span style=\"color:red\">Interaktiver Vergleich mit/ohne residuale Verbindung</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afb96ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier soll ein interaktiver Vergleich zwischen einer Schicht ohne/mit residualen Verbindungen stehen. Das geht allerdings nur, wenn man diese deaktivieren kann. Das muss implementiert werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa40a6df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Modellschichten\n",
    "\n",
    "### Aufmerksamkeit\n",
    "\n",
    "Die Neuerung von Transformern im Vergleich zu vorangegangenen Lösungen für NMT ist es, allein auf den Mechanismus als Architektur für das Verarbeiten von Sprache zu setzen. Aufmerksamkeit wird auch schon von [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE] zur Verbesserung von RNNs zur Übersetzung von Texten verwendet.\n",
    "Der Aufmerksamkeitsmechanismus, den [Attention is all you need] beschreiben orientiert sich von der Idee dabei an einer Suchanfrage. [Buch zitieren] Die Aufmerksamkeitsschicht bekommt dabei zwei oder eigentlich drei Eingaben: den Query (Q), den Key (K) und den Value (V). In der Praxis erhalten aber Key und Value in Transformern immer dieselbe Eingabe und häufig sind Query, Key und Value sogar identisch. Aus welcher Quelle Query, Key und Value kommen unterscheidet unterschiedliche Formen von Aufmerksamkeit. So nennen wir Selbstaufmerksamkeit denjenigen Fall indem Q=K=V gilt und Kreuzaufmerksamkeit denjenigen Fall indem der Query aus der Ausgabe des Encoder besteht und Key und Value beide aus der Ausgabe eines Decoderblocks.\n",
    "\n",
    "Um zu erklären, wie Aufmerksamkeit funktioniert, sollten wir aber zunächst davon ausgehen, dass Query-, Key- und Value-Eingabe verschieden sind. Ich schreibe bewusst von der Eingabe, da in jeder Aufmerksamkeitsschicht zunächst Query-, Key- und Value-Eingabe Q_in, K_in, V_in mit Hilfe von gewichteten Matrix W^Q, W^K und W^V in Query, Key und Value Q= Q_in W^Q, K = K_in W^K, V = V_in W^V umgewandelt werden. Diese gewichteten Matrizen W^Q, W^K, W^V sind die trainierbaren Gewichte einer Aufmerksamkeitsschicht. In ihnen wird das Ergebnis der Aufmerksamkeitsschicht festgelegt, da alle nachfolgenden Prozesse deterministisch sind.\n",
    "Betrachten wir aber, was passiert, wenn Query, Key und Value durch diese Matrizen gewichtet werden.\n",
    "\n",
    "#### Aufmerksamkeitsfunktion\n",
    "\n",
    "Die Aufmerksamkeitsfunktion hat die Eingaben Q, K, V und lautet:\n",
    "\n",
    "\tAttention(Q, K, V) = softmax(QK^T/sqr(d_k)) V\n",
    "\n",
    "Es wird also zuerst das Kreuzprodukt aus Q und K gebildet. Dieses Produkt wird skaliert, wie in Skalierung mit sqr(d_k) zu lesen ist, und dann die Softmax-Funktion\n",
    "\n",
    "\tsigma(x)_i = exp(x_i)/sum_j=1^n(exp(x_j)) für i=1,...,n\n",
    "positionsweise berechnet. Nennen wir \n",
    "\n",
    "\tsoftmax(QK^T/sqr(d_k)) = Score(Q,K),\n",
    "dann ist \n",
    "\n",
    "\tAttention(Q, K, V) = Score(Q,K)*V \n",
    "eine Funktion, die V mit einem Vektor multipliziert, wobei |Score(Q,K)| = 1.\n",
    "\n",
    "<span style=\"color:red\">Hier gilt vermutlich, das softmax komponentenweise angewandt wird, was aber überprüft werden sollte, bevor wir es so aufnehmen. </span>\n",
    "Score(Q,K) gibt also an, mit welchem Anteil jeder Eintrag von V in den Ausgabevektor Attention(Q,K,V) eingehen soll. Eine gute grafische Erklärung dieser Methode findet man in [https://jalammar.github.io/illustrated-transformer/].\n",
    "Da mit Score(Q,K) nun eine Gewichtung besteht, wie stark die Ausgabe Attention(Q,K,V)_i von V_j abhängt, kann man diese als Aufmerksamkeitsgewichtung in Matrixform sehr gut darstellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3afaf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149fbc2c327b441b9cdddc3914dbf36c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.', conti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df0de10512c4586b580d280500addc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Aufmerksamkeit berechnen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0c0d5e373e46b2bd7cad34e72ad2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_attn = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "attn_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_attn = widgets.Text(\n",
    "    value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_attn = widgets.Button(description='Aufmerksamkeit berechnen',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_attn = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_attn:\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 6\n",
    "        #output_widget_attn.clear_output()  # clear the previous output\n",
    "\n",
    "        # Convert input to tensor if it is not already\n",
    "        # Create a dynamic tensor to store output\n",
    "        # Make sure tensor_input is 2-D\n",
    "        tensor_input = tf.convert_to_tensor(input_widget_attn.value)\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        if len(tensor_input.shape) == 0:\n",
    "            tensor_input = tensor_input[tf.newaxis]\n",
    "        # tokenize and encode input\n",
    "        # Identify end token of the input\n",
    "        tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()\n",
    "        input_without_eos = tokenized_input[:, :-1]\n",
    "        context = transformer.encode(input_without_eos, None)\n",
    "\n",
    "        # Write the input tokens (excluding the last one) to the output array\n",
    "        for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "            output_array = output_array.write(i, value)\n",
    "\n",
    "        dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "        dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "        VisualWrapper.visualize_data(id='attention')\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 1\n",
    "            \n",
    "\n",
    "button_widget_attn.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_attn, button_widget_attn, output_widget_attn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "162b450c",
   "metadata": {},
   "source": [
    "\n",
    "#### Multi-headed Aufmerksamkeit\n",
    "\n",
    "In der Praxis hat sich bewährt parallel mehrere dieser Aufmerksamkeitsmechanismen durchzuführen. Dazu werden zu Beginn h gewichteten Matrizen W_i^Q, W_i^K, W_i^V i= 1,...,h eingeführt. Diese erzeugen also h verschiedene Matrixtripel Q_i, K_i, V_i und somit ergeben sich h verschiedene Ausgaben H_i = Attention(Q_i, K_i, V_i) i=1,...,h.\n",
    "Diese werden nun zu einer einzigen Ausgabe zusammengeführt, indem wir MultiHeadAttention(Q_in,K_in,V_in) = Concat(H_1,..., H_h)W^O berechnen. Dabei ist Concat(A_1,...,A_n) das hintereinanderschreiben mehrerer Matrizen und W^O eine weitere trainierbare Matrix, die die verschiedenen Ausgaben H_1,..., H,h gewichtet.\n",
    "Deshalb sehen wir in der obigen Ausgabe auch x verschiedene Matrizen, die Score(Q_i,K_i) darstellen.\n",
    "\n",
    "#### Verschiedene Aufmerksamkeitstypen\n",
    "\n",
    "In der Architektur werden verschiedene Aufmerksamkeitstypen unterschieden. Es gibt dabei zwei Variablen die beeinflussen, welche Art von Aufmerksamkeit wir verwenden. Die erste Variable ist woher die Eingaben Q_in, K_in und V_in kommen. Die zweite Variable ist die Maskierung, die wir auf Score(Q,K) anwenden.\n",
    "\n",
    "##### Selbst-Aufmerksamkeit\n",
    "\n",
    "Die Aufmerksamkeit nennen wir Selbst-Aufmerksamkeit, wenn gilt Q_in=K_in=V_in. Wenn sich Score(Q,K) also bildlich gesprochen daraus ergibt, welche Aufmerksamkeit jede Position der Eingabe auf eine andere Position derselben Eingabe legt und diese Aufmerksamkeit auf die Eingabe selbst angewandt wird.\n",
    "\n",
    "##### Kreuz-Aufmerksamkeit\n",
    "\n",
    "Wie nennen die Aufmerksamkeit Kreuz-Aufmerksamkeit, wenn gilt Q_in |= K_in = V_in. Wenn sich Score(Q,K) also daraus ergibt, welche Aufmerksamkeit jede Position einer Eingabe Q_in auf die Positionen einer zweiten Eingabe K_in gibt und dieser Aufmerksamkeitsscore auf die zweite Eingabe angewandt wird. Dies ist zum Beispiel in der Encoder-Decoder der Fall, wenn Q_in sich aus der Ausgabe des Encoder ergibt und K_in = V_in ein Zwischenergebnis des Decoders ist.\n",
    "\n",
    "##### Maskierte Aufmerksamkeit\n",
    "\n",
    "Ein Fall von maskierter Aufmerksamkeit liegt dann vor, wenn bestimmte Werte von Score(Q,K) maskiert werden. Das ist zum Beispiel beim Subsequent Masking der Fall, hier wird Score(Q,K)i,j = -inf gesetzt für alle Eintrage j>i. Dadurch wird verhindert, dass die Ausgabe Attention(Q,K,V)_i sich auf die Werte V_j, j>i stützt. Z.B. wird während des Trainings im Decoder dadurch verhindert, dass das Model lernt Informationen aus den zukünftigen Einträgen V_j, j>i zu benutzen, um V_i vorherzusagen. Man sieht gut in der Darstellung von Score(Q,K), dass die Werte für j>i dadurch meistens nahe bei 0 liegen.\n",
    "\n",
    "### Feed-Forward\n",
    "\n",
    "<span style=\"color:red\">Evtl. Zitat zu einer Quelle über Standard FFN einfügen.</span>\n",
    "Feed-Forward Netzwerken sind die Standard Implementations eines neronalen Netzes, in der die Neuronen einer Schicht mit jedem Neuron der nachfolgenden Schicht verbunden sind. In der hier realisierten Implementation bestehen sie aus einer Inputschicht und einer Outputschicht der Größe d_model sowie einer Hidden-Layer der Größe 2048. Dies ist die Standardimplementation von Transformermodellen.\n",
    "In [FNet] wird gezeigt, dass man die Aufmerksamkeitslayers vollständig durch die lineare Fast-Fourier Transformation ersetzen kann, was zeigt, dass die Feed-Forward Layers durchaus einen erheblichen Anteil an der Interpretation des Inputs eines Transformermodells haben.\n",
    "\n",
    "### Maskieren\n",
    "\n",
    "Das Maskieren des Inputs ist eine wichtige funktionale Komponente der Transformerarchitektur. Beim Maskieren handelt es sich in Wirklichkeit um zwei Mechanismen, die zwar dieselbe Funktionsweise besitzen, aber sehr unterschiedliche Aufgaben in der Architektur besitzen. Einerseits das Subsequent Masking und das Padding Masking. Das Padding Masking stellt jediglich sicher, dass nur Positionen mit Inhalt vom Transformer verarbeitet werden, während das Subsequent Masking dafür sorgt, dass der Decoder des Transformers autoregressiv [Zitat Autoregression] ist. Das bedeutet, bei einer Vorhersage für eine Ausgabe an der Position i, soll das Modell nur Informationen aus den Eingabepositionen 1, …,  i-1 nutzen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae0c164",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red\"> Interaktive Anwendung vgl. eines Tensors vor und nach dem Maskieren<span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "296d2bdb",
   "metadata": {},
   "source": [
    "\n",
    "#### Padding Masking\n",
    "\n",
    "Das Padding Masking ist notwendig, da Transformer sequentielle Daten so verarbeiten, als ob sie eine fixe Länge d_model hätten. Das geschieht deshalb, weil Transformer so lernen können jede Position der Ausgabelänge d_model parallel vorherzusagen. <span style=\"color:red\">Evtl. genauere Ausführung.</span>\n",
    "Um während des Trainings auch Daten mit einer Länge größer oder kleiner d_model zu nutzen, werden längere Sequenzen abgeschnitten und kürzere mit Nullen aufgefüllt. Diese Nullen müssen dann mit Hilfe von Padding Masking für das Training irrelevant gemacht werden. Das geschieht indem man alle Positionen die eine Null enthalten für das Modell auf -inf setzt, sodass sie beim Gradient Descent nicht berücksichtigt werden. [Zitat zu genaueren Erklärung] <span style=\"color:red\">Überprüfen ob das so stimmt!</span>\n",
    "\n",
    "#### Subsequent Masking\n",
    "\n",
    "Das Subsequent Masking benutzt die gleich Technik und setzt bestimmte Einträge innerhalb der Aufmerksamkeitslayers auf den Wert -inf. Subsequent Masking und Padding Masking werden dabei gleichzeitig angewandt.\n",
    "Einfügen einer mathematischen Beschreibung."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b154960",
   "metadata": {},
   "source": [
    "\n",
    "## Output\n",
    "\n",
    "### Log-Softmax\n",
    "\n",
    "<span style=\"color:red\">Hier fehlt noch der Inhalt</span>\n",
    "\n",
    "http://arxiv.org/abs/1608.05859\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d3cf257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63711eda10a44efbee72688c9116f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test sentence', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6111624e38cd444fbf4d9c96dd3691a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run interactive inference', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce43559e9c64ae08982cec0d9809ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_inf = widgets.Text(\n",
    "    value='Test sentence',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_inf = widgets.Button(description='Run interactive inference',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_inf = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_inf:\n",
    "        output_widget_inf.clear_output()  # clear the previous output\n",
    "        inference_model(input_widget_inf.value, interactive=True) # replace this with your function call\n",
    "        inference_model.print_results(visualisation=True)\n",
    "\n",
    "button_widget_inf.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_inf, button_widget_inf, output_widget_inf)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "tf_simu_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
