{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ab5bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T08:46:08.263618600Z",
     "start_time": "2023-11-13T08:46:03.260954700Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# logging and decorators\n",
    "import logging as log\n",
    "\n",
    "# tensorflow modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.python.keras import layers\n",
    "\n",
    "# necessary for visualization and user input\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact_manual, interactive, interact, VBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# modules from backend\n",
    "from interactive_inference_backend import ModelLoader, StoryTokenizer, WordComplete, VisualWrapper, positional_encoding\n",
    "from interactive_inference_backend import reserved_tokens, vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13564e-65e7-448f-8b34-e3155dc66a29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T08:46:08.301198100Z",
     "start_time": "2023-11-13T08:46:08.264621400Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cff4da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T08:46:11.493802600Z",
     "start_time": "2023-11-13T08:46:08.279112600Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ModelLoader(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                            d_model=512,\n",
    "                            n_stacks=2,\n",
    "                            h_att=4,\n",
    "                            load_model=True,\n",
    "                            model_load_path=\"model_N2_h4_d512_t20230831-134344\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589346f1",
   "metadata": {},
   "source": [
    "# Interaktive Erklärung der Transformerarchitektur\n",
    "## Einleitung\n",
    "### Kurzübersicht Transformer\n",
    "\n",
    "Transformer-Modelle sind eine von Vaswani et al. [1] vorgeschlagene Architektur für das Modellieren von sequentiellen Daten. Im Gegensatz zu den vorher genutzten Architekturen wie RNNs [2, 3] oder CNNs [4] ermöglichen Transformer allerdings das parallele Verarbeiten der sequentiellen Daten während des Trainings und ermöglichen dadurch mit erheblich reduzierter Trainingszeit und -rechenkapazität sequentielle Daten zu verarbeiten [Cite comparisson].\n",
    "\n",
    "Um diese parallele Verarbeitung zu erreichen nutzen Transformermodelle Aufmerksamkeitsblöcke. Aufmerksamkeitsblöcke sind trainierbare Matrizen, die von einem Eingabevektor auf einen Ausgabevektor projizieren. Also eine sequentielle Datenstruktur in Vektorform in eine ebenfalls sequentielle Datenstruktur in Vektorform verarbeiten, indem eine Matrixmultiplikation auf dein Eingabevektor angewandt wird. Wie das funktioniert wird in [einer eigenen Sektion behandelt](#aufmerksamkeit) gezeigt.\n",
    "Diese Matrizen projizieren also im Idealfall von jedem Eintrag im Eingabevektor genau diejenige Information auf einen Eintrag im Ausgabevektor, die an der jeweiligen Stelle die Ausgabe beeinflussen soll. Da hierbei die Ausgabe i nicht von der Ausgabe i-1 abhängt, können wir parallel die komplette Ausgabe zu jeder Eingabe erzeugen und mit einem Gradient-Descent [5] Verfahren unser Modell trainieren\n",
    "Da wir während der Inferenz (Erklärung was Inferenz ist) von sequentiellen Daten aber meist nur die vorhergehende Daten nutzen möchten benutzen Transformer Architekturen eine Maske, die nach der Anwendung der Matrixmultiplikation alle Informationen, die nachfolgende Daten liefern herausfiltern, sodass schon während dem Training nur die Verbindung zwischen der aktuellen Position und den vorhergehenden gelernt wird.\n",
    "Durch wiederholen von Aufmerksamkeitsblöcken und einem nachfolgenden Feed-Forward Netzwerk entsteht so eine Architektur zur Verarbeitung sequenzieller Daten, die verglichen mit vorherigen Architekturen weniger Rechenkapazität benötigt und trotzdem eine wesentlich reduzierte Trainigszeit aufweist. [1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0423185a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T08:46:11.528757500Z",
     "start_time": "2023-11-13T08:46:11.494802300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hier eine grafische Darstellung von Eingabe- zu Ausgabevektor darstellen. Wie werden sequentielle Daten verarbeitet und wie macht das ein Transformer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0012634",
   "metadata": {},
   "source": [
    "\n",
    "### Ziel dieses interaktiven Artikels\n",
    "\n",
    "Ziel dieses interaktiven Artikels soll es sein die Architektur die [1] beschreiben in ihren einzelnen Komponenten darzustellen. Der Fokus liegt hierbei darauf die Prozesse, die während der Verarbeitung sequentieller Daten stattfinden, grafisch durch interaktive Anwendungen darzustellen der Nutzer den Einfluss unterschiedlicher Architekturbausteine auf verschiedene Eingaben anschaulich klar wird. Ziel ist es, die verschiedenen Bausteine so zu erklären, dass einem möglichen Anwender die Implementation einer Tranformer-Architektur, durch ein Verständnis des Nutzens der einzelnen Architekturbausteine, erleichtert wird.\n",
    "\n",
    "Der Artikel soll dazu dienen eine Implementierung ohne einschlägiges Vorwissen, z.B. im Kontext von KMUs die sich bisher noch nicht mit der Modellierung sequentieller Daten beschäftigt haben, zu ermöglichen und Programmierer in der Designentscheidungen in der Transformerarchitektur zu unterstützen.\n",
    "[1] und viele der auf ihrer Architektur aufbauenden wissenschaftlichen Arbeiten [7, 8], Erklärartikel oder -videos [9, 10] beschränken sich, wenn überhaupt, darauf den Aufmerksamkeitsmechanismus ausführlich darzustellen. Dabei werden die Designentscheidungen für trainingsrelevante Elemente wie Dropout [11], Residuale Verbindungen [12] sowie die Beschreibung bereits etablierter Methoden wie Byte-Pair Encoding [13] als Einbettung oder Log-Softmax [5] vernachlässigt. Diese Elemente sollen hier aber ebenfalls dargestellt werden.\n",
    "\n",
    "### Architekturübersicht\n",
    "\n",
    "In <a href=\"#fig:input\">Abbildung 1</a> ist eine vollständige Darstellung aller Architekturelemente zu finden, die Teil der Transformerarchitektur sind. Dabei unterscheiden wir vier verschiedene Elemente. Prozesse und Methoden werden in Schwarz dargestellt. Die in diesen Prozessen generierte Daten werden in Blau dargestellt. Trainierbare Parameter werden in Gelb dargestellt. Hyperparameter, also festzulegende Eingabeparameter für das Modell werde in Grau dargestellt. Ebenfalls in Grau werden Eingabe- und Ausgabedaten dargestellt.\n",
    "Unsere Abbildung ist insofern komplett, als sie jeglichen Weg zeigen, den Daten durch das Modell nehmen können. Eine Darstellung, in der Form eines Encode-Decoder Netzwerks [14], wie sie [1] nutzen findet sich in Abbildung 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d914d5",
   "metadata": {},
   "source": [
    "<figure id=\"fig:input\" style=\"height: 700px;\">\n",
    "  <img src=\"./img/tf_arch_full.jpg\" style=\"height: 1400px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 1: Transformerarchitektur</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a6b568b",
   "metadata": {},
   "source": [
    "\n",
    "#### Encoder-Decoder\n",
    "\n",
    "Eine Encoder-Decoder Architektur besteht aus zwei voneinander getrennten neuronalen Netzen dem Encoder und dem Decoder, die aber zusammen trainiert werden. Diese Architektur wurde von [14] vorgeschlagen und dann vor allem im Kontext von Neural Machine Translation (NMT) verwendet. Der Vorteil von Encoder-Decoder liegt darin, dass man sowohl den Encoder als auch den Decoder ersetzen kann. Im Idealfall könnte man im Bereich von NMT pro Sprache einen Encoder und einen Decoder trainieren, um dann beliebige Übersetzungen zwischen allen Sprachen zu generieren. Da auch [15], die zuerst Aufmerksamkeitsmechanismen für Rekurrente Neuronale Netze (RNN) vorgeschlagen haben, eine Encoder-Decoder Architektur verwenden, verwundert es nicht, dass auch [1] eine solche Architektur wählten. Insbesondere da [1] Transformer ebenfalls für NMT benutzen.\n",
    "Transformer zur Textvervollständigung können auch ausschließlich auf Decodern beruhen [16]. In unserem Beispiel wird der Encoder zum enkodieren der Eingabe verwendet, um dann im Decoders die Eingabe als Anfang der Ausgabe zu verwenden. In Abbildung 2 sieht man wie Encoder und Decoder zusammenarbeiten. Der Decoder benutzt Selbstaufmerksamkeit, um seinen Input zu verarbeiten, nutzt dann Kreuz-Aufmerksamkeit, für den nächsten Schritt, um final erneut Selbstaufmerksamkeit zu benutzen, wohingegen der Encoder ausschließlich über Selbstaufmerksamkeit funktioniert. Näheres siehe im Kapitel [Aufmerksamkeit](#aufmerksamkeit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4798807b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T08:46:11.586778500Z",
     "start_time": "2023-11-13T08:46:11.513134200Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9be5a39d0fe4a4a8a044d58d862037e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Encoder-Decoder Test', continuous_update=False, description='Ihre Eingabe:', layout…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "transformer = model.model\n",
    "\n",
    "input_widget_enc_dec = widgets.Text(\n",
    "    value='Encoder-Decoder Test',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_enc = widgets.Button(description='Enkodiere die Eingabe',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "button_widget_dec = widgets.Button(description='Dekodiere die Eingabe',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_enc = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "output_widget_dec = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def encode():\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    if len(tensor_input.shape) == 0:                                           # Überprüft, ob der Eingabetensor im korrekten Format ist\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # Falls nicht, wird eine Dimension hinzufügt \n",
    "    \n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')\n",
    "    \n",
    "    context = transformer.encode(input_without_eos, None)                      # Kodierung des Inputsatzes von (Transformer-Modell)\n",
    "    return context, string_value, lookup\n",
    "\n",
    "\n",
    "def decode(): \n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)   # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "    if len(tensor_input.shape) == 0:                                           # wie bei der Encodierung\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # wie bei der Encodierung\n",
    "\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # wie bei der Encodierung\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    \n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')                      \n",
    "    context = transformer.encode(input_without_eos, None)                     \n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "                              \n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):                        # Schleife durch jedes Token des Satzes\n",
    "      output_array = output_array.write(i, value)                              # Speichern des Tokens im Output array\n",
    "\n",
    "    dec_input = output_array.concat()[tf.newaxis]                              # Output Array wird zu einem einzigen Tensor konkateniert \n",
    "                                                                               # und anschließend um eine zusätzliche Dimension erweitert\n",
    "\n",
    "    dec_out = transformer.decode(context, None, dec_input, None)               # Decoder des Transformer-Modells wird verwendet, um den dec_input-Tensor \n",
    "                                                                               # unter Verwendung des zuvor berechneten Kontexts zu decodieren.\n",
    "\n",
    "    return dec_out, string_value, lookup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def on_button_click_enc(b):\n",
    "  with output_widget_enc:\n",
    "    output_widget_enc.clear_output()  # clear the previous output\n",
    "    context, tokens, lookup = encode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(context) \n",
    "\n",
    "\n",
    "def on_button_click_dec(b):\n",
    "  with output_widget_dec:\n",
    "    output_widget_dec.clear_output()\n",
    "    dec_out, tokens, lookup = decode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(dec_out)\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "button_widget_enc.on_click(on_button_click_enc)\n",
    "button_widget_dec.on_click(on_button_click_dec)\n",
    "\n",
    "widgets.VBox([input_widget_enc_dec, widgets.HBox([widgets.VBox([button_widget_enc, output_widget_enc]), widgets.VBox([button_widget_dec, output_widget_dec])])])\n",
    "#display(input_widget_enc_dec, button_widget_enc, output_widget_enc, button_widget_dec, output_widget_dec)\n",
    "\n",
    "\n",
    "#print('tok_out', tokenized_input)\n",
    "#print('enc_out', context)\n",
    "#print(\"dec_out\", dec_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa505ea8-2a43-43f6-936e-0d45b4c96f75",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "Über der Grafik sind die verarbeiteten Tokens zu sehen. In der Grafik wird die Position der Tokens wird durch die y-Achse angezeigt und entspricht der Reihenfolge der Tokens im Beispielsatz.  Dabei werden Leerzeichen weggelassen. Das Model erkennt unterschiedliche Wörter mit Hilfe der \"##\" Zeichen. Durch diese Zeichen sieht das Modell, dass ein Token mit \"##\" Zeichenkette zu dem vorherigen Token gehört wie im Beispiel bei dem Wort \"Endcoder\", welches aus den Tokens 'e', '##n', '##co', '##der' besteht. Weitere Informationen in Abschnitt [Byte-Pair Encoding](#byte-pair-encoding).\r\n",
    "\r\n",
    "Schaut man sich die Wörter vor der Tokenisierung an, sieht man das alle Wörter nun kleingeschrieben werden. Somit werden groß- und kleingeschriebene Formen des Wortes zusammengefasst. Dadurch wird die Anzahl des Vokabulars reduziert. Linguistische und grammatische Informationen, die aufgrund der Zusammenführung verloren zu gehen scheinen, werden durch die Position und den Kontext des Wortes im Satz erhalten (z.B. Substantivierungen). Ebenso wird das Modell dazu verleitet, mehr Acht auf die Position des Wortes im Satz zu geben.  Der Tokenizer fügt außerdem ein Start- und Endtoken hinzu. Das Endtoken wird im Modell nicht weiterverwendet, während das Starttoken die Position 0 in der weiteren Verarbeitung und Visualisierung einnimmt.\r\n",
    "\r\n",
    "Die x-Achse repräsentiert die Tiefe der Token bzw. die Position der einzelnen Werte des Vektors, der die Tokens darstellt. In diesem Fall beträgt die Tiefe 512, was bedeutet, dass die Wörter durch 512 verschiedene Werte dargestellt werden. Diese Vektoren sind jedoch nicht zufällig, sondern weisen untereinander Abhängigkeiten auf. Das heißt, ähnliche Wörter haben z.B. ähnliche Vektoren. Darüber hinaus sind in diesen Vektoren auch die Abhängigkeiten der Wörter im Satz enthalten, was als sogenannt [Positional Encoding](#positional-encoding) bezeichnet wird.  \r\n",
    ".  \r\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028a3317",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Architekturblöcke\n",
    "\n",
    "Prinzipiell lassen sich Transformer in mehrere Blöcke einteilen.\n",
    "\n",
    "1. Eingabeblock\n",
    "\n",
    "Die Eingabepipeline verarbeitet die Eingabedaten in eine Form die für die Matrixtransformation im Aufmerksamkeitsblock genutzt werden kann. Diese unterscheidet sich für verschiedene Datentypen. In unserem Beispiel nutzen wir Textdaten, die wir durch Tokenisierung [17] und ein Embedding [18] in Tensoren verwandeln.\n",
    "\n",
    "2. Aufmerksamkeitsblock\n",
    "\n",
    "Der Aufmerksamkeitsblock verarbeitet Daten in Tensorform und liefert somit eine Abbildung von den Eingabetensoren auf die Ausgabetensoren, die jeweils von den Eingabe- und Ausgabeblöcken interpretiert wird.\n",
    "Man kann verschiedene Modulvarianten innerhalb eines Aufmerksamkeitsblocks unterscheiden. Aufmerksamkeitsmodule erhalten als Eingabedaten immer drei Tensoren. Diese werden Query, Key und Value genannt. Aus diesen berechnet ein Aufmerksamkeitsmodul eine Ausgabe.\n",
    "Typischerweise werden Aufmerksamkeitsmodule dabei dadurch unterschieden aus welcher Quelle Query, Key und Value stammen.\n",
    "Es gibt \n",
    "\n",
    "- Self-Attention bei der Query, Key und Target alle aus einer Quelle stammen und\n",
    "- Source-Attention (Hier ist der Query aus einer anderen Quelle als Key und Value, z.B. wenn der Query die Ausgabe eines Encoders ist, während Target die Ausgabe eines Decoderblocks ist).\n",
    "\n",
    "Desweiteren kann man nach Art des Maskings unterscheiden. Masking wird verwendet, um das Aufmerksamkeitsmodul daran zu hindern aus einem Teil der Ausgabedaten zu lernen. In Transformern gibt es \n",
    "\n",
    "- Subsequent Masking, das ein Decoderaufmerksamkeitsblock daran hindert für die Vorhersage der Position i Daten aus den Positionen i+j nutzt, und\n",
    "- Padding Masking, das verhindert, dass Fehler verhindert, die aufgrund dessen entstehen, dass die Eingabe eines Transformers, unabhängig vom Inhalt, immer die selbe Anzahl an Tokens besitzen muss.\n",
    "\n",
    "3. Ausgabeblock\n",
    "\n",
    "Die Ausgabepipeline interpretiert die Ausgaben des Aufmerksamkeitsmoduls, sodass sie in eine für menschlichen Gebrauch nützlichen Form vorliegen (typischerweise z.B. Textdaten, Bilddaten, etc.). Dafür wird in Transfomern eine Log-Softmax Funktion genutzt, die auf die Tensorausgabe des letzten Aufmerksamkeitsblockes angewandt wird.\n",
    "\n",
    "In <a href=\"#fig:input\">Abbildung 1</a> entspricht das Aufmerksamkeitsmodul den Prozessen innerhalb der grauen Umrandung, während die Eingabe- und Ausgabepipeline darunter bzw. darüber zu finden sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f86373b1",
   "metadata": {},
   "source": [
    "## Input\n",
    "\n",
    "Der erste Teil eines Transformermodells besteht aus der Eingabepipeline. Diese verarbeitet die Eingabe in das Modells, z.B. die Texteingabe eines Nutzers und bereitet sie darauf vor durch wiederholte Anwendung der Aufmerksamkeitsmodule in einem Transformermodell verarbeitet zu werden. Die Aufmerksamkeitsmodule arbeiten über eine Aufmerksamkeitsmatrix, die aus der jeweiligen Eingabe eine Ausgabe berechnet. Wir müssen also aus einer Eingabe in Textform eine Vektorrepräsentation erzeugen, die alle notwendigen Informationen für ein Modell enthalten, um nützliche Vorhersagen über eine Übersetzung oder eine Textvervollständigung machen zu können.\n",
    "\n",
    "<a href=\"#fig:input\">Hier</a> sehen sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt. \n",
    "Wie zu erkennen ist, wird in der Eingabpipeline während des Training eines Transformermodells ausschließlich die Gewichte für die Einbettung trainiert, alles andere funktioniert deterministisch und ist damit unbeeinflusst vom Trainningsprozess.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4caafaa5",
   "metadata": {},
   "source": [
    "\n",
    "<figure id=\"fig:input\" style=\"height: 700px;\">\n",
    "  <img src=\"./img/tf_input_pipeline.jpg\" style=\"height: 700px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4578ca",
   "metadata": {},
   "source": [
    "Prinzipiell besteht die Eingabepipeline aus drei Modulen:\n",
    "\n",
    "1. [Tokenisierung](#tokenisierung),\n",
    "2. [Einbettung](#einbettung),\n",
    "3. [Positionale Kodierung](#positional-encoding).\n",
    "\n",
    "Dies entspricht drei Verarbeitungsschritten:\n",
    "\n",
    "1. Bei der Tokenisierung, wird der Text mithilfe eines Zeichenalphabets in eine Zahlenkodierung umgewandelt.\n",
    "2. Bei der Einettung wird diese mithilfe eines trainierbaren Algorithmus in eine Vektordarstellung mit sehr viel mehr Parametern umgewandelt. Diese Einbettung lernt die komplexe Struktur eines Textes so darzustellen, dass sie informativ für die nachfolgenden Module ist. Wie genau das passiert ist dabei aufgrund der stochastischen Natur von Deep Learning Verfahren nicht offen einsehbar.\n",
    "3. Die Positionelle Kodierung ist einzigartig für Transformermodelle. Im Gegensatz zu RNN, die die Eingabedaten sequentiell präsentiert bekommen [19], bekommen Transformer die Eingabe ohne Informationen zur relativen Position der Tokens. Diese fehlenden Informationen werden in diesem Schritt manuell hinzugefügt.\n",
    "\n",
    "### Tokenisierung\n",
    "\n",
    "Bei der Tokenisierung wird der Satz in Textform z.B. \"Das ist ein Testsatz.\" in einen Zahlencode verwandelt. Hierfür kommen verschieden Methoden in Frage. Einer der simpelsten Methoden ist es z.B. jedem Buchstaben eine Zahl zuzuordnen. Das führt allerdings zu einer sehr langen Kodierung.\n",
    "\n",
    "Die entscheidenden Faktoren für eine gute Kodierung sind\n",
    "- Vollständigkeit der Kodierung, \n",
    "- Länge des kodierten Vektors und \n",
    "- Größe des dafür nötigen Vokabulars.\n",
    "\n",
    "Die Kodierung mit einzelnen Buchstaben ist vollständig (man kann beliebige Zeichenkombinationen kodieren) und besitzt ein kurzes Vokabular (26 für alle Buchstaben plus alle Punktierungs und Sonderzeichen, die im Text vorkommen), allerdings ist die Länge der kodierten Vektoren groß.\n",
    "\n",
    "Benutzt man ein Vokabular an festgeleten Wörtern, wird die Länge der Kodierung verkürzt, allerdings besteht die Gefahr der Unvollständigkeit. Um das nach Möglichkeit zu verhindern, benötigt man ein Vokabular, dass den Raum der möglichen Wörter vollständig abdeckt. Diese Vokabular müsste daher sehr umfangreich sein.\n",
    "\n",
    "Die Probleme der obigen Methoden hat dazu geführt, dass sich gemischte Verfahren ergeben haben. Die vorher an einem möglichst großem Korpus trainiert werden. Zum einen Top-Down Verfahren, die von einem aus dem Korpus extrahierten Vokabular ausgehen und lernen unbekannte Worte in Teilworte zu zerlegen, die dann in das Vokabular aufgenommen werden. Zum anderen Bottom-Up Verfahren, die von einem Buchstabenvokabular ausgehen und häufig wiederkehrende Kombinationen in dieses explizit aufnehmen.\n",
    "Die Transformerarchitektur nach [1] verwendet ein solches Bottom-Up Verfahren namens Byte-Pair Encoding [13], dass sich als effizient erwiesen hat und ein relativ kompaktes Vokabular ermöglicht dabei aber die Länge der Tokenisierung klein hält. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b56024be",
   "metadata": {},
   "source": [
    "Den Tokenizer findet man in unserer <a href=\"#fig:transformer\">Abbildung 1</a> ganz unten und ist der erste Schritt, um eine Eingabe zu verarbeiten\n",
    "<figure id=\"fig:tokenizer\" style=\"height: 300px;\">\n",
    "  <img src=\"./img/tf_tokenizer.jpg\" style=\"height: 300px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a5e1d1",
   "metadata": {},
   "source": [
    "\n",
    "### Byte-Pair Encoding\n",
    "\n",
    "Byte-Pair Encoding nutzt ein Vokabular mit einer festgelegten Länge. In unserer Implementation des Tokenizer nutzt er ein Vokabular von 8000 Einheiten. Das Vokabular wird folgendermaßen erstellt:\n",
    "\n",
    "  1. Ein Text, der für die Erstellung des Vokabulars verwendet wird, wird in eine Sequenz von Buchstaben und Symbole zerlegt. Wortenden werden mit einem zusätzlichen Symbol kodiert.\n",
    "  2. Alle Buchstaben und Symbole werden in das Vokabular aufgenommen.\n",
    "  3. Nun wird das häufigste 2-Gramm, also zwei aufeinander folgende Symbole, gesucht, das im Text zu finden ist.\n",
    "  4. Dieses wird ins Vokabular aufgenommen und im Text durch ein einzelnes Symbol ersetzt.\n",
    "  5. Dieser Prozess wird nun wiederholt, bis die vorgegebene Länge des Vokabulars erreicht ist.\n",
    "\n",
    "Dabei werden sowohl ganze Wörter ins Vokabular aufgenommen, wenn sie oft genug auftauchen (bespielweise werden die Worte \"a\", \"the\", \"and\" bei englischen Texte sicher aufgenommen), aber auch einzelne Wortteile wie z.B. \"en##\", \"##ment\" oder \"##ed\" werden in diesem Vokabular sicher vorkommen, um seltene Kombinationen wie \"enablement\" in die Wortteile \"en##\", \"able\" und \"##ment\" zerlgen zu können oder grammatikalische Formen wie \"wanted\" zu bilden.\n",
    "\n",
    "In unserem Testbeispiel können Sie testen, wie Ihre Eingabe in Tokens getrennt und dann in numerische Werte umgewandelt wird, je nachdem, welche Position das Token in unserem Vokabular hat.\n",
    "Wie Sie sehen, wird unser Bytepaar-Kodierungsvokabular durch ein [START]- und [END]-Token erweitert und enthält auch Elemente vom Typ 'abc##' oder '##abc'. Diese stellen eine Sequenz am Anfang oder Ende eines Wortes dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60193c08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T08:46:11.655652Z",
     "start_time": "2023-11-13T08:46:11.546776700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682faa4c09754eeea2d54567304c29c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Tokenizer test', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bb5493c53d4dd9ba2da77ea0d9de87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run tokenizer on input', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72df2a5119844249a32d228c1256b826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_tok = widgets.Text(\n",
    "    value='Tokenizer test',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  \n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_tok = widgets.Button(description='Run tokenizer on input',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_tok = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def tokenize(input_widget_tok):\n",
    "    tokens = tokenizer.tokenize(input_widget_tok.value)                    # Erstellung der Tokens als Index für Vokabular\n",
    "    lookup = tokenizer.lookup(tokens)                                      # Abrufen der Zeichenkette des Index im Vokabular                    \n",
    "    \n",
    "    return tokens, lookup\n",
    "    \n",
    "def on_button_click(b):\n",
    "    with output_widget_tok:\n",
    "        output_widget_tok.clear_output()                                                        \n",
    "        tokens, lookup = tokenize(input_widget_tok)\n",
    "\n",
    "        VisualWrapper.display_text('Tokens die aus der Eingabe mit Byte-Pair Encoding extrahiert werden:'.rjust(100) + ', '.join([token.decode('utf-8').rjust(10) for token in lookup.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "        VisualWrapper.display_text('Ihre Positionsnummer im Alphabet des Byte-Pair Encoding Algorithmus:'.rjust(100) + ', '.join([str(token).rjust(10) for token in tokens.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "\n",
    "button_widget_tok.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_tok, button_widget_tok, output_widget_tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e5b84a",
   "metadata": {},
   "source": [
    "### Einbettung\n",
    "\n",
    "Wo die Sequenzen die durch den Tokenizer entsteht in ihrer Länge, je nach Länge der Eingabe, aber auch durch die Eigenschaften des Byte-Pair-Encoding, variiert, ist es für Transformer notwendig eine gleichbleibendes Eingabeformat zu erhalten.\n",
    "\n",
    "Dies wird dadurch gelöst, dass Padding Tokens zum Einsatz kommen, also Tokens, die keine Information codieren, sondern der Eingabe beigefügt werden, um die erforderliche Länge zu erreichen.\n",
    "\n",
    "Außerdem wird die Ausgabe des Tokenizers mithilfe einer trainierbaren Einbettungsmatrix in das notwendige Format gebracht\n",
    "\n",
    "Die Einbettung sorgt dafür, dass die Ausgabe des Tokenizer in einen Vektor der Modellgröße $d_{model}$ kodiert wird. Das heißt jedes Token, das zuvor durch eine Zahl kodiert wurde wird nun durch einen Vektor der Länge $d_{model}$ kodiert. \n",
    "Die Einbettung ist Teil der vom Modell gelernten Parameter (siehe <a href=\"#fig:embedding\">Abbildung 5</a>) und somit nicht deterministisch gegeben. Welche Informationen aus der Ausgabe des Tokenizers wo gespeichert wird ist also nicht nachvollziehbar.\n",
    "Vorstellen kann man sich aber, dass in jedem der $d_{model}$ Parametern einige der Informationen gespeichert werden, die für das jeweilige Token wichtig sind. Beispielsweise die Bedeutung des Tokens, seine grammatikalische Form, in welchem Kontext es benutzt wurde, steht es am Anfang eines Satzes oder am Ende, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab2ec3f9",
   "metadata": {},
   "source": [
    "\n",
    "<figure id=\"fig:embedding\" style=\"height: 300px;\">\n",
    "  <img src=\"./img/tf_embedding.jpg\" style=\"height: 300px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 4: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b34b4d",
   "metadata": {},
   "source": [
    "Wie eine solche Einbettung aussieht und wie sie sich verändert, wenn man beispielsweise neue Teile an den Satz anfügt können Sie in der nachfolgenden Simulation ausprobieren. Der Eingabetext wird erst vom Tokeniser in Tokens umgewandelt und dann durch die Einbettung in einen Tensor.\n",
    "\n",
    "An jeder Position (vertikal dargestellt) ist dann die Einbettung des Tokens an dieser Stelle zu sehen (horizontal dargestellt). Die farbliche Kodierung stellt dabei das Zahlenspektrum dar, indem sich die Vektoreinträge bewegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "758a1d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T08:46:11.722858900Z",
     "start_time": "2023-11-13T08:46:11.579776400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1775dc66abe44c1bb1746d816ef41b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p style=\"font-size:18px; color:blue;\">Hier kannst du einen Text einbetten lassen. Wenn du die Ein…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7125a8bc988d43ca826f2d89d8cf438d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Einbettung Test', continuous_update=False, description='Ihre Eingabe:', layout=Layout(margin='0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2082ff593e450ba7c0c08e5436fca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Einbettung erstellen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66753921a81e4f96888847d85b18bb93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EmbeddingExample():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "        self.input_widget = widgets.Text(\n",
    "            value = 'Einbettung Test',\n",
    "            description = 'Ihre Eingabe:',\n",
    "            continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "            layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "        )\n",
    "\n",
    "        self.button_widget = widgets.Button(description='Einbettung erstellen',\n",
    "                                    layout = widgets.Layout(width='auto'))\n",
    "\n",
    "        self.output_widget = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "        self.old_context = None\n",
    "\n",
    "    def create_tokenized_embeddings(self):\n",
    "        tokens = self.tokenizer.tokenize(self.input_widget.value)                                 # Tokenisierung der Eingabe\n",
    "        tokens_all = tokens[tf.newaxis, :, :]                                                     # Hinzufügen einer weiteren Dimension\n",
    "        input_without_eos = tokens[tf.newaxis, :, :-1]                                            # Auswahl der Tokens bis zum [END] Token\n",
    "        token_input = self.tokenizer.detokenize(tokens_all)                                       # Nur zur Ausgabe Zwecken\n",
    "        string_value = token_input.numpy()[0][0].decode('utf-8')                                  # Nur zur Ausgabe Zwecken\n",
    "        lookup = tokenizer.lookup(input_without_eos)                                              # Nur zur Ausgabe Zwecken\n",
    "        lookup = [item.decode('utf-8') for sublist in lookup.numpy()[0] for item in sublist]      # Nur zur Ausgabe Zwecken\n",
    "        print(\"Wörter: \", string_value)\n",
    "        print(\"Tokens: \", lookup)\n",
    "        context = model.model.enc_embed(input_without_eos)                                        # Erstellung des Kontext Embedding \n",
    "        VisualWrapper.display_text('So sieht die Einbettung der Eingabe aus.')\n",
    "        VisualWrapper.color_bar(context.to_tensor())\n",
    "        if self.old_context is not None:\n",
    "             padded_context, padded_old_context = self.pad_tensors(context, self.old_context)     # Erstellung des Padding Vektors der Eingaben\n",
    "             VisualWrapper.display_text('So unterscheiden sich die alte und die neue Einbettung voneinander.')\n",
    "             context_diff = padded_context - padded_old_context                                   # Berechnung der Unterschiede beider Vektoren\n",
    "             VisualWrapper.color_bar(context_diff)\n",
    "\n",
    "        self.old_context = context\n",
    "\n",
    "    \n",
    "    def on_button_click(self, b):\n",
    "        with self.output_widget:\n",
    "            self.output_widget.clear_output()  # clear the previous output\n",
    "            VisualWrapper.reset_visualiser()\n",
    "            self.create_tokenized_embeddings()\n",
    "    \n",
    "    def pad_tensors(self, ragged_tensor1, ragged_tensor2):\n",
    "        \"\"\"Funktion um die Tensoren der Eingabe auf die gleiche Länge zu transformieren\"\"\"\n",
    "        tensor1 = ragged_tensor1.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "        tensor2 = ragged_tensor2.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "\n",
    "        shape1 = tf.shape(tensor1)\n",
    "        shape2 = tf.shape(tensor2)\n",
    "\n",
    "        target_shape = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            target_shape.append(tf.maximum(shape1[i], shape2[i]))                                # Die maximale Größe der Dimension wird an die Zielform angehängt.\n",
    "\n",
    "        target_shape = tf.stack(target_shape)                                                    # Umwandlung der Zielform in einen Tensor\n",
    "\n",
    "\n",
    "        paddings1 = []\n",
    "        paddings2 = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            paddings1.append([0, target_shape[i] - shape1[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "            paddings2.append([0, target_shape[i] - shape2[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "\n",
    "        paddings1 = tf.stack(paddings1)                                                          # Konvertieren der Paddings in Tensoren\n",
    "        paddings2 = tf.stack(paddings2)                                                          # Konvertieren der Paddings in Tensoren\n",
    "\n",
    "        tensor1_padded = tf.pad(tensor1, paddings1)                                              # Tensoren an die Zielform anpassen\n",
    "        tensor2_padded = tf.pad(tensor2, paddings2)                                              # Tensoren an die Zielform anpassen\n",
    "\n",
    "        return tensor1_padded, tensor2_padded\n",
    "\n",
    "emb_ex = EmbeddingExample()\n",
    "\n",
    "VisualWrapper.display_text('Hier kannst du einen Text einbetten lassen. Wenn du die Eingabe veränderst wird außerdem gezeigt, wie sich die Einbettung geändert hat.')\n",
    "\n",
    "emb_ex.button_widget.on_click(emb_ex.on_button_click)\n",
    "display(emb_ex.input_widget, emb_ex.button_widget, emb_ex.output_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef7ac5-da74-40c0-be78-f0f866ca9fd9",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesen Beispiel wie ein Embedding grafisch aussieht. Das Embedding wird in der Funktion *create_tokenized_embeddings()* erstellt. Dazu wird zu erst der Eingabetext vom Tokenizer in Tokens unterteilt (Zeile 20). Die Tokens können Sie über der Grafik sehen. In Zeile 29 werden diese dann vom Transformer Modell in die Embeddings umgewandelt. \n",
    "\n",
    "Verwendet man zum Beispiel das Eingabebeispiel \"Einbettung Test\" und als zweites \"Einbettung Test neu\", sieht man im zweiten Bild wie die zusätzlichen Positionen dazu kommen. Die Visualisierung von \"Einbettung Test\" zeigt auf der y-Achse sieben Tokens. Bei dem Satz \"Einbettung Test neu\" kommen dann 3 Tokens dazu, diese sind \"#n\", \"##e\", \"##u \". Ebenso kann man ein Muster bei dem Wertebereich erkennen. Der Bereich der Tiefe zwischen 0bis 256 bewegt sich hauptsächlich im Wertebereich -1 bis 2 und der Bereich von 257 bis 512 im Wertebereich 0 bis -3. Dies geht auf die Sinus-Funktion des Positional Encodings zurück, welche auf das Embedding angewendet wird. Diese hat für diese Bereiche stark unterschiedliche Werte. \n",
    "\n",
    "Die Werte in den ersten 256 Positionen können bestimmte Merkmale oder Eigenschaften der Wörter repräsentieren, während der Bereich der Positionen von 266 bis 512 andere Merkmale oder Eigenschaften widerspiegelt. Diese getrennte Darstellung ermöglicht dem Modell, komplexe Beziehungen und Muster in den Daten zu erfassen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e70583",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Positionale Kodierung\n",
    "\n",
    "Da in der Einbettung keine Informationen über die Position der verschiedenen Worte kodiert wird, muss diese manuell hinzugefügt werden. Hierfür benutzt die Transformerarchitektur für jede Position der Einbettung (also jedes enkodierte Wort) eine Sinuskurve mit anderer Frequenz und Phase. [1] Diese wird der Einbettung an der jeweiligen Stelle hinzugefügt.\n",
    "\n",
    "Dadurch lassen sich die verschiedenen Worte sehr gut voneinander trennen. Die Idee dahinter ist, dass sich:\n",
    "1. die grobe Position der Worte durch die langfrquenten Sinuskurven bestimmen lässt, da sie sich über die gesamte Länge der Eingabe nur allmählich verändert und die Werte der Einbettung durch diese insgesamt in eine bestimmte Richtung verschoben werden, z.B. die Worte im hinteren Teil der Eingabe größere Werte besitzen als im vorderen Teil der Eingabe,\n",
    "2. die genaue Position durch die hochfrequentere Sinuskurven bestimmen lässt, da diese sich schon für benachbarte Vektoren unterscheidet und somit klar wird, welches Wort an welcher Stelle in der Einbettung kodiert wurde.\n",
    "\n",
    "In der unten stehenden Simulation ist zu sehen, wie die positionale Kodierung beipielhaft für eine 2048 x 512 lange und tiefe Einbettung aussieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d875186",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-13T08:46:11.610658900Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd289a35d5f5422990fdd9dc6400cf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1025, description='length', max=2048, min=2), IntSlider(value=257, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(length=(2,2048,1), depth=(2,512,1))\n",
    "def print_pos_enc(length, depth):\n",
    "    VisualWrapper.color_bar(positional_encoding(length, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb626ffc-d9b6-4181-92d3-1bf3618660b4",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "An diesem Beispiel sieht man den Effekt des Positional Encodings auf verschiedene lange bzw. tiefe Einbettungen. Wenn man mit den zwei Reglern spielt, sieht man, verschieden große Zooms auf den Effekt. Erhöht man den \"length\"-Regler sieht man die Auswirkungen, welche die Länge des Satzes betreffen. Die Werte die durch das Positional Encoding zu dem Vektor hinzugefügt werden sind dabei jedoch immer dieselben. Zum Bespiel wird an Stelle 100 immer derselbe Wert hinzugefügt, egal ob die maximale Länge des Satzes (\"length\") sich verändert. Dasselbe gilt auch für die Tiefe jedes einzelnen Vektors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389fa0d1",
   "metadata": {},
   "source": [
    "\n",
    "## Trainingsmethoden\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout [11] ist eine Methode, die während des Trainingsprozesses eines neuronalen Netzes genutzt wird, um zu verhindern, dass die gelernte Ausgabe eines Modells sich zu sehr auf einen einzelnen Prädikator stützt.\n",
    "Dafür wird zwischen zwei Schritten desselben Modells, eine Dropout-Layer eingefügt. Diese setzt zufällig einige der vom ersten Modellteil generierten Ausgabe auf einen vordefinierten Wert (meistens -inf), um den nachfolgenden Schichten diese Information vorzuenthalten. Da diese Operation zufällig erfolgt, müssen die nachfolgenden Teile des Modells lernen ihre Ausgabe auch ohne diese Information zu erstellen. Somit lernt das Model seine Vorhersage auf eine möglichst breite Kombination an Merkmalen aufzubauen und man verhindert, dass Vorhersagen nur aufgrund eines einzigen Merkmals der vorherigen Ausgabe gemacht werden.\n",
    "\n",
    "Ein gutes Beispiel ist das Ende eines Satzes vorherzusagen. In europäischen Sprachen wird ein Satz fast immer mit einem Punkt beendet, also ist es eine gute Strategie zu lernen, dass ein Satz druch einen Punkt beendet wird. Doch ein Modell, dass einen Punkt als einziges Merkmal eines Satzendes nutzt ist wenig robust, wenn man an falscher Stelle einen Punkt setzt oder ihn an einem Satzende druch ein anderes Zeichen ersetzt. Dabei gibt es auch andere Hinweise auf ein Satzende, wie z.B. das vorkommen eines Verbs in der deutschen Sprache oder von Ort und Zeitangaben im Englischen.\n",
    "\n",
    "Um dem Modell keine Informationen vorzuenthalten, wenn es tatsächlich eingesetzt wird, ist Dropout immer so konzipiert, dass es zwar während des Trainings aktiv ist, aber danach abgeschalten wird, sodass während der Inferenzphase keine Informationen gelöscht werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b5c7833",
   "metadata": {},
   "source": [
    "Wie sich eine Ausgabe eines Teils eines Transformermodells unterscheiden kann, wenn man das Dropout anwendet können Sie in der nachfolgenden Simulation testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b358029",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d245081713404c99a66e32c9f837001d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=30, continuous_update=False, description='Länge des Tensors:', max=2048, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5651f9d468f647edba914357f9f5e53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=512, continuous_update=False, description='Tiefe des Tensors:', max=512, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3c072d680c49d4a507b2d61e809931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.1, continuous_update=False, description='Dropoutrate:', max=0.9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7af2b308e240a4b40fbf38eec067d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Tes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b93abbcad441629c5942be48866765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_drop = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_drop = widgets.Text(value = 'Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test',\n",
    "                                 description = 'Ihre Eingabe:',\n",
    "                                 continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                                 layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "                                 )\n",
    "\n",
    "length_widget_drop = widgets.IntSlider(value=30,\n",
    "                           min=2,\n",
    "                           max=2048,\n",
    "                           description='Länge des Tensors:',\n",
    "                           continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                           )\n",
    "depth_widget_drop = widgets.IntSlider(value=512,\n",
    "                          min=2,\n",
    "                          max=512,\n",
    "                          description='Tiefe des Tensors:',\n",
    "                          continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                          )\n",
    "dropout_widget = widgets.FloatSlider(value=0.1,\n",
    "                              min=0,\n",
    "                              max=0.9,\n",
    "                              step=0.1,\n",
    "                              description='Dropoutrate:',\n",
    "                              continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "def dropout_function(length, depth, dropout, input):\n",
    "\n",
    "    # Erstellung der Dropout Layer\n",
    "    dropout_layer = layers.Dropout(dropout)                                                         # Anwendung des Dropout auf die Layer\n",
    "    one_tensor = tf.ones([length, depth])                                                           # Erstellung eines Arrays aus 1-en\n",
    "    dropout_tensor = dropout_layer(one_tensor, training=True)                                       # Anwendung des Dropout auf die Layer\n",
    "\n",
    "    # Erstellung der Kontext Layer\n",
    "    tokens = tokenizer_drop.tokenize(input)                                                         # Tokenisierung des Inputs\n",
    "    input_without_eos = tokens[tf.newaxis, :, 1:-1]                                                 # Auswahl der Tokens bis zum [END] Token\n",
    "    context = model.model.enc_embed(input_without_eos)                                              # Erstellung des Embedding durch das Modell\n",
    "    context_drop = dropout_layer(context, training=True)                                            # Anwendung des Dropout auf das Embedding\n",
    "\n",
    "    return dropout_tensor, context_drop, context\n",
    "\n",
    "def out(length, depth, dropout, input):\n",
    "    VisualWrapper.reset_visualiser()                                                   \n",
    "    dropout_tensor, context_drop, context = dropout_function(length, depth, dropout, input)\n",
    "    VisualWrapper.color_bar(dropout_tensor)\n",
    "                               \n",
    "    VisualWrapper.color_bar(context.to_tensor())                     \n",
    "    VisualWrapper.color_bar(context_drop.to_tensor())\n",
    "    \n",
    "\n",
    "output_widget_dropout = widgets.interactive_output(out,\n",
    "                                                   {'length': length_widget_drop, 'depth': depth_widget_drop, 'dropout': dropout_widget, 'input': input_widget_drop}, \n",
    "                                                   )\n",
    "\n",
    "display(length_widget_drop, depth_widget_drop, dropout_widget, input_widget_drop, output_widget_dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d176e24-7e39-4e92-95d5-3d24eef076ec",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesem Beispiel wird der Effekt von Dropout auf die Layer und das Embedding dargestellt. Dafür werden in der Funktion *dropout_function()* jeweils Dropout auf die Layer und auf das Embedding angewendet. Dafür wird in Zeile 36 und 42 jeweils die jeweilige Transformation auf den beiden Objekten angewendet. Damit wird also ein gewisser Teil, welche mit dem Parameter \"Dropoutrate\" bestimmt wird, der Werte Layer bzw. des Embeddings den weiteren Verarbeitungsschritten vorenthalten. Dieser Parameter ist ein prozentualer Wert, d.h. bei einem Wert von 0.2 werden 20% der Werte vorenthalten. \r\n",
    "\r\n",
    "Für das Beispiel können dieses Mal die Länge des Tensors (Eingabe) und die Tiefe des Tensors bestimmt werden. Ebenso kann die Dropoutrate verändert werden. In der ersten Grafik sieht man welche Werte in einem uniformen Vektor vom Dropout verändert werden. Das bedeutet, welche der Werte in der Tiefe des Tensors ausgeblendet werden. Der Wert ist entspricht dabei einer Prozentzahl bei 0.4 werden somit 40 Prozent der Werte ausgeblendet bzw. auf den festgesetzten Wert gesetzt.\r\n",
    "In den beiden darauffolgenden Grafiken wird das Dropout auf den im Textfeld eingegebenen Beispieltext angewandt, nachdem er durch den Tokenizer und die Einbettung in Vektorform gebracht wurde. Die erste Grafik zeigt den vollständigen Vektor und die zweite Grafik den Vektor, der vom Dropout verändert wurde. Hier sieht man die ausgelassenen Positionen sehr gut. Mit dem Erhöhen der Dropout Rate, werden diese mehr. Außerdem kann man erkennen, dass sich, wenn man den Dropout erhöht, der Wertebereich ebenfalls ausweitet. Erklärbar ist dies dadurch, dass Merkmale, vor allem prägnante Merkmale, sich durch das weitere Training verstärken bzw. abschwächen. Die Dropoutrate kann jedoch nicht beliebig erhöht werden bei gleichzeitigem Erhalt der Güte der Klassifikation.\r\n",
    " \r\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2d169f9",
   "metadata": {},
   "source": [
    "### Normalisierung\n",
    "\n",
    "Normalisierung ist eine Technik, die von [20] eingeführt wurde. In tiefen neuronalen Netzen, mit nicht-linearen Aktivierungsfunktionen wie der Sigmoid-Funktion\n",
    "\n",
    "$$g(x) = 1/(1+exp(-x))$$\n",
    "\n",
    "trainiert werden gilt, dass $g’(x) -> 0 für |x|-> inf$. \n",
    "\n",
    "Das führt dazu, dass diese Funktionen in einen Bereich geraten können, in der der Gradient für Stochastic Gradient Descent (SGD) minimal wird und man spricht vom Vanishing Gradient Problem.\n",
    "\n",
    "In neuronalen Netzen ist hierbei das Problem, dass eine Layer $z = g(Wx + b)$ mit der Sigmoid-Funktion g versucht den Output des gesamten vorherigen Netzes x zu gewichten. Dabei hängen sowohl W als auch b von x ab. Da sich während des Trainings alle Layers des Netzes fortwährend aktualisieren, ändert sich auch der Input x fortwährend. \n",
    "Dieser Effekt wird von [20] Internal Covariate Shift genannt. \n",
    "\n",
    "Je tiefer das neuronale Netz, umso größer sind diese Veränderungen, da es mehr Schichten gibt, die sich verändern können. Die Tiefe einer neuronalen Netzes erhöht also die Wahrscheinlichkeit das ein Vanishing Gradient Problem auftritt und ein effektives Training stoppt.\n",
    "\n",
    "Transformer wie sie [1] beschrieben nutzen hierfür Layer Normalisation ein Normalisierungsalgorithmus den [21] entwickelt hat.\n",
    "\n",
    "<span style=\"color:red\">Interactive Application - In- Output of LayerNorm Comparison funktioniert nicht richtig.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a9d2b5",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60cda8fb3214d1d85128bdb186d2b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Button(description='Click to proceed', style=ButtonStyle())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VisualWrapper.reset_visualiser()\n",
    "tokenizer_norm = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input=\"Test\"\n",
    "\n",
    "tensor_input = tf.convert_to_tensor(input)\n",
    "if len(tensor_input.shape) == 0:\n",
    "    tensor_input = tensor_input[tf.newaxis]\n",
    "\n",
    "tokenized_input = tokenizer_norm.tokenize(tensor_input).to_tensor()                             # Anwendung eines Tokenizers mit Normalisierung\n",
    "input_without_eos = tokenized_input[:, :-1]\n",
    "context = model.model.encode(input_without_eos, None)\n",
    "\n",
    "VisualWrapper.visualize_data(id='layer')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57791aa1",
   "metadata": {},
   "source": [
    "\n",
    "### Residuale Verbindung\n",
    "\n",
    "Die Idee für das Nutzen von Residualen Verbindungen kommt von [22]. Die Autoren stellten fest, dass bei neuronalen Netzen sowohl die Genauigkeit während des Trainings als auch die Genauigkeit auf dem Testdatensatz mit zunehmender Tiefe schlechter wird.\n",
    "\n",
    "Da durch Normalisierung bereits sichergestellt ist, dass Vanishing Gradients kein Problem darstellen, scheitert die Optimierung der neuronalen Netze aus anderen Gründen.\n",
    "Einer der Gründe hierfür liegt vermutlich darin, dass die tieferen Schichten eines Modells zu Beginn des Trainings sehr viel stärker zur Ausgabe beitragen, als die vorhergehenden Schichten. Sie werden somit zuerst trainiert. Die weniger tiefen Schichten werden erst ausreichend trainiert, wenn in den tiefen Schichten keine Optimierung mehr möglich ist.\n",
    "\n",
    "Eine Lösung hierfür sind die residualen Verbindungen.\n",
    "Residuale Verbindungen ersetzen eine Schicht F(x) durch ihre residuale Verbindung\n",
    "\n",
    "\\\n",
    "$$H(x) = F(x) + x.$$\n",
    "\n",
    "<br>\n",
    "In das Ergebnis von H(x) geht also sowohl der Output, als auch der Input von F direkt mit ein. Wendet man dieses Prinzip auf die Schichten tiefer neuronaler Netze an, sorgt das dafür, dass gleich zu Beginn, der Output der wenig tiefen Schichten relevant in den Output des gesamten Netzes einfließt, denn es gilt für das gesamte Netz N:\n",
    "\n",
    "\\\n",
    "$$N(x) = H_n(H_n-1(x)) + H_n-1(x) = H_n(H_n-1(x)) + H_n-1(H_n-2(x)) + … + H_2(H_1(x)) + H_1(x)$$\n",
    "\n",
    "<br>\n",
    "Wie man in <a href=\"#fig:input\">Abbildung 1</a> sehen kann, haben alle Aufmerksamkeitsmodule, sowie alle Feed-Forward Schichten in einem Transformermodell eine residuale Verbindung.\n",
    "\n",
    "In der nachfolgenden Simulation können Sie sehen, wie sich die Ausgabe einer neuronalen Schicht verändert, wenn man ihr eine residuale Verbindung beifügt. Der Effekt auf den Trainingsprozess lässt sich dabei natürlich nur schwer darstellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb96ca1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Interaktiver Vergleich mit/ohne residuale Verbindung. Evtl. wäre es auch spannend den Trainingsprozess hier zu starten und Veränderungen dabei aufzuzeigen. Dies geht aber vermutlich interaktiv nicht, da es zu lange dauert und zu viele Ressourcen benötigt. Man könnte allerdings eine Aufzeichnung der zugehörigen Daten machen. Dies zu implementieren ist aber gerade nicht machbar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa40a6df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Modellschichten\n",
    "\n",
    "### Aufmerksamkeit\n",
    "\n",
    "Die Neuerung von Transformern im Vergleich zu vorangegangenen Lösungen für Neuronalen Maschinellen Übersetzen (NMT) ist es, allein auf die Aufmerksamkeit als Mechanismus für das Verarbeiten von Sprache zu setzen. Aufmerksamkeit wurde auch vorher schon von [15] zur Verbesserung von RNNs zur Übersetzung von Texten verwendet.\n",
    "\n",
    "Der Aufmerksamkeitsmechanismus, wie ihn [1] beschreiben, orientiert sich dabei an der Idee einer Suchanfrage des Ausgabetextes and sich selbst, bzw. den Eingabetext. Die Aufmerksamkeitsschicht bekommt dabei zwei oder eigentlich drei Eingaben: \n",
    "\n",
    "1. den Query (Q), \n",
    "2. den Key (K),\n",
    "3. den Value (V).\n",
    "\n",
    "In der Praxis erhalten aber Key und Value in Transformern immer dieselbe Eingabe und häufig sind Query, Key und Value sogar identisch. Aus welcher Quelle Query, Key und Value kommen unterscheidet unterschiedliche Formen von Aufmerksamkeit. So nennen wir Selbstaufmerksamkeit denjenigen Fall indem Q=K=V gilt und Kreuzaufmerksamkeit denjenigen Fall indem der Query aus der Ausgabe des Encoder besteht und Key und Value beide aus der Ausgabe eines Decoderblocks stammen.\n",
    "\n",
    "Um zu erklären, wie Aufmerksamkeit funktioniert, sollten wir aber zunächst davon ausgehen, dass Query-, Key- und Value-Eingabe verschieden sind. Ich schreibe bewusst von der Eingabe, da in jeder Aufmerksamkeitsschicht zunächst Query-, Key- und Value-Eingabe Q', K', V' mit Hilfe von gewichteten Matrix W^Q, W^K und W^V in Query, Key und Value \n",
    "\n",
    "$$Q=Q′×W^Q$$\n",
    "$$K=K′×W^K$$\n",
    "$$V=V′×W^V$$ \n",
    "\n",
    "umgewandelt werden. Diese gewichteten Matrizen W^Q, W^K, W^V sind die trainierbaren Gewichte der Aufmerksamkeitsschicht. Alle nachfolgenden Prozesse sind deterministisch. Das bedeutet, dass diese Umwandlungprozesse festelegen, welches Ergebnis die Aufmerksamkeitsschicht liefert.\n",
    "Betrachten wir aber, was passiert, wenn Query, Key und Value durch diese Matrizen gewichtet wurden. Eine grafisch aufbereitete Erklärung dessen, was ich beschreibe findet sich übrigens bei [9].\n",
    "\n",
    "#### Aufmerksamkeitsfunktion\n",
    "\n",
    "Die Aufmerksamkeitsfunktion bekommt die Eingaben Q, K, V aus der vorher beschriebenen Gewichtung und lautet:\n",
    "<br>\n",
    "<br>\n",
    "$$Attention(Q, K, V)=softmax(QK^T/sqr(d_k))V$$\n",
    "<br>\n",
    "Es wird also zuerst das Kreuzprodukt aus Q und K gebildet. Dieses Produkt wird mit sqr(*d_k*) skaliert, weshalb ist in folgendem [Kaptiel](#skalierung-mit-sqrd_k) nachzulesen, und auf dieses Ergebnis wird dann die Softmax-Funktion sigma(x) angewandt.\n",
    "Diese Funktion lässt sich am besten positionsweise beschreiben\n",
    "<br>\n",
    "<br>\n",
    "$$sigma(x)_i = exp(x_i)/sum_j=1^n(exp(x_j)) für i=1,...,n.$$\n",
    "<br>\n",
    "Nennen wir also\n",
    "<br>\n",
    "$$softmax(QK^T/sqr(d_k)) = Score(Q,K),$$\n",
    "<br>\n",
    "dann ist \n",
    "<br>\n",
    "$$Attention(Q, K, V) = Score(Q,K)*V $$\n",
    "<br>\n",
    "eine Funktion, die $V$ mit einem Vektor multipliziert, wobei für den Vektor gilt $|Score(Q,K)| = 1$.\n",
    "\n",
    "$Score(Q,K$) ist also ein Vektor exakt der Länge von $V$, der angibt, mit welchem Anteil jeder Eintrag von $V$ in den Ausgabevektor $Attention(Q,K,V)$ eingehen soll. Dabei summiert sich $Score(Q,K)$ zu 1, ist also tatsächlich eine Gewichtung der Einträge von $V$.\n",
    "\n",
    "Da mit $Score(Q,K)$ nun also eine Gewichtung besteht, wie stark die Ausgabe $Attention(Q,K,V)_i$ von $V_j$ abhängt, kann man dieses Verhältnis als Matrix grafisch darstellen. Dies geschieht in der Simulation unten, bei der für den vorgegebenen Satz eine Aufmerksamkeitsgewichtung aus einer der Aufmerksamkeitsschichten dargestellt wird. Der Eintrag in der $i-ten$ Zeile und $j-ten$ Spalte gibt dabei den einfluss von $V_j$ auf $Attention(Q,K,V)_i$ an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3afaf5d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9605a0adbe604a809a3b5426d5334423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.', conti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be53eab578444754b53b96e827dbe427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Aufmerksamkeit berechnen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77fe40b656c94bc693342f3b52a39a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_attn = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "attn_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_attn = widgets.Text(\n",
    "    value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_attn = widgets.Button(description='Aufmerksamkeit berechnen',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_attn = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def create_tokenized_embeddings():\n",
    "        tensor_input = tf.convert_to_tensor(input_widget_attn.value)                # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)    # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "        if len(tensor_input.shape) == 0:                                            # Überprüft, ob der Eingabetensor im korrekten Format ist                                     \n",
    "            tensor_input = tensor_input[tf.newaxis]                                 # Falls nicht, wird eine Dimension hinzufügt \n",
    "\n",
    "    \n",
    "        tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()              # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "        input_without_eos = tokenized_input[:, :-1]\n",
    "        context = transformer.encode(input_without_eos, None)                       # Erstellung der Kontext-Vektoren vom Transformer-Modell\n",
    "\n",
    "        # Write the input tokens (excluding the last one) to the output array\n",
    "        for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "            output_array = output_array.write(i, value)\n",
    "\n",
    "        dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "        dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_attn:\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 6\n",
    "        #output_widget_attn.clear_output()  # clear the previous output\n",
    "        create_tokenized_embeddings()\n",
    "        VisualWrapper.visualize_data(id='attention')\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 1\n",
    "            \n",
    "\n",
    "button_widget_attn.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_attn, button_widget_attn, output_widget_attn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "162b450c",
   "metadata": {},
   "source": [
    "\n",
    "#### Multi-headed Aufmerksamkeit\n",
    "\n",
    "In der Praxis hat sich bewährt parallel mehrere dieser Aufmerksamkeitsmechanismen durchzuführen. Dazu werden zu Beginn h gewichteten Matrizen \n",
    "\n",
    "$$W_i^Q, W_i^K, W_i^V i= 1,...,h$$ \n",
    "\n",
    "eingeführt. Diese erzeugen also h verschiedene Matrixtripel \n",
    "\n",
    "$$Q_i, K_i, V_i i = 1,...,h$$\n",
    "\n",
    "und somit ergeben sich h verschiedene Ausgaben \n",
    "\n",
    "$$H_i = Attention(Q_i, K_i, V_i) i=1,...,h.$$\n",
    "\n",
    "Diese werden nun zu einer einzigen Ausgabe zusammengeführt, indem wir \n",
    "\n",
    "$$MultiHeadAttention(Q_in,K_in,V_in) = Concat(H_1,..., H_h)W^O$$\n",
    "\n",
    "berechnen. Dabei ist $Concat(A_1,...,A_n)$ das hintereinanderschreiben mehrerer Matrizen und $W^O$ eine weitere trainierbare Matrix, die die verschiedenen Ausgaben $H_1,..., H,h$ gewichtet.\n",
    "Deshalb sehen wir in der obigen Ausgabe auch x verschiedene Matrizen, die $Score(Q_i,K_i)$ darstellen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fec2a5af",
   "metadata": {},
   "source": [
    "\n",
    "### Maskieren\n",
    "\n",
    "Das Maskieren des Inputs ist eine wichtige funktionale Komponente der Transformerarchitektur. Beim Maskieren handelt es sich in Wirklichkeit um zwei Mechanismen, die zwar dieselbe Funktionsweise besitzen, aber sehr unterschiedliche Aufgaben in der Architektur besitzen. Einerseits das Subsequent Masking und das Padding Masking. Das Padding Masking stellt jediglich sicher, dass nur Positionen mit Inhalt vom Transformer verarbeitet werden, während das Subsequent Masking dafür sorgt, dass der Decoder des Transformers autoregressiv ist. Das bedeutet, bei einer Vorhersage für eine Ausgabe an der Position $i$, soll das Modell nur Informationen aus den Eingabepositionen $1,…,i-1$ nutzen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e465d9",
   "metadata": {},
   "source": [
    "#### Padding Masking\n",
    "\n",
    "Das Padding Masking ist notwendig, da Transformer sequentielle Daten so verarbeiten, als ob sie eine fixe Länge $d_{model}$ hätten. Das geschieht deshalb, weil Transformer so lernen können jede Position der Ausgabelänge $d_{model}$ parallel vorherzusagen. \n",
    "\n",
    "Um während des Trainings auch Daten mit einer Länge größer oder kleiner $d_{model}$ zu nutzen, werden längere Sequenzen abgeschnitten und kürzere mit Nullen aufgefüllt. Diese Nullen müssen dann mit Hilfe von Padding Masking für das Training irrelevant gemacht werden. Das geschieht indem man alle Positionen die eine Null enthalten für das Modell auf $-inf$ setzt, sodass sie beim Gradient Descent nicht berücksichtigt werden.\n",
    "\n",
    "#### Subsequent Masking\n",
    "\n",
    "Das Subsequent Masking benutzt die gleich Technik des Werte auf $-inf$ setzen. Dabei wird aber nicht das mit irrelevanten Informationen angefüllte Ende des Eingabevektors auf Null gesetzt, sondern es werden diejenige Werte von $Score(Q, V)$ auf $-inf$ gesetzt, die einen Wert $V_j$, für die Ausgabe $i$ $Attention(Q,K,V)_i$ gewichten würden, obwohl $j>i$ gilt. Also es wird beschränkt, dass $Score(Q, V)$ nur diejenigen Werte von $V$ einbeziehen darf, die bei der Vorhersage für den i-ten Wert schon bekannt sind.\n",
    "\n",
    "Dass das relevant ist liegt daran, dass beim Training von Transformern ja der komplette Input gleichzeitig verwertet wird. Ein Satz wird als ganzes vom Transformer verarbeitet und er erzeugt eine Vorhersage, welches Token an einer bestimmten Position in diesem Satz vorkommt. Dabei erhält er in der Eingabe aber schon die Information, welches Token an dieser Position tatsächlich steht, da er ja den kompletten Satz als Eingabe bekommen hat. Dies muss nun also innerhalb des Modells mit einer Maskierung wieder rückgängig gemacht."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a0ea31",
   "metadata": {},
   "source": [
    "\n",
    "#### Verschiedene Aufmerksamkeitstypen\n",
    "\n",
    "In der Architektur werden verschiedene Aufmerksamkeitstypen unterschieden. Es gibt dabei zwei Variablen die beeinflussen, welche Art von Aufmerksamkeit wir verwenden. Die erste Variable ist, woher die Eingaben $Q', K' und V'$ kommen. Die zweite Variable ist die Maskierung, die wir auf $Score(Q,K)$ anwenden.\n",
    "\n",
    "##### Selbst-Aufmerksamkeit\n",
    "\n",
    "Die Aufmerksamkeit nennen wir Selbst-Aufmerksamkeit, wenn gilt \n",
    "\n",
    "$$Q'=K'=V'.$$ \n",
    "\n",
    "Wenn sich $Score(Q,K)$ also bildlich gesprochen daraus ergibt, welche Aufmerksamkeit jede Position der Eingabe auf eine andere Position derselben Eingabe legt und diese Aufmerksamkeit auf die Eingabe selbst angewandt wird.\n",
    "\n",
    "##### Kreuz-Aufmerksamkeit\n",
    "\n",
    "Wie nennen die Aufmerksamkeit Kreuz-Aufmerksamkeit, wenn gilt \n",
    "\n",
    "$$K' = V'$$\n",
    "\n",
    "aber $Q'$ von diesen beiden Werten verschieden ist.\n",
    "Wenn sich $Score(Q,K)$ also daraus ergibt, welche Aufmerksamkeit jede Position einer Eingabe $Q'$ auf die Positionen einer zweiten Eingabe $K'$ gibt und dieser Aufmerksamkeitsscore auf die zweite Eingabe angewandt wird.\n",
    "\n",
    "Dies ist zum Beispiel in Transformern der Fall, wenn $Q'$ sich aus der Ausgabe des Encoder ergibt und $K' = V'$ ein Zwischenergebnis des Decoders ist.\n",
    "\n",
    "##### Maskierte Aufmerksamkeit\n",
    "\n",
    "Ein Fall von maskierter Aufmerksamkeit liegt dann vor, wenn bestimmte Werte von $Score(Q,K)$ maskiert werden. Das ist zum Beispiel beim Subsequent Masking der Fall, hier wird $Score(Q,K)i,j = -inf$ gesetzt für alle Einträge $j>i$. Dadurch wird verhindert, dass die Ausgabe $Attention(Q,K,V)_i$ sich auf die Werte $V_j, j>i $ stützt. Zum Beispiel wird während des Trainings im Decoder dadurch verhindert, dass das Model lernt Informationen aus den zukünftigen Einträgen $V_j, j>i$ zu benutzen, um $V_i$ vorherzusagen. Man sieht gut in der Darstellung von $Score(Q,K)$, dass die Werte für $j>i$ dadurch meistens nahe bei $0$ liegen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b92c7bb2",
   "metadata": {},
   "source": [
    "### Skalierung mit sqr(d_k)\n",
    "\n",
    "Dasselbe Problem des Vanishing Gradient könnte innerhalb der Aufmerksamkeitsfunktion auftauchen, da hier $softmax(QK^T)V$ berechnet wird, das Skalarprodukt $QK^T$ mit $d_{model}$ skaliert und die Softmaxfunktion\n",
    "    \n",
    "$$sigma(z)_i = exp(z_i)/sum_j=1^N(exp(z_j)) für j=1,...,N $$\n",
    "\n",
    "somit leicht in einen saturierten Bereich mit extrem kleinen Gradienten gerät. Deshalb wird $QK^T$ mit $sqr(d_{model})$ skaliert: $softmax(QK^T/sqr(d_{model}))$, um die Skalierung mit $d_{model}$ zu minimieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cae0c164",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Interaktive Anwendung vgl. eines Tensors vor und nach dem Maskieren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77e5aaf9",
   "metadata": {},
   "source": [
    "Zuletzt findet sich hier nun noch eine Simulation des kompletten Inferenzvorgangs innerhalb eines Transformermodells. In dieser Simulation kann man noch einmal alle vorher beschriebenen Schritte nachvollziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d3cf257",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60de3c9717784f019e8f12ffd7f27fad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test sentence', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e3c9c5d5c84cdfb873c9bdeebfd7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run interactive inference', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f903b4d530ab49ddb757750b22deef8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_inf = widgets.Text(\n",
    "    value='Test sentence',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  \n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_inf = widgets.Button(description='Run interactive inference',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_inf = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_inf:\n",
    "        output_widget_inf.clear_output()  \n",
    "        inference_model(input_widget_inf.value, interactive=True) \n",
    "        inference_model.print_results(visualisation=True)\n",
    "\n",
    "button_widget_inf.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_inf, button_widget_inf, output_widget_inf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82dc7ff0",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "[1] Vaswani, A. et al. Attention Is All You Need. Preprint at http://arxiv.org/abs/1706.03762 (2017).\n",
    "\n",
    "[2] Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Computation 9, 1735–1780 (1997).\n",
    "\n",
    "[3] Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks 5, 157–166 (1994).\n",
    "\n",
    "[4] Cho, K., van Merrienboer, B., Bahdanau, D. & Bengio, Y. On the Properties of Neural Machine Translation: Encoder–Decoder Approaches. in Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation 103–111 (Association for Computational Linguistics, 2014). doi:10.3115/v1/W14-4012.\n",
    "\n",
    "[6] Kaplan, J. et al. Scaling Laws for Neural Language Models. Preprint at http://arxiv.org/abs/2001.08361 (2020).\n",
    "\n",
    "[5] Goodfellow, I., Bengio, Y. & Courville, A. Deep learning. (The MIT Press, 2016).\n",
    "\n",
    "[7] Radford, A. et al. Language Models are Unsupervised Multitask Learners. (2019).\n",
    "\n",
    "[8] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Preprint at http://arxiv.org/abs/1810.04805 (2019).\n",
    "\n",
    "[9] Alammar, J. The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. https://jalammar.github.io/illustrated-transformer/ (2018).\n",
    "\n",
    "[10] Encoder-Decoder. Understanding The Model Architecture | by Naoki | Medium. https://naokishibuya.medium.com/transformers-encoder-decoder-434603d19e1.\n",
    "\n",
    "[11] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. (2014).\n",
    "\n",
    "[12] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[13] Gage, P. A New Algorithm for Data Compression. (1994).\n",
    "\n",
    "[14] Cho, K. et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Preprint at http://arxiv.org/abs/1406.1078 (2014).\n",
    "\n",
    "[15] Bahdanau, D., Cho, K. & Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate. Preprint at http://arxiv.org/abs/1409.0473 (2016).\n",
    "\n",
    "[16] OpenAI. GPT-4 Technical Report. Preprint at http://arxiv.org/abs/2303.08774 (2023).\n",
    "\n",
    "[17] Grefenstette, G. & Tapanainen, P. What is a word, What is a sentence? Problems of Tokenization.\n",
    "\n",
    "[18] Lin, Z. et al. A Structured Self-attentive Sentence Embedding. Preprint at http://arxiv.org/abs/1703.03130 (2017).\n",
    "\n",
    "[19] Schmidt, R. M. Recurrent Neural Networks (RNNs): A gentle Introduction and Overview. Preprint at http://arxiv.org/abs/1912.05911 (2019).\n",
    "\n",
    "[20] Ioffe, S. & Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Preprint at http://arxiv.org/abs/1502.03167 (2015).\n",
    "\n",
    "[21] Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer Normalization. Preprint at http://arxiv.org/abs/1607.06450 (2016).\n",
    "\n",
    "[22] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
