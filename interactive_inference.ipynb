{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ab5bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:21.815663900Z",
     "start_time": "2024-01-16T09:57:16.327504900Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Tensorflow Module\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf_text\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Visualisierung und Eingabe\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "# Logging und Decorators\n",
    "import logging as log\n",
    "\n",
    "# Tensorflow Module\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Visualisierung und Eingabe\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact_manual, interactive, interact, VBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Backend Module\n",
    "from interactive_inference_backend import ModelLoader, StoryTokenizer, WordComplete, VisualWrapper, positional_encoding\n",
    "from interactive_inference_backend import reserved_tokens, vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff4da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.461354500Z",
     "start_time": "2024-01-16T09:57:23.186207800Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ModelLoader(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                            d_model=512,\n",
    "                            n_stacks=2,\n",
    "                            h_att=4,\n",
    "                            load_model=True,\n",
    "                            model_load_path=\"model_N2_h4_d512_t20230831-134344\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94fa9c-ba0a-4b73-92ed-2f9d2965a7ac",
   "metadata": {},
   "source": [
    "# Interaktive Erklärung der Transformerarchitektur\n",
    "\n",
    "## Inhaltsverzeichnis\n",
    "- [Einleitung](#einleitung)\n",
    "    - [Disclaimer](#disclaimer)\n",
    "    - [Ziel des Artikels](#ziel-dieses-interaktiven-artikels)\n",
    "    - [Transformer: Motivation & Kernkomponenten](#kurzübersicht-transformer)\n",
    "    - [Architekturübersicht](#architekturübersicht)\n",
    "        - [Encoder-Decoder](#encoder-decoder)\n",
    "        - [Architekturvarianten](#architekturvarianten)\n",
    "        - [Architekturblöcke](#architekturblöcke)\n",
    "- [Input](#input)\n",
    "    - [Tokenization](#Tokenization)\n",
    "    - [Byte-Pair Encoding](#byte-pair-encoding)\n",
    "    - [Embedding](#embedding)\n",
    "    - [Positional Encoding](#positional-encoding)\n",
    "- [Trainingsmethoden](#trainingsmethoden)\n",
    "    - [Dropout](#dropout)\n",
    "    - [Normalisierung](#normalization)\n",
    "    - [Residual Connection](#residual-connection)\n",
    "- [Layers](#layers)\n",
    "    - [Attention](#attention)\n",
    "        - [Vorteile von Transformern](#vorteile-von-transformern)\n",
    "        - [Attention als Funktion](#attention-function)\n",
    "        - [Skalierung mit $\\sqrt{d_{model}}$](#skalierung-mit-sqrd_k)\n",
    "        - [Multi-Headed Attention](#multi-headed-attention)\n",
    "    - [Masking](#masking)\n",
    "        - [Padding Masking](#padding-masking)\n",
    "        - [Subsequent Masking](#subsequent-masking)\n",
    "    - [Attention-Mechanismen](#attention-mechanismen)\n",
    "        - [Self-Attention](#self-attention)\n",
    "        - [Cross-Attention](#cross-attention)\n",
    "        - [Masked Attention](#masked-attention)\n",
    "- [Simulation](#simulation)\n",
    "- [Bibliographie](#bibliographie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634588dd",
   "metadata": {},
   "source": [
    "## <a id=\"einleitung\"></a>Einleitung\n",
    "\n",
    "### <a id=\"disclaimer\"></a>Disclaimer\n",
    "\n",
    "Wir haben diesen Artikel für eine deutschsprachige Leserschaft verfasst. Da viele Begriffe aus dem Bereich des Machine Learning kein präzises deutsches Pendant besitzen, haben wir uns bewusst dafür entschieden, die Fachterminologie möglichst konsistent in englischer Sprache zu verwenden.\n",
    "\n",
    "Jeder Abschnitt beginnt mit einer kompakten, verständlich formulierten Einführung in einer grau hinterlegten Box, die sich an Leser:innen mit Abiturniveau richtet. Im Anschluss daran tauchen wir jeweils tiefer in die zugrunde liegenden Konzepte und Funktionsweisen ein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a793e0",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"ziel-dieses-interaktiven-artikels\"></a>Ziel des Artikels \n",
    "\n",
    "Das Ziel dieses Artikels besteht darin, die Transformerarchitektur aus Vaswani et al. [1] durch die Simulation ihrer Verarbeitungsschritte und Komponenten zu erläutern. Diese Simulation erlaubt es, interaktiv die Auswirkungen verschiedener Eingaben auf die Verarbeitungsschritte zu untersuchen und so ein schrittweises Verständnis der Gesamtverarbeitung zu entwickeln. Während sich wissenschaftliche Literatur wie der ursprüngliche Artikel [1] und darauf aufbauende wissenschaftliche Arbeiten [7, 8], Erklärartikel oder -videos [9, 10] sich oft auf einzelne Komponenten wie z.B. Attention-Blöcke konzentrieren, werden andere Elemente, die technischen Feinheiten der Architektur, wie z.B. \"<a href=\"#dropout\">Dropout</a>\" [11], \"<a href=\"#residual-connection\">Residual Connections</a>\" [12], \"<a href=\"#byte-pair-encoding\">Byte-Pair Encoding</a>\" [13], Embedding oder des \"Log-Softmax Algorithmus\" [5] oft nicht oder nur rudimentär erklärt. Genau diese Begriffe möchten wir erklären und darüber hinaus durch unsere Simulation erfahrbar machen. Sollten sie als Manager oder als Entwickler in Betracht sein eine Transformerarchitektur für eine Machine Learning Anwendung in Betracht zu ziehen, dann möchten wir, dass sie am Ende des Artikels verstehen, wie die Technologie funktioniert und wie sie sie implementieren könnten oder jemand anderes sie implementiert hat.\n",
    "Zudem soll über die Simulation das Zusammenspiel der Elemente der Transformer Architektur verdeutlicht werden, was wesentlich ist für ihr Verständnis. Wir wünschen viel Spaß beim Lesen und Simulieren.\n",
    "\n",
    "#### Voraussetzungen\n",
    "\n",
    "Trotz unseres Ziels die Transformer möglichst umfänglich zu erklären setzen wir einige Informationen ihrerseits voraus. Sie müssen keinerlei Vorwissen über die Funktionsweise von Transformern mitbringen, allerdings sollten sie dazu in der Lage sein die mathematische Theorie, sowie die Logik, die hinter der Entwicklung von Machine Learning Algorithmen zu verstehen. Spezifisch setzen wir die Begriffe und Ideen der lineare Algebra und der Wahrscheinlichkeitstheorie, die dem Machine Learning zugrundeliegen voraus. Alternativ empfehlen wir sich die Grundlagen beider Theorien im Standardwerk des Deep Learning anzueignen [5]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d44de",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"kurzübersicht-transformer\"></a>Transformer: Motivation & Kernkomponenten\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Stellen wir uns eine einfache Aussage vor: \"Die Hauptstadt von Deutschland ist ...\"  Ein Mensch liest diese Aussage, erkennt, dass es sich um eine geografische Wissensaussage handelt, und antwortet sofort mit „Berlin“. Dabei wird intuitiv auf relevante Informationen zurückgegriffen, während irrelevante ignoriert werden. Transformer-Modelle funktionieren ähnlich: Sie verarbeiten Texte, erkennen dabei Muster und Zusammenhänge und geben eine Antwort auf Basis der ihnen zur Verfügung stehenden Informationen. Ihre besondere Stärke liegt darin, „Aufmerksamkeit“ gezielt auf wichtige Stellen im Text zu richten genau wie ein Mensch, der beim Lesen erkennt, welche Wörter für die Beantwortung der Frage besonders wichtig sind.\n",
    "</div>\n",
    "Transformer-Modelle sind eine von Vaswani et al. [1] vorgeschlagene Architektur zur Verarbeitung sequenzieller Daten, also solcher, die eine bestimmte Reihenfolge aufweisen etwa Zeitreihen, Texte oder Bildfolgen. Im Gegensatz zu früher weit verbreiteten Architekturen wie Recurrent Neural Networks (RNN) [2, 3] oder Convolutional Neural Networks (CNN) [4] ermöglichen Transformer das parallele Verarbeiten dieser Daten. Dies führt bei entsprechender Hardware wie etwa GPUs (Grafikkarten mit typischerweise tausenden parallelen Recheneinheiten) oft zu einer deutlich kürzeren Trainingszeit [1, 6].\n",
    "\n",
    "Transformer-Modelle werden typischerweise für Aufgaben eingesetzt, bei denen das nächste Element in einer Sequenz vorhergesagt werden soll sei es der nächste Datenpunkt einer Zeitreihe, das nächste Wort in einem Text, der nächste Datenpunkt in einer Zeitreihe oder das nächste Bild in einem Video. Im Folgenden konzentrieren wir uns zur Veranschaulichung auf die Verarbeitung von Textdaten.\n",
    "\n",
    "Das zentrale Element von Transformern ist der sogenannte „Attention“-Block. Er bestimmt, welchen Teilen der Eingabe das Modell beim Verarbeiten besondere Aufmerksamkeit schenkt. Denn nicht jedes Wort in einem Satz trägt gleich viel Bedeutung für die Vorhersage des nächsten Worts. Anstelle ganzer Wörter arbeitet das Modell mit sogenannten Token – häufig auftretenden Buchstabenkombinationen, die Wörter, Wortteile oder ganze Ausdrücke darstellen können. Die Attention-Blöcke bestehen mathematisch gesehen aus trainierbaren Matrizen, welche die Eingabe (etwa einen Vektor, der eine Frage an einen Chatbot kodiert) in eine Ausgabe überführen – also einen Vektor, der die passende Antwort repräsentiert. Ziel ist es, durch diese Projektion möglichst präzise Antworten zu erzeugen. Das Vorgehen ähnelt dabei dem menschlichen Lesen: Auch hier werden gezielt relevante Informationen herausgefiltert, um eine konkrete Frage zu beantworten.\n",
    "\n",
    "Beim Training mit Texten nutzen Transformer eine sogenannte Maske, die verhindert, dass das Modell bereits beim Lesen eines Wortes Informationen über zukünftige Wörter erhält, die es eigentlich vorhersagen soll. Dadurch wird ein natürlicher Lesefluss simuliert. So sieht das Modell bei dem Satz „Der Himmel ist klar und blau“ zunächst nur „Der Himmel ist“ und soll daraus das nächste Wort „klar“ vorhersagen. Die folgenden Wörter „und blau“ werden dabei maskiert, da sie zu diesem Zeitpunkt im Lesefluss noch nicht bekannt sind. Schrittweise verschiebt sich die Maske über den Text, sodass das Modell lernt, jede Vorhersage nur auf bereits gelesene Informationen zu stützen.\n",
    "\n",
    "In der sogenannten Inferenzphase – also während das Modell zur Beantwortung von Fragen oder Generierung von Text verwendet wird – funktioniert der Prozess ähnlich: Zunächst wird das erste Token der Antwort erzeugt, das dann zur Eingabe hinzugefügt wird, um das nächste Token zu generieren. Sobald das sogenannte Context-Window, also die maximale Anzahl an Tokens in der Eingabe, erreicht ist, wird das jeweils älteste Token entfernt. Dieses Context-Window bildet gewissermaßen das Gedächtnis des Modells.\n",
    "\n",
    "Im Transformer werden sowohl Eingaben als auch Ausgaben intern als Vektoren dargestellt. Wie diese Vektoren aus Text erzeugt werden, welche Rolle Attention-Blöcke, Feedforward-Netze und weitere Komponenten spielen, und wie daraus wieder lesbare Antworten entstehen, wird in den folgenden Kapiteln erläutert. Dort werden auch zentrale Konzepte wie „Embedding“, „Positional Encoding“ und „Normalisierung“ näher erklärt. Ein interaktives Transformermodell im weiteren Verlauf des Artikels ermöglicht darüber hinaus eine schrittweise Simulation dieser Prozesse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a6b568b",
   "metadata": {},
   "source": [
    "### <a id=\"architekturübersicht\"></a>Architekturübersicht\n",
    "\n",
    "#### <a id=\"encoder-decoder\"></a>Encoder-Decoder\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Wenn man ein Sprachmodell fragt: „Die Hauptstadt von Deutschland ist “, muss es nicht nur die Wörter erkennen, sondern auch deren Bedeutung erfassen und die passende Antwort – „Berlin“ – generieren. Genau hier setzt die Encoder-Decoder-Architektur an: Sie trennt das Verstehen und das Antworten in zwei spezialisierte Teilmodelle. Der Encoder übernimmt das Verstehen, der Decoder das Generieren einer sinnvollen Antwort. Diese Struktur ist der Kern vieler moderner Sprachmodelle und bildet die Grundlage für das in diesem Abschnitt beschriebene Architekturkonzept.\n",
    "</div>\n",
    "Die Encoder-Decoder-Struktur wurde von Cho et al. [14] eingeführt und ist insbesondere in der maschinellen Übersetzung weit verbreitet. Sie besteht aus den zwei vorher erwähnten trainierten Modulen: dem <a href=\"#encoder-decoder\">Encoder</a>, der die Eingabe analysiert und abstrahiert, und dem <a href=\"#encoder-decoder\">Decoder</a>, der auf Basis dieser abstrahierten Information eine Ausgabe erzeugt. Der Vorteil dieser Struktur liegt in ihrer Flexibilität: Verschiedene Encoder und Decoder lassen sich für unterschiedliche Sprachen kombinieren, um so Übersetzungen zwischen beliebigen Sprachpaaren zu ermöglichen.\n",
    "\n",
    "<figure id=\"fig:fig2\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_model_architecture.jpg\" style=\"width: auto; height: 1000px;\" alt=\"Transformer als Encoder-Decoder\"/>\n",
    "    <figcaption>Abbildung 1: Transformer als Encoder-Decoder</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "In unserer konkreten Architektur wird die Eingabe – beispielsweise ein Satz zunächst vom <a href=\"#encoder-decoder\">Encoder</a> verarbeitet. Dabei wird sie durch mehrere Transformer-Layer geleitet, die jeweils auf <a href=\"#attention\">Self-Attention</a> basieren. Das Ergebnis ist eine Vektor-Repräsentation im sogenannten Latent Space, die den semantischen Gehalt der Eingabe zusammenfasst. Diese Repräsentation ist die einzige Information, die dem <a href=\"#encoder-decoder\">Decoder</a> übergeben wird. Der Decoder erhält nicht direkt Zugriff auf den ursprünglichen Text oder die Token der Eingabe, sondern nutzt ausschließlich diese verdichtete Darstellung. \n",
    "<!--\n",
    "<i><b style=\"color:red;\">Der Decoder nutzt nicht direkt die Eingaben des Encoders, sondern nur den Latent Space des Encoders. Die weitere Eingabe in den Decoder ist der selbst von Decoder generierte Kontext. Bitte einmal in Text und Grafik differenzieren.</b></i>\n",
    "-->\n",
    "Der <a href=\"#encoder-decoder\">Decoder</a> generiert seine Ausgabe sequenziell. \n",
    "Dabei greift er zum einen auf den vom Encoder erzeugten Latent Space zurück über sogenannte <a href=\"#attention\">Cross-Attention</a>-Mechanismen –, zum anderen aber auch auf den bereits erzeugten Text, den er selbst in vorherigen Schritten produziert hat. Dieser eigene Kontext wird intern mithilfe von <a href=\"#attention\">Self-Attention</a> verarbeitet. Der entscheidende Punkt ist: Der Decoder kombiniert seine eigenen bisherigen Outputs mit der kodierten Repräsentation des Encoders, ohne direkten Zugriff auf den ursprünglichen Input.\n",
    "\n",
    "Wie diese Prozesse ablaufen, zeigt <a href=\"#fig:fig1\">Abbildung 1</a>: Die hellblau markierten Bereiche umfassen den <a href=\"#encoder-decoder\">Encoder</a>, bestehend aus mehreren identischen Schichten, die alle auf <a href=\"#attention\">Self-Attention</a> basieren. Die dunkelblauen Bereiche zeigen den <a href=\"#encoder-decoder\">Decoder</a>, der pro Schicht zwei zentrale Komponenten enthält – <a href=\"#attention\">Self-Attention</a> und <a href=\"#attention\">Cross-Attention</a>. Die Cross-Attention-Module kombinieren jeweils den Latent Space aus dem Encoder (Source Input) mit der internen Repräsentation des Decoders (Target Input), also mit dem bereits generierten Text. Dadurch entsteht eine gezielte Informationsverschmelzung zwischen semantischem Kontext und sprachlicher Ausgabe.\n",
    "\n",
    "Die Nutzereingabe wird in der sogenannten Input-Pipeline vorbereitet. Diese besteht zunächst aus einem Tokenizer, der die Eingabe in einzelne Tokens zerlegt. Anschließend erfolgt ein trainierbares Input Embedding, welches die Tokens in Vektoren übersetzt. Diese werden durch das <a href=\"#positional-encoding\">Positional Encoding</a> um Positionsinformationen ergänzt, damit das Modell Wortreihenfolgen erfassen kann. Erst danach gelangen sie in die Transformer-Layer des Encoders oder Decoders. \n",
    "\n",
    "Alle Elemente dieser Architektur sind in <a href=\"#fig:fig1\">Abbildung 1</a> übersichtlich dargestellt. Die grauen Bereiche kennzeichnen Eingabe- und Ausgabekomponenten sowie Komponenten mit fixierten Hyperparametern, etwa den fest eingestellten <a href=\"#dropout\">Dropout</a>. Schwarze Elemente wie das <a href=\"#positional-encoding\">Positional Encoding</a> oder der logaritmische Softmax sind deterministisch und enthalten keine lernbaren Parameter. Gelb hinterlegte Module wie die Embedding Weights oder die Transformer-Layer enthalten trainierbare Parameter und werden im Training angepasst. Eine genauere Darstellung der Transformer-Layer findet sich in <a href=\"#fig:fig4\">Abbildung 4</a>. Die blauen Elemente repräsentieren Datenzustände innerhalb des Modells, die durch verschiedene Operationen transformiert werden. Sind zwei blaue Elemente durch eine Linie verbunden, handelt es sich um dieselben Daten in verschiedenen Verarbeitungsschritten.\n",
    "\n",
    "Ein weiterer zentraler Punkt ist die Wiederholung der Transformer-Layer. Diese Schichtenstruktur wird nicht nur einmal durchlaufen, sondern n-mal, wobei n ein Hyperparameter des Modells ist. In der Architektur-Grafik ist dies exemplarisch durch die Verkettung zweier Layer dargestellt – tatsächlich wiederholt sich dieses Prinzip mehrfach, um eine tiefere semantische Verarbeitung zu ermöglichen. Informationen fließen dabei stets in Pfeilrichtung durch das Modell – rückwärts gerichtete Pfade sind nicht vorgesehen.\n",
    "\n",
    "Durch diese klare funktionale Trennung und die Kombination von <a href=\"#attention\">Self-Attention</a> und <a href=\"#attention\">Cross-Attention</a> gelingt es dem Modell, komplexe Eingaben zu analysieren und passende Ausgaben zu generieren – selbst bei Aufgaben, die ein tiefes Verständnis sprachlicher Zusammenhänge erfordern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847af04f",
   "metadata": {},
   "source": [
    "<!--\n",
    "<figure id=\"fig:fig2\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_model_architecture.jpg\" style=\"width: 300px; height: 150px; height: auto;\" alt=\"Transformer als Encoder-Decoder\"/>\n",
    "    <figcaption>Abbildung 1: Transformer als Encoder-Decoder</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0d18b",
   "metadata": {},
   "source": [
    "#### <a id=\"architekturvarianten\"></a> Architekturvarianten und verwirklichte Modelle\n",
    "\n",
    "Transformer können auch als reine Encoder oder reine Decoder-Architektur verwendet werden. Encoder verdichten Informationen (z.B. zur Klassifikation, Sentiment-Analyse, oder Clustering), während Decoder generativ (z.B. zur Textfortsetzung oder Bildgenerierung) eingesetzt werden, um aus einer verdichteten Repräsentation wieder Informationen zu generieren. Wie beschrieben war die Encoder-Decoder Architektur vor allem für Aufgaben des maschinellen Übersetzens gedacht. In der aktuellen Umsetzung sieht man jedoch häufig reine Encoder oder Decoder Architekturen. Dies liegt daran, dass die Architekturkomplexität dabei geringer ist und die spezifische Encoder-Decoder Architektur in anderen Aufgabenfeldern bzgl. Anforderungen und Trainingseffizienz gleichauf liegt. In der folgenden Tabelle [23] findet sich eine Auflistung der bekanntesten Modelle nach Architekturtyp.\n",
    "\n",
    "| Encoder           | Encoder-Decoder | Decoder        |\n",
    "|-------------------|-----------------|----------------|\n",
    "| BERT, DistillBERT | T5              | GPT            |\n",
    "| RoBERTa           | BART            | GPT-2          |\n",
    "| XLM, XLM-R        | M2M-100         | GPT-3          |\n",
    "| ALBERT            | BugBird         | GPT-4          |\n",
    "| ELECTRA           |                 | GPT-Neo, GPT-J |\n",
    "| DeBERTa           |                 |                | "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "398698e4",
   "metadata": {},
   "source": [
    "Hier können Sie nun einen Beispielsatz zuerst vom Encoder kodieren lassen, um ihn dann im nächsten Schritt vom Decoder dekodieren zu lassen und damit eine Ausgabe zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4798807b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.504807400Z",
     "start_time": "2024-01-16T09:57:25.462149900Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "transformer = model.model\n",
    "\n",
    "input_widget_enc_dec = widgets.Text(\n",
    "    value='Was ist die Hauptstadt von Deutschland?',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_enc = widgets.Button(description='Wende den Encoder auf die Eingabe an.',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "button_widget_dec = widgets.Button(description='Wende den Decoder auf die Eingabe an.',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_enc = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "output_widget_dec = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def encode():\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    if len(tensor_input.shape) == 0:                                           # Überprüft, ob der Eingabetensor im korrekten Format ist\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # Falls nicht, wird eine Dimension hinzufügt \n",
    "    \n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')\n",
    "    \n",
    "    context = transformer.encode(input_without_eos, None)                      # Kodierung des Inputsatzes von (Transformer-Modell)\n",
    "    return context, string_value, lookup\n",
    "\n",
    "\n",
    "def decode(): \n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)   # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "    if len(tensor_input.shape) == 0:                                           # wie bei der Encodierung\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # wie bei der Encodierung\n",
    "\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # wie bei der Encodierung\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    \n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')                      \n",
    "    context = transformer.encode(input_without_eos, None)                     \n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "                              \n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):                        # Schleife durch jedes Token des Satzes\n",
    "      output_array = output_array.write(i, value)                              # Speichern des Tokens im Output array\n",
    "\n",
    "    dec_input = output_array.concat()[tf.newaxis]                              # Output Array wird zu einem einzigen Tensor konkateniert \n",
    "                                                                               # und anschließend um eine zusätzliche Dimension erweitert\n",
    "\n",
    "    dec_out = transformer.decode(context, None, dec_input, None)               # Decoder des Transformer-Modells wird verwendet, um den dec_input-Tensor \n",
    "                                                                               # unter Verwendung des zuvor berechneten Kontexts zu decodieren.\n",
    "\n",
    "    return dec_out, string_value, lookup\n",
    "\n",
    "def on_button_click_enc(b):\n",
    "  with output_widget_enc:\n",
    "    output_widget_enc.clear_output()  # clear the previous output\n",
    "    context, tokens, lookup = encode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(context)\n",
    "\n",
    "\n",
    "def on_button_click_dec(b):\n",
    "  with output_widget_dec:\n",
    "    output_widget_dec.clear_output()\n",
    "    dec_out, tokens, lookup = decode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(dec_out)\n",
    "\n",
    "button_widget_enc.on_click(on_button_click_enc)\n",
    "button_widget_dec.on_click(on_button_click_dec)\n",
    "\n",
    "ui = widgets.VBox([\n",
    "  input_widget_enc_dec, \n",
    "  widgets.HBox([\n",
    "    widgets.VBox([\n",
    "      button_widget_enc, \n",
    "      output_widget_enc],\n",
    "      layout=widgets.Layout(width='50%')),\n",
    "    widgets.VBox([\n",
    "      button_widget_dec, \n",
    "      output_widget_dec],\n",
    "      layout=widgets.Layout(width='50%'))\n",
    "    ])\n",
    "  ])\n",
    "display(ui)\n",
    "\n",
    "\n",
    "#print('tok_out', tokenized_input)\n",
    "#print('enc_out', context)\n",
    "#print(\"dec_out\", dec_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa505ea8-2a43-43f6-936e-0d45b4c96f75",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "Über der Grafik sind die verarbeiteten Tokens dargestellt. Die Positionierung der Tokens entlang der y-Achse der Grafik zeigt die Reihenfolge der Tokens im Beispielsatz an, wobei Leerzeichen nicht berücksichtigt werden. Das Model erkennt unterschiedliche Wörter mit Hilfe der \"##\" Zeichen. Durch diese Zeichen sieht das Modell, dass ein Token mit \"##\" Zeichenkette zu dem vorherigen Token gehört wie im Beispiel bei dem Wort \"Encoder\", welches aus den Tokens 'e', '##n', '##co', '##der' besteht. Weitere Informationen in Abschnitt [Byte-Pair Encoding](#byte-pair-encoding).\n",
    "\n",
    "Schaut man sich die Wörter vor der Tokenisierung an, sieht man das alle Wörter nun kleingeschrieben werden. Durch derartige Zusammenfassungen wird die Größe des Vokabulars reduziert. Linguistische und grammatische Informationen, die aufgrund der Zusammenführung verloren zu gehen scheinen, bleiben i.d.R. durch die Position und den Kontext des Wortes im Satz erhalten, wie zum Beispiel Substantivierungen. Dies führt dazu, dass das Modell stärker auf die Position des Wortes im Satz achtet. Der Tokenizer fügt zudem ein Start- und Endtoken hinzu. Das Endtoken wird im Modell nicht weiterverwendet, während das Starttoken die Position 0 in der weiteren Verarbeitung und Visualisierung einnimmt.\n",
    "\n",
    "Die x-Achse repräsentiert die Tiefe der Token bzw. die Anzahl der Dimensionen des Vektors, der jedes Token darstellt. In diesem Fall beträgt die Tiefe 512, was bedeutet, dass jedes Token durch 512 verschiedene Werte charakterisiert wird. Diese Vektoren werden so gebildet, dass Tokens, die eine ähnliche Bedeutung haben oder aus einem Themenfeld stammen, ähnlichere Vektoren haben. Im Gegensatz dazu haben unähnliche Token eher unähnliche Vektoren. Eine hohe Dimensionalität der Vektoren kann allerdings dazu führen, dass Ähnlichkeiten primär in individuellen Vektorwerten erkennbar sind, wodurch die allgemeine Übersichtlichkeit eines Vektors im Vergleich zu einem ansonsten ähnlichen Vektor beeinträchtigt werden kann. Trotzdem tendieren ähnliche Vektoren dazu, auch in ihrer Gesamtorientierung Übereinstimmungen aufzuweisen. <b style='color:red;'><i>Die vorherigen beiden Sätze erschließen sich mir nicht. - So besser?</b></i> Zusätzlich ist in den Vektoren die Reihenfolge der Wörter im Satz kodiert, was als sogenanntes [Positional Encoding](#positional-encoding) bezeichnet wird.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028a3317",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### <a id=\"architekturbloecke\"></a>Architekturblöcke\n",
    "\n",
    "Prinzipiell lassen sich Transformer in mehrere Module einteilen: die Umwandlung der Eingabe in eine Vektorrepräsentation, die Abbildung der Eingaberepräsentation durch die Attention-Layers in einen Ausgabevektor, die Umwandlung des Ausgabevektor in eine menschenlesbare Datenform als Ausgabe.\n",
    "In <a href=\"#fig:fig1\">Abbildung 1</a> können wir die Ausgabe leicht von den anderen Elementen unterscheiden, sie besteht aus all denen Teilen die außerhalb der Umrandungen für Encoder und Decoder zu finden sind. Die Eingabe wiederum besteht aus dem sich für Encoder und Decoder gleichenden Teil der die Nutzer Input Daten in den Source und Target Input für die Transformer Layer umwandelt.\n",
    "\n",
    "1. [Eingabe](#input)\n",
    "\n",
    "Die Eingabe wandelt die Eingabedaten in eine Form, die für die Matrixtransformation genutzt werden kann. Wie diese Umwandlung aussieht unterscheidet sich für jeden Eingabedatentypen. In unserem Beispiel nutzen wir Textdaten, die wir durch <a href=\"#Tokenization\">Tokenization</a> [17] und <a href=\"#embedding\">Embedding</a> [18] in Tensoren verwandeln. \n",
    "\n",
    "- Ein Tensor ist eine mathematische Entität um multidimensionale Daten darzustellen. \n",
    "- Ein Skalar (eine einzelne Zahl) ist ein Tensor der 0. Ordnung, ein Vektor (eine eindimensionale Liste von Werten) ein Tensor der 1. Ordnung, eine Matrix ein Tensor der 2. Ordnung (eine zweidimensionale Tabelle von Zahlen) und multidimensionale Arrays von Zahlen ein Tensor höherer Ordnung. \n",
    "\n",
    "2. [Attention](#attention)\n",
    "\n",
    "Die Attention-Layers verarbeiten Daten in Tensorform und liefern eine Abbildung von den Eingabetensoren auf die Ausgabetensoren, die jeweils von den Eingabe- und Ausgabemodulen interpretiert wird.\n",
    "Man kann verschiedene Varianten von Attention-Layers dahingehend unterscheiden wie sich ihre Eingabedaten zusammensetzen. Jede Attention-Layer erhält als Eingabedaten dabei immer drei Tensoren. Diese werden Query, Key und Value genannt. Aus diesen berechnet eine Attention-Layer eine Ausgabe. Die Query fragt nach dem relevanten Kontext und repräsentiert das aktuelle Token für das das Modell Kontextinformationen sucht. Key stellt den Zusammenhang zwischen der Query und allen anderen Token her. Die Query wird mit allen Keys multipliziert und wenn der sich ergebende Wert groß ist, dann ist der Zusammenhang der zwei Token ebenfalls groß. Der Value gewichtet diesen Zusammenhang zur Bestimmung der Aufmerksamkeitswerte zwischen zwei Token (Attention Scores).\n",
    "\n",
    "Typischerweise werden Attention-Module dadurch unterschieden aus welcher Quelle Query, Key und Value stammen. Es wird unterschieden zwischen:\n",
    "\n",
    "- \"Self-Attention\" hierbei stammen Query, Key und Value aus einer Quelle. Jedes Token wird mit allen anderen Token der Sequenz verglichen, um kontextuelle Beziehungen in der Sequenz selbst zu extrahieren (z.B. in dem Satz \"Die Katze jagt die Maus um das Haus.\" könnte für das Wort \"Katze\" das Wort \"jagt\" eine hohe Relevanz haben, weil es die Aktion der Katze beschreibt). \n",
    "- \"Source-Attention\" oder \"Cross-Attention\" hierbei stammt Query aus einer anderen Quelle als Key und Value, z.B. der Decoder verarbeitet seine Ausgabe als Query, um das nächste Token zu berechnen. Dieses Queries werden mit den Keys und Values der Eingabe des Encoders abgeglichen. Die Keys dienen dazu, die Relevanz der Encoder Information für den aktuellen Query zu bestimmen.\n",
    "\n",
    "Desweiteren wird nach Art des Maskings unterschieden. Masking verdeckt immer einen Teil der Daten. In Transformern gibt es folgende Arten von Masking:\n",
    "\n",
    "- \"Subsequent Masking\", verdeckt alle Token nach dem zu prognostizierenden Token im Decoder-Attention-Block, z.B. \"Diese Eingabe ist [...]\" eine maskierte Version von \"Diese Eingabe ist ab hier maskiert.\"\n",
    "- \"Padding Masking\", verdeckt sog. \"Padding Tokens\", das sind Platzhalter Tokens mit der eine Sequenz auf die Länge des Context Windows aufgefüllt wird, um eine einheitliche Länge von Token als Eingabe zu erzeugen. Wenn wir z.B. nur Sätze mit fünf Wörtern erlauben, dann ist \"Ein kurzer Satz. <i>pad</i> <i>pad</i>\" eine erlaubte Version des Satzes \"Ein kurzer Satz.\" und \"Ein kurzer Satz. [...]\" wiederum eine maskierte Variante des erlaubten Satzes.\n",
    "\n",
    "3. Ausgabe\n",
    "\n",
    "Die Ausgabe interpretiert die Daten die das Attention-Modul erzeugt und formt sie in eine für menschlichen Gebrauch nützliche Form um, wie z.B. Textdaten oder Bilddaten. Dafür wird in Transformern oft eine Log-Softmax-Funktion verwendet, die auf die Tensorausgabe des letzten Attention-Blockes angewandt wird, um Wahrscheinlichkeitsverteilungen über mögliche Ausgabewerte, z.B. alle im Transformer kodierten Token bei Texten oder über alle sog. Bildpatches, d.h. Sammlungen von Pixeln, bei Bilddaten zu erzeugen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f86373b1",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"input\"></a>Input\n",
    "\n",
    "Der erste Teil eines Transformermodells besteht aus der Eingabepipeline. Diese verarbeitet die Eingabe, z.B. die Texteingabe eines Nutzers, und bereitet sie auf die Verarbeitung in den Attention-Modulen vor. Die Attention-Module arbeiten über eine Attention-Matrix, die aus der jeweiligen Eingabe eine Ausgabe berechnet. Wir müssen also aus einer Eingabe in Textform eine Vektorrepräsentation erzeugen, die alle notwendigen Informationen für das Modell enthalten, um mithilfe von Matrixmanipulationen nützliche Vorhersagen zu machen.\n",
    "\n",
    "In <a href=\"#fig:fig2\">Abbildung 2</a> sehen sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt. \n",
    "Wie zu erkennen ist, werden in der Eingabepipeline während des Training eines Transformermodells ausschließlich die Weights für das Embedding verändert, alle anderen Funktionen sind rein deterministisch und bleiben damit vom Trainingsprozess unbeeinflusst.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4caafaa5",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig2\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_input_pipeline.jpg\" style=\"height: auto; width: 300px; height: 450px; \" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "    <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4578ca",
   "metadata": {},
   "source": [
    "Prinzipiell besteht die Eingabepipeline aus drei Modulen, die in den folgenden drei Abschnitten genauer erläutert werden:\n",
    "\n",
    "1. Die [Tokenization](#Tokenization), die den Text mithilfe eines Symbolalphabets in eine Zahlenkodierung umwandelt. So wird z.B. \"Transformer\" in die Zahlenfolge \"2 61 4334 93 6622 202 3\" umgewandelt.\n",
    "2. Das [Embedding](#embedding), das diese Kodierung mithilfe eines trainierbaren Algorithmus in eine Vektordarstellung umwandelt. Das Embedding lernt die komplexe Struktur eines Textes so darzustellen, dass sie informativ für die nachfolgenden Module ist. Wie genau diese Umwandlung aussieht ist dabei aufgrund der stochastischen Natur von Deep Learning Modellen nur schlecht logisch nachzuvollziehen. Eine Kodierung könnte z.B. die obige Zahlenfolge \"2 61 4334 93 6622 202 3\" in eine zweidimensinalen Vektor der Form (7, 512) mit Einträgen zwischen -1 und 1 verwandeln.\n",
    "3. Das [Positional Encoding](#positional-encoding) ein mit der Transformer-Architektur eingeführter Mechanismus. Im Gegensatz zu RNNs, die die Eingabedaten sequenziell präsentiert bekommen [19], enthalten bei Transformermodellen die Eingaben keine Information zur relativen Position der Tokens. Diese fehlenden Informationen werden in diesem Schritt manuell hinzugefügt indem <a href=\"#pos-enc-formula\">Sinuskurven</a> mit verschiedener Frequenz und Phase über die Eingabedaten gelegt werden.\n",
    "\n",
    "### <a id=\"Tokenization\"></a>Tokenization\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Um ein Sprachmodell mit Texten zu füttern, muss Sprache zuerst in eine maschinenlesbare Form überführt werden. Dabei geht es nicht darum, Wörter einfach zu erkennen, sondern sie so zu zerlegen, dass sie vom Modell als Zahlenfolge verarbeitet werden können. Diese Umwandlung wird als <a href=\"#Tokenization\">Tokenization</a> bezeichnet. Nehmen wir den Beispielsatz „Die Hauptstadt von Deutschland ist “ – dieser wird nicht als Ganzes, sondern schrittweise in sogenannte Tokens aufgeteilt und anschließend in Zahlen übersetzt. Die Wahl der Methode, mit der das geschieht, beeinflusst Effizienz und Genauigkeit des Modells erheblich.\n",
    "</div>\n",
    "Eine der grundlegendsten Methoden ist die Zeichencodierung, bei der jedem Buchstaben eine eindeutige Zahl zugeordnet wird. Dieses Verfahren ist vollständig – es lassen sich damit beliebige Zeichenkombinationen darstellen – und das Vokabular bleibt klein, da nur Buchstaben und Sonderzeichen erfasst werden müssen. Der Nachteil: Die entstehenden Sequenzen sind sehr lang, da jeder Buchstabe einzeln kodiert wird. Das macht die Verarbeitung ineffizient.\n",
    "\n",
    "Wählt man hingegen ein Vokabular auf Wortebene, werden die Sequenzen deutlich kürzer, da ganze Wörter in einem Schritt kodiert werden. Damit sinkt die Länge der Eingabesequenz, aber ein neues Problem entsteht: Die Methode ist potenziell unvollständig. Es ist nahezu unmöglich, ein Vokabular zu definieren, das alle möglichen Wörter einer Sprache abdeckt. Um das Risiko zu minimieren, müsste das Vokabular enorm groß sein – was wiederum die Modellgröße und den Speicherbedarf stark erhöht.\n",
    "\n",
    "Um beide Probleme – lange Sequenzen und unvollständige Wortabdeckung – auszugleichen, haben sich gemischte Verfahren etabliert, die auf großen Korpora trainiert werden. Diese Ansätze kombinieren Zeichen- und Wortebene und lassen sich in zwei Richtungen unterteilen: Top-Down- und Bottom-Up-Verfahren. Top-Down-Methoden starten mit einem umfangreichen Wort-Vokabular, das aus einem Korpus extrahiert wurde, und erweitern es um sinnvolle Teilworte, wenn unbekannte Wörter auftreten. Bottom-Up-Verfahren hingegen beginnen mit einem kleinen Zeichenvorrat und fügen häufig auftretende Zeichenfolgen als neue Einheiten hinzu.\n",
    "\n",
    "Die <a href=\"#encoder-decoder\">Transformer</a>-Architektur nach [1] setzt auf ein solches Bottom-Up-Verfahren, genauer gesagt auf Byte-Pair Encoding [13]. Dieses Verfahren hat sich als effizient erwiesen: Es erlaubt eine kompakte Repräsentation mit begrenztem Vokabular und gleichzeitig eine relativ kurze Sequenzlänge. So lassen sich Eingaben wie „Das ist ein Testsatz.“ präzise, vollständig und effizient kodieren – ein entscheidender Schritt für jede weitere Verarbeitung im Modell.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a5e1d1",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"byte-pair-encoding\"></a>Byte-Pair Encoding\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Stell dir vor, du willst ein großes Buch so speichern, dass es möglichst wenig Platz braucht – aber du willst trotzdem jeden Satz später genau wieder zusammensetzen können. Das Byte-Pair Encoding (kurz: BPE) hilft genau dabei. Nehmen wir als einfaches Beispiel den Satz: „Was ist die Hauptstadt von Deutschland?“ Dieser Satz wird zunächst in einzelne Buchstaben oder kleine Zeichenfolgen zerlegt. Dann schaut sich das System an, welche Zeichen besonders oft zusammen vorkommen – zum Beispiel vielleicht „st“ oder „de“. Diese Paare werden dann durch neue Symbole ersetzt, um Speicherplatz zu sparen. So entsteht Stück für Stück ein Vokabular aus oft genutzten Teilen. Am Ende können sowohl ganze Wörter als auch häufige Wortbestandteile wie „##land“ oder „##en“ im Vokabular vorkommen. Das System „lernt“ also, welche Kombinationen besonders nützlich sind, um Sprache kompakt und effizient darzustellen.\n",
    "</div>\n",
    "Das Byte-Pair Encoding Verfahren nutzt ein Vokabular mit einer festgelegten Länge. In unserer Implementation des Tokenizer nutzten wir ein Vokabular von der Länge 8000. Das Vokabular wird dabei folgendermaßen erstellt:\n",
    "\n",
    "  1. Ein Text, der für die Erstellung des Vokabulars verwendet wird, wird in eine Sequenz von Buchstaben zerlegt. Wortenden werden mit einem zusätzlichen Symbol kodiert. Z.B. wird \"Ein Satz\" in \"[start]\", \"e\", \"i\", \"n\", \"s\", \"a\", \"t\", \"z\", \"[ende]\" zerlegt.\n",
    "  2. Alle vorhandenen Symbole werden automatisch in das Vokabular aufgenommen.\n",
    "  3. Nun wird das häufigste 2-Gramm, also zwei aufeinander folgende Symbole, gesucht, das im Text zu finden ist.\n",
    "  4. Dieses wird ins Vokabular aufgenommen und im Text durch ein einzelnes Symbol ersetzt.\n",
    "  5. Dieser Prozess wird nun wiederholt bei Schritt 3 (d.h. 2-Gramme aus dem ersten Durchlauf werden zu 4-Grammen im zweiten Durchlauf u.s.w.) bis die vorgegebene Länge des Vokabulars erreicht ist.\n",
    "\n",
    "Dabei können sowohl ganze Wörter ins Vokabular aufgenommen werden, wenn sie denn oft genug auftauchen (bespielweise werden die Worte \"a\", \"the\", \"and\" bei englischen Texte sicherlich mitaufgenommen werden), aber auch einzelne Wortteile wie z.B. \"en##\", \"##ment\" oder \"##ed\" werden in diesem Vokabular sicherlich vorkommen, um seltene Kombinationen wie \"enablement\" in die Wortteile \"en##\", \"able\" und \"##ment\" zerlegen zu können oder grammatikalische Formen wie \"wanted\" zu bilden. Die Zeichenfolge \"##\" beschreibt dabei, dass hier ein anderer Wortteil anschließen muss.\n",
    "\n",
    "In unserem Testbeispiel ist zu sehen, wie Ihre Eingabe in Tokens getrennt und dann in eine Kodierung umgewandelt wird, je nachdem, welche Position das Token in unserem Vokabular hat.\n",
    "Wie Sie sehen, enthält das Byte-Pair Encoding Vokabular auch ein [START]- und [END]-Token für Satzanfang und Satzende, sowie Elemente vom Typ 'abc##' oder '##abc'. Die Elemente mit Doppel-'#' stellen eine Sequenz am Anfang bzw. Ende eines Wortes dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60193c08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.516820100Z",
     "start_time": "2024-01-16T09:57:25.499286900Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_tok = widgets.Text(\n",
    "    value='Die Hauptstadt von Deutschland ist ',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_tok = widgets.Button(description='Tokenizer auf Input anwenden',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_tok = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def tokenize(input_widget_tok):\n",
    "    tokens = tokenizer.tokenize(input_widget_tok.value)                    # Erstellung der Tokens als Index für Vokabular\n",
    "    lookup = tokenizer.lookup(tokens)                                      # Abrufen der Zeichenkette des Index im Vokabular                    \n",
    "    \n",
    "    return tokens, lookup\n",
    "    \n",
    "def on_button_click(b):\n",
    "    with output_widget_tok:\n",
    "        output_widget_tok.clear_output()                                                        \n",
    "        tokens, lookup = tokenize(input_widget_tok)\n",
    "\n",
    "        VisualWrapper.display_text('Tokens die aus der Eingabe mit Byte-Pair Encoding extrahiert werden:'.rjust(100) + ', '.join([token.decode('utf-8').rjust(10) for token in lookup.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "        VisualWrapper.display_text('Ihre Positionsnummer im Alphabet des Byte-Pair Encoding Algorithmus:'.rjust(100) + ', '.join([str(token).rjust(10) for token in tokens.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "\n",
    "button_widget_tok.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_tok, button_widget_tok, output_widget_tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e5b84a",
   "metadata": {},
   "source": [
    "### <a id=\"embedding\"></a>Embedding\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Nach der <a href=\"#Tokenization\">Tokenization</a> des Satzes „Was ist die Hauptstadt von Deutschland?“ liegt dieser als Sequenz von Zahlen vor, deren Länge von der ursprünglichen Texteingabe sowie vom verwendeten Byte-Pair-Encoding abhängt. Da gleichlange Sätze je nach Tokenisierung unterschiedlich viele Tokens erzeugen können, variiert auch die Sequenzlänge. Für das Training von <a href=\"#encoder-decoder\">Transformer</a>-Modellen ist jedoch eine einheitliche Eingabelänge erforderlich, um parallele Verarbeitung zu ermöglichen.\n",
    "</div>\n",
    "Um das zu gewährleisten, werden sogenannte Padding Tokens eingeführt. Diese enthalten keine semantische Information, sondern dienen lediglich dazu, die Sequenz künstlich auf eine vordefinierte Länge zu bringen. Erst anschließend wird die tokenisierte Eingabe in das notwendige Vektorformat überführt: Jedes numerisch kodierte Token wird durch eine trainierbare Gewichtsmatrix in einen Vektor der festen Länge $d_{model}$ eingebettet.\n",
    "\n",
    "Dieser Schritt wird als <a href=\"#embedding\">Embedding</a> bezeichnet. Dabei handelt es sich um eine lernbare Abbildung, die sicherstellt, dass jedes Token unabhängig von seiner ursprünglichen numerischen Form in denselben hochdimensionalen Raum projiziert wird. Der dabei entstehende Vektor bestehend aus $d_{model}$ Elementen enthält keine direkt ablesbare Struktur: Es ist nicht ersichtlich, welche Informationen in welchem Teil des Vektors gespeichert sind. Dennoch lassen sich die einzelnen Dimensionen grob als Träger unterschiedlicher Merkmale interpretieren: etwa semantische Bedeutung, grammatikalische Rolle oder Position im Satz.\n",
    "\n",
    "Das <a href=\"#embedding\">Embedding</a> ist ein zentraler Bestandteil der Modellparameter und wird während des Trainings kontinuierlich angepasst. Es gehört damit zu den nicht-deterministischen Komponenten des Modells. In <a href=\"#fig:embedding\">Abbildung 3</a> ist dargestellt, wie die Zahlenfolge aus der Tokenization durch das trainierbare Embedding in Vektoren umgewandelt wird – ein essenzieller Schritt, um die Eingabe für die nachfolgenden Schichten des Modells nutzbar zu machen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab2ec3f9",
   "metadata": {},
   "source": [
    "<figure id=\"fig:embedding\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_embedding.jpg\" style=\"max-width: 25%; max-height: 150vh; height: auto;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "    <figcaption>Abbildung 3: Gewichte der Eingabepipeline</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b34b4d",
   "metadata": {},
   "source": [
    "Wie ein solches Embedding aussieht und wie es sich verändert, wenn man beispielsweise neue Teile an den Satz anfügt können Sie in der nachfolgenden Simulation ausprobieren. Der Eingabetext wird erst vom Tokenizer in Tokens umgewandelt und dann durch das Embedding in einen Tensor.\n",
    "\n",
    "An jeder Position (vertikal dargestellt) ist dann das Embedding des Tokens an dieser Stelle zu sehen (horizontal dargestellt). Die farbliche Kodierung stellt dabei das Zahlenspektrum dar, indem sich die Vektoreinträge bewegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a1d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.563139100Z",
     "start_time": "2024-01-16T09:57:25.524821600Z"
    }
   },
   "outputs": [],
   "source": [
    "class EmbeddingExample():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "        self.input_widget = widgets.Text(\n",
    "            value = 'Die Hauptstadt von Deutschland ist ',\n",
    "            description = 'Ihre Eingabe:',\n",
    "            continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "            layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "        )\n",
    "\n",
    "        self.button_widget = widgets.Button(description='Einbettung erstellen',\n",
    "                                    layout = widgets.Layout(width='auto'))\n",
    "\n",
    "        self.output_widget = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "        self.old_context = None\n",
    "\n",
    "    def create_tokenized_embeddings(self):\n",
    "        tokens = self.tokenizer.tokenize(self.input_widget.value)                                 # Tokenisierung der Eingabe\n",
    "        tokens_all = tokens[tf.newaxis, :, :]                                                     # Hinzufügen einer weiteren Dimension\n",
    "        input_without_eos = tokens[tf.newaxis, :, :-1]                                            # Auswahl der Tokens bis zum [END] Token\n",
    "        token_input = self.tokenizer.detokenize(tokens_all)                                       # Nur zur Ausgabe Zwecken\n",
    "        string_value = token_input.numpy()[0][0].decode('utf-8')                                  # Nur zur Ausgabe Zwecken\n",
    "        lookup = tokenizer.lookup(input_without_eos)                                              # Nur zur Ausgabe Zwecken\n",
    "        lookup = [item.decode('utf-8') for sublist in lookup.numpy()[0] for item in sublist]      # Nur zur Ausgabe Zwecken\n",
    "        print(\"Wörter: \", string_value)\n",
    "        print(\"Tokens: \", lookup)\n",
    "        context = model.model.enc_embed(input_without_eos)                                        # Erstellung des Kontext Embedding \n",
    "        VisualWrapper.display_text('So sieht die Einbettung der Eingabe aus.')\n",
    "        VisualWrapper.color_bar(context.to_tensor())\n",
    "        if self.old_context is not None:\n",
    "             padded_context, padded_old_context = self.pad_tensors(context, self.old_context)     # Erstellung des Padding Vektors der Eingaben\n",
    "             VisualWrapper.display_text('So unterscheiden sich die alte und die neue Einbettung voneinander.')\n",
    "             context_diff = padded_context - padded_old_context                                   # Berechnung der Unterschiede beider Vektoren\n",
    "             VisualWrapper.color_bar(context_diff)\n",
    "\n",
    "        self.old_context = context\n",
    "\n",
    "    \n",
    "    def on_button_click(self, b):\n",
    "        with self.output_widget:\n",
    "            self.output_widget.clear_output()  # clear the previous output\n",
    "            VisualWrapper.reset_visualiser()\n",
    "            self.create_tokenized_embeddings()\n",
    "    \n",
    "    def pad_tensors(self, ragged_tensor1, ragged_tensor2):\n",
    "        \"\"\"Funktion um die Tensoren der Eingabe auf die gleiche Länge zu transformieren\"\"\"\n",
    "        tensor1 = ragged_tensor1.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "        tensor2 = ragged_tensor2.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "\n",
    "        shape1 = tf.shape(tensor1)\n",
    "        shape2 = tf.shape(tensor2)\n",
    "\n",
    "        target_shape = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            target_shape.append(tf.maximum(shape1[i], shape2[i]))                                # Die maximale Größe der Dimension wird an die Zielform angehängt.\n",
    "\n",
    "        target_shape = tf.stack(target_shape)                                                    # Umwandlung der Zielform in einen Tensor\n",
    "\n",
    "\n",
    "        paddings1 = []\n",
    "        paddings2 = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            paddings1.append([0, target_shape[i] - shape1[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "            paddings2.append([0, target_shape[i] - shape2[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "\n",
    "        paddings1 = tf.stack(paddings1)                                                          # Konvertieren der Paddings in Tensoren\n",
    "        paddings2 = tf.stack(paddings2)                                                          # Konvertieren der Paddings in Tensoren\n",
    "\n",
    "        tensor1_padded = tf.pad(tensor1, paddings1)                                              # Tensoren an die Zielform anpassen\n",
    "        tensor2_padded = tf.pad(tensor2, paddings2)                                              # Tensoren an die Zielform anpassen\n",
    "\n",
    "        return tensor1_padded, tensor2_padded\n",
    "\n",
    "emb_ex = EmbeddingExample()\n",
    "\n",
    "VisualWrapper.display_text('Hier können Sie einen Text einbetten lassen. Wenn du die Eingabe veränderst wird außerdem gezeigt, wie sich die Einbettung geändert hat.')\n",
    "\n",
    "emb_ex.button_widget.on_click(emb_ex.on_button_click)\n",
    "display(emb_ex.input_widget, emb_ex.button_widget, emb_ex.output_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef7ac5-da74-40c0-be78-f0f866ca9fd9",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesen Beispiel ist zu sehen, wie die Werteverteilung eines Embeddings grafisch dargestellt werden kann.\n",
    "\n",
    "*Codeerläuterung:* Das Embedding wird in dem von uns implementierten [Code](https://github.com/LangLoffelLako/TF_simulator_tensorflow/blob/main/interactive_inference.ipynb) der Funktion *create_tokenized_embeddings()* erstellt. Dazu wird zu erst der Eingabetext vom Tokenizer in Tokens unterteilt (Zeile 20). Die Tokens können Sie über der Grafik sehen. In Zeile 29 werden diese dann vom Transformer Modell in die Embeddings umgewandelt. \\n\"\n",
    "\n",
    "<p>Im Vergleich der Sätze „Die Hauptstadt von Deutschland ist“ und „Die Hauptstadt von Frankreich ist“ wird deutlich, dass im französischen Satz drei zusätzliche Token erscheinen.</p>\n",
    "<p>Die Tokenisierung von „Die Hauptstadt von Deutschland ist“ ergibt 15 Tokens: <code>[START] die ha ##up ##ts ##ta ##d ##t von de ##uts ##ch ##land is ##t</code>.</p>\n",
    "<p>Im französischen Satz kommen zusätzlich die Token <span style=\"background-color: yellow;\">#n</span>, <span style=\"background-color: yellow;\">##e</span> und <span style=\"background-color: yellow;\">##u</span> hinzu, wodurch sich das Token-Layout und das Positional Encoding verändern.</p>\n",
    "<p>Ebenso kann man ein Muster im Wertebereich erkennen: Der Bereich der Tiefe zwischen 0 bis 256 bewegt sich hauptsächlich im Wertebereich –1 bis 2, während der Bereich von 257 bis 512 im Wertebereich 0 bis –3 liegt. Dies geht auf die Sinus-Funktion des Positional Encodings zurück, welche auf das Embedding angewendet wird und in diesen Bereichen stark unterschiedliche Werte erzeugt.</p>\n",
    "<p>Die Werte in den ersten 256 Positionen können bestimmte Merkmale oder Eigenschaften der Wörter repräsentieren, während der Bereich der Positionen von 266 bis 512 andere Merkmale oder Eigenschaften widerspiegelt. Diese getrennte Darstellung ermöglicht dem Modell, komplexe Beziehungen und Muster in den Daten zu erfassen.</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e70583",
   "metadata": {},
   "source": [
    "\n",
    "### Positional Encoding <a id=\"positional-encoding\"></a>\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Ein Satz wie „Die Hauptstadt von Deutschland ist Berlin“ enthält nicht nur Informationen über die Wörter selbst, sondern auch über deren Reihenfolge. Der Satz „Berlin ist die Hauptstadt von Deutschland“ hat dieselben Wörter, bedeutet aber etwas leicht anderes, weil die Positionen der Wörter vertauscht wurden. Für ein Sprachmodell ist es daher wichtig zu wissen, an welcher Stelle ein Wort steht. Die eingebetteten Wortvektoren (Embeddings), die ein Transformer-Modell verarbeitet, enthalten jedoch von sich aus keine Angaben zur Wortposition. Um diese Information zu ergänzen, wird das sogenannte Positional Encoding eingesetzt – eine Methode, mit der die Position jedes einzelnen Wortes im Satz mathematisch kodiert wird.\n",
    "</div>\n",
    "Da im Embedding keine Informationen über die relative Position der verschiedenen Worte kodiert werden, muss diese manuell hinzugefügt werden. Hierfür verwendet die Transformerarchitektur für jede Position des Embeddings (also jedes enkodierte Token) eine veränderte Sinuskurve. Es ändern sich die Frequenz, also die Abstände der Nulldurchgänge, sowie die Phase, also die x-Werte der Nulldurchgänge. [1] Der sich ergebende Wert wird dem Embedding an der jeweiligen Stelle hinzugefügt.\n",
    "\n",
    "Dadurch lassen sich die verschiedenen Worte sehr gut voneinander trennen. Die Idee dahinter ist, dass:\n",
    "\n",
    "die grobe Position eines Wortes anhand der langfrequenten Sinuskurven bestimmt werden kann, da sie sich über die gesamte Länge der Eingabe nur allmählich verändern und die Werte des Embeddings insgesamt in eine bestimmte Richtung verschieben. Beispielsweise besitzen die Worte im hinteren Teil der Eingabe größere Werte als die im vorderen Teil der Eingabe. Dies ist in der untenstehenden Simulation an großflächigen Rot- und Grünverschiebungen zu erkennen.\n",
    "die genaue Position durch die hochfrequenten Sinuskurven bestimmt werden kann, da diese sich bereits für benachbarte Vektoren klar unterscheiden. Dadurch wird deutlich, welches Wort an welcher Stelle im Embedding kodiert wurde. Dies entspricht den sehr chaotisch wirkenden Bereichen in der untenstehenden Simulation.\n",
    "Die für diese Verschiebungen verwendeten Formeln sind deterministisch für Position und Tiefe des Embeddings festgelegt und lauten:\n",
    "\n",
    "<a id=\"pos-enc-formula\"></a>\n",
    "\n",
    "$$ PE(\\text{pos}, i) = \\begin{cases} \\sin\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), & \\text{falls } i \\text{ gerade ist} \\\\ \\cos\\left(\\frac{\\text{pos}}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), & \\text{falls } i \\text{ ungerade ist}, \\end{cases} $$\n",
    "\n",
    "wobei\n",
    "\n",
    "$\\text{pos}$ die Position des Tokens in der Sequenz ist,\n",
    "$i$ der Index der Dimension in der Positionsverschlüsselung ist,\n",
    "$d_{\\text{model}}$ die Dimensionalität des Modells ist.\n",
    "\n",
    "In der untenstehenden Simulation ist zu sehen, wie das Positional Encoding beispielhaft für ein 1024 x 257 langes und tiefes Embedding aussieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d875186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:25.823595Z",
     "start_time": "2024-01-16T09:57:25.551970Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact(length=(2,2048,1), depth=(2,512,1))\n",
    "def print_pos_enc(length, depth):\n",
    "    VisualWrapper.color_bar(positional_encoding(length, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb626ffc-d9b6-4181-92d3-1bf3618660b4",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "An diesem Beispiel sieht man den Effekt des Positional Encodings auf verschiedene lange bzw. tiefe Einbettungen. Wenn man mit den zwei Reglern spielt, sieht man, verschieden große Zooms auf den Effekt. Erhöht man den \"length\"-Regler sieht man die Auswirkungen, welche die Länge des Satzes betreffen. Die Werte die durch das Positional Encoding zu dem Vektor hinzugefügt werden sind dabei jedoch immer dieselben. Zum Bespiel wird an Stelle 100 immer derselbe Wert hinzugefügt, egal ob die maximale Länge des Satzes (\"length\") sich verändert. Dasselbe gilt auch für die Tiefe jedes einzelnen Vektors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389fa0d1",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"trainingsmethoden\"></a>Trainingsmethoden\n",
    "\n",
    "### <a id=\"dropout\"></a>Dropout\n",
    "\n",
    "Dropout [11] ist eine Methode, die während des Trainingsprozesses eines neuronalen Netzes genutzt wird, um zu verhindern, dass die gelernte Ausgabe eines Modells sich zu sehr auf einen einzelnen Prädikator stützt.\n",
    "Dafür wird zwischen zwei Schritten desselben Modells, eine Dropout-Layer eingefügt. Diese setzt zufällig einige der vom ersten Modellteil generierten Ausgabe auf einen vordefinierten Wert (meistens -inf), um den nachfolgenden Schichten diese Information vorzuenthalten. Da diese Operation zufällig erfolgt, müssen die nachfolgenden Teile des Modells lernen ihre Ausgabe auch ohne diese Information zu erstellen. Somit lernt das Model seine Vorhersage auf eine möglichst breite Kombination an Merkmalen aufzubauen und man verhindert, dass Vorhersagen nur aufgrund eines einzigen Merkmals der vorherigen Ausgabe gemacht werden.\n",
    "\n",
    "Ein gutes Beispiel ist das Ende eines Satzes vorherzusagen. In europäischen Sprachen wird ein Satz fast immer mit einem Punkt beendet, also ist es eine gute Strategie zu lernen, dass ein Satz durch einen Punkt beendet wird. Doch ein Modell, dass einen Punkt als einziges Merkmal eines Satzendes nutzt ist wenig robust. Wenn man an falscher Stelle einen Punkt setzt oder ihn an einem Satzende durch ein anderes Zeichen ersetzt werden die Vorhersagen des Models schlecht sein. Dabei gibt es auch andere Hinweise auf ein Satzende, z.B. das Vorkommen eines Verbs in der deutschen Sprache oder von Ort und Zeitangaben im Englischen.\n",
    "\n",
    "Um dem Modell keine Informationen vorzuenthalten, wenn es tatsächlich eingesetzt wird, ist das Dropout immer nur während des Trainings aktiv und wird danach abgeschalten, sodass während der Inferenzphase keine Informationen gelöscht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b358029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.618429300Z",
     "start_time": "2024-01-16T09:57:25.825653400Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_drop = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_drop = widgets.Text(value = 'Was ist die Hauptstadt von Deutschland?',\n",
    "                                 description = 'Ihre Eingabe:',\n",
    "                                 continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                                 layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "                                 )\n",
    "\n",
    "length_widget_drop = widgets.IntSlider(value=30,\n",
    "                           min=2,\n",
    "                           max=2048,\n",
    "                           description='Länge des Tensors:',\n",
    "                           continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                           )\n",
    "depth_widget_drop = widgets.IntSlider(value=512,\n",
    "                          min=2,\n",
    "                          max=512,\n",
    "                          description='Tiefe des Tensors:',\n",
    "                          continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                          )\n",
    "dropout_widget = widgets.FloatSlider(value=0.1,\n",
    "                              min=0,\n",
    "                              max=0.9,\n",
    "                              step=0.1,\n",
    "                              description='Dropoutrate:',\n",
    "                              continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "def dropout_function(length, depth, dropout, input):\n",
    "\n",
    "    # Erstellung der Dropout Layer\n",
    "    dropout_layer = layers.Dropout(dropout)                                                         # Anwendung des Dropout auf die Layer\n",
    "    one_tensor = tf.ones([length, depth])                                                           # Erstellung eines Arrays aus 1-en\n",
    "    dropout_tensor = dropout_layer(one_tensor, training=True)                                       # Anwendung des Dropout auf die Layer\n",
    "\n",
    "    # Erstellung der Kontext Layer\n",
    "    tokens = tokenizer_drop.tokenize(input)                                                         # Tokenisierung des Inputs\n",
    "    input_without_eos = tokens[tf.newaxis, :, 1:-1]                                                 # Auswahl der Tokens bis zum [END] Token\n",
    "    context = model.model.enc_embed(input_without_eos)                                              # Erstellung des Embedding durch das Modell\n",
    "    context_drop = dropout_layer(context, training=True)                                            # Anwendung des Dropout auf das Embedding\n",
    "\n",
    "    return dropout_tensor, context_drop, context\n",
    "\n",
    "def out(length, depth, dropout, input):\n",
    "    VisualWrapper.reset_visualiser()                                                   \n",
    "    dropout_tensor, context_drop, context = dropout_function(length, depth, dropout, input)\n",
    "    VisualWrapper.color_bar(dropout_tensor)\n",
    "                               \n",
    "    VisualWrapper.color_bar(context.to_tensor())                     \n",
    "    VisualWrapper.color_bar(context_drop.to_tensor())\n",
    "    \n",
    "\n",
    "output_widget_dropout = widgets.interactive_output(out,\n",
    "                                                   {'length': length_widget_drop, 'depth': depth_widget_drop, 'dropout': dropout_widget, 'input': input_widget_drop}, \n",
    "                                                   )\n",
    "\n",
    "display(length_widget_drop, depth_widget_drop, dropout_widget, input_widget_drop, output_widget_dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d176e24-7e39-4e92-95d5-3d24eef076ec",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesem Beispiel wird der Effekt von Dropout auf die Layer und das Embedding dargestellt.\n",
    "\n",
    "*Codeerläuterung:* Dafür werden im von uns implementierten [Code](https://github.com/LangLoffelLako/TF_simulator_tensorflow/blob/main/interactive_inference.ipynb) in der Funktion *dropout_function()* jeweils Dropout auf die Layer und auf das Embedding angewendet. Dafür wird in Zeile 36 und 42 jeweils die jeweilige Transformation auf den beiden Objekten angewendet.\n",
    "\n",
    "Damit wird also ein gewisser Teil, welche mit dem Parameter \"Dropoutrate\" bestimmt wird, der Werte Layer bzw. des Embeddings den weiteren Verarbeitungsschritten vorenthalten. Dieser Parameter ist ein prozentualer Wert, d.h. bei einem Wert von 0.2 werden 20% der Werte vorenthalten. \n",
    "Für das Beispiel können dieses Mal die Länge des Tensors (Eingabe) und die Tiefe des Tensors bestimmt werden. Ebenso kann die Dropoutrate verändert werden.\n",
    "\n",
    "In der ersten Grafik sieht man welche Werte in einem uniformen Vektor vom Dropout verändert werden. In den beiden darauffolgenden Grafiken wird das Dropout auf den im Textfeld eingegebenen Beispieltext angewandt, nachdem er durch den Tokenizer und ein Embedding in Vektorform gebracht wurde. Die erste Grafik zeigt den vollständigen Vektor und die zweite Grafik den Vektor, der vom Dropout verändert wurde.\n",
    "Hier sieht man die ausgelassenen Positionen sehr gut. Mit dem Erhöhen der Dropout Rate, werden diese mehr. Außerdem kann man erkennen, dass sich, wenn man den Dropout erhöht, der Wertebereich ebenfalls ausweitet. Dies geschieht, da in der Tensorflow-Implementation die durch das Dropout unveränderten Werte mit $1 / (1-\\text{Dropoutrate})$ skaliert werden, um die Summe aller Werte konstant zu halten.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2d169f9",
   "metadata": {},
   "source": [
    "### <a id=\"normalization\"></a>Normalisierung\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Fragt man nach der Hauptstadt von Deutschland, erwartet man immer dieselbe Antwort: Berlin. Würde sich diese Antwort ständig ändern, wäre kein zuverlässiges Lernen möglich. Genauso ist es in neuronalen Netzen: Wenn sich die Verteilung der Eingabewerte in jeder Schicht ständig verändert, wird das Training instabil. Genau das verhindert Normalisierung – sie sorgt dafür, dass jede Schicht konsistente Eingaben erhält und damit besser lernen kann.\n",
    "</div>\n",
    "\n",
    "Normalisierung ist eine Technik, die von [20] eingeführt wurde. In Deep Neural Networks, die mit nicht-linearen Aktivierungsfunktionen wie der Sigmoid-Funktion\n",
    "\n",
    "$$g(x) = \\frac{1}{1 + \\exp(-x)}$$\n",
    "\n",
    "trainiert werden gilt, dass $g'(x) \\rightarrow 0$ für $|x| \\rightarrow \\infty$. \n",
    "\n",
    "Das führt dazu, dass diese Modelle in einen Bereich geraten können, in dem $g'(x)$ sehr klein wird. Dadurch wird auch das Training durch Stochastic Gradient Descent (SGD) minimal, sodass das Training des Modells stagniert. Man spricht vom Vanishing Gradient Problem.\n",
    "\n",
    "In neuronalen Netzen ist hierbei das Problem, dass die tieferen Layer des Modells, z.B. eine Layer $z = g(Wx + b)$ mit der Sigmoid-Funktion g versucht mithilfe seiner trainierbaren Werte $W$ und $b$ den Output des gesamten vorherigen Netzes zu gewichten. Dabei werden sowohl $W$ und $b$ abhängig von vorherigen Werten $x$ trainiert und hängen somit selbst auch von $x$ ab. Da sich während des Trainings alle Layers des Netzes fortwährend aktualisieren, ändert sich auch der Input $x$ fortwährend, sodass ein Training späterer Schichten erst möglich ist, wenn die vorhergehenden sich weitgehend stabilisiert haben. \n",
    "Dieser Effekt wird von [20] Internal Covariate Shift genannt. \n",
    "\n",
    "Je tiefer das neuronale Netz, umso größer sind diese Veränderungen, da es mehr Schichten gibt, die sich verändern können. Die Tiefe einer neuronalen Netzes erhöht also die Wahrscheinlichkeit das ein Vanishing Gradient Problem auftritt und ein effektives Training frühzeitig aufhört.\n",
    "\n",
    "Transformer wie sie in [1] beschrieben sind nutzen um diesem Problem entgegenzuwirken Layer Normalization, einen Normalisierungsalgorithmus den [21] entwickelt hat. Eine Normalisierung führt dazu, dass zumindest der Wertebereich indem sich der Input $x$ aufhält während des gesamten Trainings stabil bleibt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9d2b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.709361500Z",
     "start_time": "2024-01-16T09:57:26.620436400Z"
    }
   },
   "outputs": [],
   "source": [
    "VisualWrapper.reset_visualiser()\n",
    "tokenizer_norm = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input=\"Die Hauptstadt von Deutschland ist \"\n",
    "\n",
    "tensor_input = tf.convert_to_tensor(input)\n",
    "if len(tensor_input.shape) == 0:\n",
    "    tensor_input = tensor_input[tf.newaxis]\n",
    "\n",
    "tokenized_input = tokenizer_norm.tokenize(tensor_input).to_tensor()                             # Anwendung eines Tokenizers mit Normalisierung\n",
    "input_without_eos = tokenized_input[:, :-1]\n",
    "context = model.model.encode(input_without_eos, None)\n",
    "\n",
    "VisualWrapper.visualize_data(id='layer')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57791aa1",
   "metadata": {},
   "source": [
    "### <a id=\"residual-connection\"></a> Residual Connection\n",
    "\n",
    "Die Idee für das Nutzen von Residual Connections kommt von [22]. Die Autoren stellten fest, dass bei neuronalen Netzen sowohl die Genauigkeit während des Trainings als auch die Genauigkeit auf dem Testdatensatz mit zunehmender Tiefe schlechter wird.\n",
    "\n",
    "Da durch Normalization bereits sichergestellt ist, dass das Vanishing Gradient Problem nicht auftritt, scheitert die Optimierung der neuronalen Netze aus anderen Gründen.\n",
    "Einer der Gründe hierfür liegt vermutlich darin, dass die tieferen Schichten eines Modells zu Beginn des Trainings sehr viel stärker zur Ausgabe beitragen, als die vorhergehenden Schichten. Sie werden somit zuerst trainiert. Die weniger tiefen Schichten werden erst ausreichend trainiert, wenn in den tiefen Schichten keine Optimierung mehr möglich ist. \n",
    "\n",
    "Um dafür zu sorgen, dass direkt mit Beginn des Trainings alle Teile des Modells gleichmäßig trainiert werden bieten sich Residual Connections an.\n",
    "Sie ersetzen eine Schicht F(x) durch\n",
    "\n",
    "$$H(x) = F(x) + x.$$\n",
    "\n",
    "In das Ergebnis von H(x) geht also sowohl der Output, als auch der Input von F direkt mit ein. Wendet man dieses Prinzip auf die Schichten tiefer neuronaler Netze an, sorgt das dafür, dass gleich zu Beginn der Output der wenig tiefen Schichten relevant in den Output des gesamten Netzes einfließt, denn es gilt für das gesamte Netz $N$:\n",
    "\n",
    "$$N(x) = H_n(H_{n-1}(x)) + H_{n-1}(x) = H_n(H_{n-1}(x)) + H_{n-1}(H_{n-2}(x)) + … + H_2(H_1(x)) + H_1(x)$$\n",
    "\n",
    "Wie man in <a href=\"#fig:fig4\">Abbildung 4</a> sehen kann, haben alle Attention-Module sowie alle Feed Forward Layer in einem Transformermodell eine residuale Verbindung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO:\n",
    "# Hier fehlt noch die Implementierung der Residual Connection als Simulation.\n",
    "# In der nachfolgenden Simulation können Sie sehen, wie sich die Ausgabe einer neuronalen Schicht verändert, wenn man ihr eine Residual Connection beifügt. Der Effekt auf den gesamten Trainingsprozess lässt sich dabei natürlich nur schwer darstellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf4718db1a7166",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <a id=\"layers\"></a> Layers\n",
    "\n",
    "Der größte Teil der Verarbeitung findet in den Transformer Layer statt. Diese werden in <a href=\"fig:fig4\">Abbildung 4</a> detailliert dargestellt. Innerhalb der Transformer kann man die Attention Layer, einige dazwischen liegende Schritte und zuletzt eine Feed Forward Layer unterscheiden.\n",
    "\n",
    "#### Erklärung der Grafik\n",
    "\n",
    "In unserer Grafik werden alle Elemente in Datenelemente (blau) z.B. der Source und Target Input, deterministische Prozesse (schwarz) z.B. die Matrix Multiplikation verschiedener Matrizen, Prozesse mit trainierbaren Parametern (gelb) z.B. die Normailsierung des Output und Prozesse mit Hyperpararmetern (grau), z.B. Dropout unterschieden.\n",
    "Generell durchlaufen die Daten dabei unsere Grafik entlang der Pfeile von den beiden Input-Optionen am unteren Ende, die übrigens auch äquivalent sein können (siehe gestrichelter Pfeil mit Gleichheitszeichen), zum Output am oberen Ende der Grafik.\n",
    "Gestrichelte Pfeile sind dabei nur in manchen der <a href=\"attention-mechanismen\">verschiedenen Attention-Mechanismen</a> vorhanden, wie weiter unten erläutert wird (siehe Link). So ist zum Beispiel das Masking optional.\n",
    "Zu sehen ist in der Grafik, wie der Source und Target Input parallel verarbeitet als Query, Key und Value verarbeitet wird. Query und Key gemeinsam werden dann gegebenenfalls mit einer Mask versehen, bevor sie wieder mit dem Value zusammengeführt werden. Die Ergebnisse der n verschiedenen Attention-Köpfen (n ist dabei ein Hyperparameter des Modells) werden verkettet und zu einem Vektorembedding zusammengeführt. Dieses wird mit dem residualen Target Embedding addiert und so als Eingabe in die Feed Forward Layer gegeben. Diese berechnet den Ouput der Transformer Layer und als Kombination aus Output und residualem Input wird diese ausgegeben.\n",
    "\n",
    "Wie diese Mechanismen im Detail funktionieren wird im folgenden Kapitel geklärt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e0f5d",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig4\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_layer_architecture.jpg\" style=\"width: 400px; height: 375px; height: auto; margin: auto;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "    <figcaption>\n",
    "      Abbildung 4: Transformerarchitektur <br>\n",
    "      In dieser Abbildung wird der Aufbau einer Attention Layer gezeigt. Dabei werden die verschiedenen Varianten (Self-, Cross-, Masked-Attention) parallel dargestellt (siehe gpunktierte Linien als Alternativen). Die beiden Input Embeddings werden von mehreren Attention-Köpfen parallel verarbeitet, um dann gemeinsam mit dem Target Embedding addiert von einer Feed Forward Layer zum Output Embedding der Attention Layer transformiert zu werden.\n",
    "    </figcaption>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c08fb1",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"attention\"></a> Attention\n",
    "<div style=\"background-color: #e0e0e0; padding: 1em; border-radius: 8px;\">\n",
    "Stell dir vor, du bekommst die Aussage: \"Die Hauptstadt von Deutschland ist \" Damit du die richtige Antwort – Berlin – geben kannst, musst du in deinem Gedächtnis nach der passenden Information suchen. Du konzentrierst dich also auf genau den Teil deines Wissens, der zur Frage passt. In ähnlicher Weise funktioniert der sogenannte Attention-Mechanismus in modernen Sprachmodellen wie dem Transformer. Er sorgt dafür, dass das Modell sich beim Verarbeiten eines Textes gezielt auf die wichtigen Wörter konzentriert – je nachdem, welche Information gerade gebraucht wird. So wie du dich bei einer Frage auf das relevante Wissen fokussierst, fokussiert sich der Transformer auf relevante Textstellen, um eine gute Übersetzung oder Antwort zu erzeugen. Dieser Mechanismus ersetzt bei den Transformern die bisher genutzten rekursiven Netzwerke vollständig.\n",
    "</div>\n",
    "Die Neuerung von Transformern im Vergleich zu vorangegangenen Lösungen für Neural Machine Translation (NMT) ist es, allein auf Attention als Mechanismus für das Verarbeiten von Sprache zu setzen. Attention wurde auch vorher schon von [15] zur Verbesserung von RNNs zur Übersetzung von Texten verwendet.\n",
    "\n",
    "Der Attention-Mechanismus, wie ihn [1] beschreiben, orientiert sich dabei an der Idee einer Suchanfrage des Ausgabetextes and sich selbst, bzw. den Eingabetext. Die Attention-Layer bekommt dabei zwei oder eigentlich drei Eingaben: \n",
    "\n",
    "1. den Query ($Q$), \n",
    "2. den Key ($K$),\n",
    "3. den Value ($V$).\n",
    "\n",
    "In der Praxis erhalten aber Key und Value in Transformern immer dieselbe Eingabe und häufig sind Query, Key und Value sogar alle identisch. Aus welcher Quelle Query, Key und Value kommen unterscheidet verschiedene Formen von Attention. So nennen wir Self-Attention denjenigen Fall indem $Q=K=V$ gilt und Cross-Attention denjenigen Fall indem der Query aus der Ausgabe des Encoder besteht und Key und Value beide aus der Ausgabe eines Decoder-Blocks stammen (siehe <a href=\"fig:fig4\">Abbildung 4</a>).\n",
    "\n",
    "Um zu erklären, wie Attention funktioniert, sollten wir aber zunächst davon ausgehen, dass Query-, Key- und Value-Eingabe verschieden sind. Ich schreibe bewusst von der Eingabe, da in jeder Attention-Layer zunächst eine Eingabe $Q'$, $K'$, $V'$ mit Hilfe von gewichteten Matrix $W^Q$, $W^K$ und $W^V$ in Query, Key und Value \n",
    "\n",
    "$$Q=Q′ \\times W^Q$$\n",
    "$$K=K′ \\times W^K$$\n",
    "$$V=V′ \\times W^V$$ \n",
    "\n",
    "umgewandelt werden. Die gewichteten Matrizen $W^Q$, $W^K$, $W^V$ sind die trainierbaren Weights der Attention-Layer. Alle nachfolgenden Prozesse sind deterministischer Natur. Das bedeutet, dass diese Matrizen festlegen, welches Ergebnis die Attention-Layer liefert.\n",
    "Eine gute grafisch aufbereitete Erklärung dessen, was hier beschrieben wird findet sich übrigens bei [9].\n",
    "\n",
    "Attention unterscheidet sich deutlich von vorherigen Verfahren. Am ehsten ist es vergleichbar mit der oftmals bei der Programmierung verwendeten Datenstruktur \"Dictionary\". Ein Dictionary ist eine unsortierte Liste mit \"Key-Value\" Paaren. Hierbei lässt sich ein Wert, Objekt, Variable (Value) in der Liste speichern, welcher wiederum über den Key abgefragt werden kann. Folgende Grafik zeigt die Gemeinsamkeiten und Unterschiede zu dem Attention-Mechanismus.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da880e3c13929c09",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<figure id=\"fig:fig_attention\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_attention.png\" style=\"max-width: 100%; max-height: 150vh; height: auto;\" alt=\"Abbildung 4: Vergleich von Attention mit Datenstruktur 'Dictionary'.\"/>\n",
    "    <figcaption>Abbildung 5: Vergleich von Attention mit einem klassischen Dictionary</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47d379a5e3fcd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Erklärung der Grafik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f91d9b-05cc-4af9-a709-a2d3dbcf785a",
   "metadata": {},
   "source": [
    "\n",
    "In beiden dargestellten Szenarien, \"Dictionary\" und \"Attention\", wird das Konzept von Key-Value-Paaren genutzt, wobei die Keys als Referenzindizes fungieren und die Values die eigentlichen zu verarbeitenden Informationen enthalten. Queries dienen in beiden Fällen dazu, relevante Daten aus diesen Paaren zu selektieren, und am Ende wird jeweils ein Output generiert, der aus den Informationen der Value-Komponenten resultiert.\n",
    "\n",
    "Jedoch gibt es markante Unterschiede zwischen den beiden Ansätzen. Im Dictionary findet eine direkte Zuordnung statt, bei der ein Query-Element einem Key zugeordnet und das zugehörige Value direkt als Output übernommen wird. Bei \"Attention\" wird hingegen eine gewichtete Kombination der Values vorgenommen, die von der Relevanz der Keys, bestimmt durch die Queries, abhängt. Dies spiegelt sich auch in der Art der Beziehungen wider: Während im \"Dictionary\" eine eindeutige 1:1-Beziehung herrscht, besteht im \"Attention\"-Mechanismus eine 1:n-Beziehung, bei der ein Query mehrere Keys beeinflusst. Dementsprechend ist die Ausgabe im \"Dictionary\" statisch und hängt ausschließlich von der direkten Übereinstimmung ab, während sie im \"Attention\"-Modell dynamisch ist und durch die berechneten Gewichtungen eine nuanciertere Informationszusammenstellung ermöglicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f0686",
   "metadata": {},
   "source": [
    "#### <a id=\"vorteile\"></a>Vorteile von Transformern\n",
    "\n",
    "Die Einführung des Attention-Mechanismus in Transformer-Architekturen hat zu bedeutenden Verbesserungen in der Verarbeitung natürlicher Sprache geführt, insbesondere im Vergleich zu traditionellen rekurrenten neuronalen Netzwerken (RNNs). Einer der herausragenden Vorteile des Attention-Mechanismus ist seine Fähigkeit zur parallelen Verarbeitung von Daten. Im Gegensatz zu den sequenziellen Verarbeitungsgrenzen von RNNs ermöglicht diese Eigenschaft eine wesentlich effizientere Datenverarbeitung. Während für eine sequenzielle Verarbeitung die Komplexität von der Textlänge $n$ exponentiell abhängt $\\exp(n)$ gilt für die parallele Verarbeitung wie in Transformern nur eine lineare Abhängigkeit $a \\times n$. Das ist eine dramtische Verbesserung.\n",
    "\n",
    "Ein weiterer entscheidender Fortschritt, den der Attention-Mechanismus mit sich bringt, ist dass er bei der Verarbeitung der Daten an Position $n$ uneingeschränkt auf alle vorherige $n-1$ Daten zugreifen kann. Im Unterschied zu dem festen, oft begrenzten Gedächtnis der RNNs, das sich Daten von Position $1$ bis zur verarbeitung an Position $n$ bereits $n-1 \\text{-mal}$ merken musste, erlaubt der dynamische und kontextabhängige Speicher des Attention-Mechanismus eine umfassendere und flexiblere Berücksichtigung von Informationen. Dies ist besonders nützlich für das Verständnis und die Verarbeitung komplexer Sprachstrukturen.\n",
    "\n",
    "Besonders bemerkenswert ist auch, wie der Attention-Mechanismus die Handhabung von Langzeitabhängigkeiten verbessert. Durch die Fähigkeit, direkte Verbindungen zwischen weit auseinanderliegenden Elementen einer Sequenz herzustellen, können Transformer-Modelle effektiver mit Langzeitabhängigkeiten umgehen, was bei RNNs oft eine Herausforderung darstellt. Diese Fähigkeit verbessert das Verständnis und die Generierung von Sprache über längere Textabschnitte hinweg erheblich.\n",
    "\n",
    "Schließlich ermöglicht der Attention-Mechanismus eine verbesserte Kontextverarbeitung. Die Fähigkeit, die Bedeutung von Wörtern und Phrasen im Kontext ihres Auftretens zu erfassen, führt zu einem präziseren und tieferen Verständnis der Sprache. Diese kontextuelle Bewusstheit, die über die Fähigkeiten traditioneller RNNs hinausgeht, ist entscheidend für anspruchsvolle sprachverarbeitende Aufgaben wie z.B. Übersetzung oder Zusammenfassungen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa40a6df",
   "metadata": {},
   "source": [
    "#### <a id=\"attention-function\"></a> Attention als Funktion\n",
    "\n",
    "Die Funktion, die die Attention für uns berechnet, bekommt die Eingaben $Q$, $K$, $V$, also Query, Key und Value, die aus der Multiplikation der Eingabe mit den Gewichtsmatrizen entstanden sind. Sie lautet:\n",
    "\n",
    "$$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{\\text{model}}}}\\right)V$$\n",
    "\n",
    "Es wird also zuerst das Kreuzprodukt aus $Q$ und $K$ gebildet. Dieses Produkt wird mit $\\sqrt{d_{\\text{model}}}$ skaliert (den Grund dafür findest du im folgenden [Kapitel](#skalierung-mit-sqrd_k)), und auf dieses Ergebnis wird dann die Softmax-Funktion $\\sigma(x)$ angewandt.\n",
    "Diese Funktion lässt sich am besten positionsweise beschreiben:\n",
    "\n",
    "$$\\sigma(x)_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^n \\exp(x_j)} \\text{ für } i=1, \\dots, n.$$\n",
    "\n",
    "Nennen wir also\n",
    "\n",
    "$$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{model}}}\\right) = \\text{Score}(Q,K),$$\n",
    "\n",
    "dann ist\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{Score}(Q,K) \\times V$$\n",
    "\n",
    "eine Funktion, die $V$ mit einem Vektor multipliziert, wobei für den Vektor gilt $|\\text{Score}(Q,K)| = 1$.\n",
    "\n",
    "$\\text{Score}(Q,K)$ ist also ein Vektor exakt der Länge von $V$, der angibt, mit welchem Anteil jeder Eintrag von $V$ in den Ausgabevektor $\\text{Attention}(Q,K,V)$ eingehen soll. Dabei summiert sich $\\text{Score}(Q,K)$ zu $1$, es handelt sich also tatsächlich um eine Gewichtung der Einträge von $V$.\n",
    "\n",
    "Da mit $\\text{Score}(Q,K)$ nun also eine Gewichtung besteht, wie stark die Ausgabe $\\text{Attention}(Q,K,V)_i$ von $V_j$ abhängt, kann man dieses Verhältnis als Matrix grafisch darstellen. Dies geschieht in der Simulation unten, bei der für den vorgegebenen Satz eine Gewichtung aus einer der Attention-Layer dargestellt wird. Der Eintrag in der $i$-ten Zeile und $j$-ten Spalte gibt dabei den Einfluss von $V_j$ auf $\\text{Attention}(Q,K,V)_i$ an.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffd9c1",
   "metadata": {},
   "source": [
    "##### <a id=\"skalierung-mit-sqrd_k\"></a> Skalierung mit $\\sqrt{d_{model}}$\n",
    "\n",
    "Zuletzt sollte noch kurz erklärt werden, weshalb innerhalb der Funktion $\\text{Attention}$ mit $\\sqrt{d_{model}}$ skaliert wird. Das Problem des Vanishing Gradients kann auch innerhalb der $\\text{Attention}$-Funktion auftreten, da hier $\\text{softmax}(QK^T)V$ berechnet wird und das Skalarprodukt $QK^T$ mit $d_{model}$ skaliert. Somit gilt, dass die Softmax-Funktion, definiert durch\n",
    "    \n",
    "$$\\sigma(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^N \\exp(z_j)} \\text{ für } j = 1, \\dots, N$$\n",
    "\n",
    "leicht in einen saturierten Bereich mit extrem kleinen Gradienten gerät. Deshalb wird in der Umsetzung der Architektur $QK^T$ mit $sqr(d_{model})$ skaliert: \n",
    "\n",
    "$$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{model}}}\\right),$$\n",
    "\n",
    "und so die Skalierung der Attention mit $d_{model}$ minimiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afaf5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.743517300Z",
     "start_time": "2024-01-16T09:57:26.711359600Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_attn = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "attn_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_attn = widgets.Text(\n",
    "    value='Die Hauptstadt von Deutschland ist ',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_attn = widgets.Button(description='Embedding berechnen',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_attn = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def create_tokenized_embeddings():\n",
    "        tensor_input = tf.convert_to_tensor(input_widget_attn.value)                # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)    # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "        if len(tensor_input.shape) == 0:                                            # Überprüft, ob der Eingabetensor im korrekten Format ist                                     \n",
    "            tensor_input = tensor_input[tf.newaxis]                                 # Falls nicht, wird eine Dimension hinzufügt \n",
    "\n",
    "    \n",
    "        tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()              # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "        input_without_eos = tokenized_input[:, :-1]\n",
    "        context = transformer.encode(input_without_eos, None)                       # Erstellung der Kontext-Vektoren vom Transformer-Modell\n",
    "\n",
    "        # Write the input tokens (excluding the last one) to the output array\n",
    "        for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "            output_array = output_array.write(i, value)\n",
    "\n",
    "        dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "        dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_attn:\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 6\n",
    "        #output_widget_attn.clear_output()  # clear the previous output\n",
    "        create_tokenized_embeddings()\n",
    "        VisualWrapper.visualize_data(id='attention')\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 1\n",
    "            \n",
    "\n",
    "button_widget_attn.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_attn, button_widget_attn, output_widget_attn)\n",
    "\n",
    "# TODO: In diesem Codeblock müssen noch einige Anpassungen am Text geschehen.\n",
    "# TODO: Die Aufmerksamkeitsmatrizen sind momentan 2x2 Matrizen. Hier gibt es einen Bug."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dca50aa-8c8c-49e4-955b-3c0791913f5d",
   "metadata": {},
   "source": [
    "#### <a id=\"multi-headed-attention\"></a> Multi-headed Attention\n",
    "\n",
    "Multi-headed Attention ist eine Erweiterung des Attention-Mechanismus, die darauf abzielt, die komplexen Strukturen von Texten besser zu erfassen. In herkömmlichen Modellen konkurrieren zahlreiche Beziehungen und Verbindungen innerhalb eines Textes um die Aufmerksamkeit eines einzigen Mechanismus, was zu einer Überlastung führen kann. Durch die Einführung von Multi-headed Attention wird diese Einschränkung überwunden, indem mehrere, parallel arbeitende Attention-Ströme geschaffen werden, von denen sich jeder auf unterschiedliche Aspekte des Textes konzentriert.\n",
    "\n",
    "Diese spezialisierten \"Köpfe\" können verschiedene Typen von Zusammenhängen innerhalb der Eingabedaten verarbeiten. Ein Kopf könnte sich auf die Beziehung zwischen Subjekten und Prädikaten konzentrieren, ein anderer auf die Kohärenz thematischer Elemente, und ein dritter könnte die Verbindung zwischen Haupt- und Nebensätzen analysieren. Ob das so passiert kann natürlich nicht nachgewiesen werden. Analysiert man allerdings die Aktivierungsmatrix der verschiedenen Köpfe, so kann man klare Unterschiede feststellen, sodass eine Spezialisierung anzunehmen ist. Durch diese Aufteilung wird vermieden, dass die Köpfe in Konkurrenz zueinander treten; stattdessen ergänzen sie sich, was zu einer umfassenderen Analyse führt.\n",
    "\n",
    "Die resultierenden, von jedem Kopf generierten Outputs werden anschließend zu einer einzigen, zusammenhängenden Darstellung kombiniert. Diese Synthese bietet eine reichhaltige, vielschichtige Perspektive auf die Eingabedaten, die weit über das hinausgeht, was mit einem einzigen Attention-Mechanismus möglich wäre. Multi-headed Attention ist somit ein Schlüsselelement, das die Fähigkeit von Modellen verbessert, subtile und komplexe Muster in Daten zu erkennen und darauf zu reagieren.\\\n",
    "\n",
    "Mathematisch betrachtet werden dazu zu Beginn $h$ gewichtete Matrizen \n",
    "\n",
    "$$W_i^Q, W_i^K, W_i^V \\quad i = 1, \\ldots, h$$ \n",
    "\n",
    "eingeführt. Diese erzeugen also $h$ verschiedene Matrixtripel \n",
    "\n",
    "$$Q_i = Q W_i^Q, \\quad K_i = K W_i^K, \\quad V_i = V W_i^V \\quad i = 1, \\ldots, h$$\n",
    "\n",
    "und somit ergeben sich $h$ verschiedene Ausgaben \n",
    "\n",
    "$$H_i = \\text{Attention}(Q_i, K_i, V_i) \\quad i = 1, \\ldots, h.$$\n",
    "\n",
    "Diese werden nun zu einer einzigen Ausgabe zusammengeführt, indem wir \n",
    "\n",
    "$$\\text{MultiHeadAttention}(Q,K,V) = \\text{Concat}(H_1, \\ldots, H_h) W^O$$\n",
    "\n",
    "berechnen. Dabei ist $\\text{Concat}(A_1, \\ldots, A_n)$ das Hintereinanderschreiben mehrerer Matrizen und $W^O$ eine weitere trainierbare Matrix, die die verschiedenen Ausgaben $H_1, \\ldots, H_h$ gewichtet. Deshalb sehen wir in der obigen Ausgabe auch mehrere Matrizen, die $\\text{Score}(Q_i, K_i)$ darstellen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb503571-d491-4561-9e54-f245685c66fc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<figure id=\"fig:fig_mhattention\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_mhattention.png\" style=\"max-width: 100%; max-height: 150vh; height: auto;\" alt=\"Abbildung x: Beispiel Multi-headed Attention.\"/>\n",
    "    <figcaption>Abbildung 6: Beispiel Multi-headed Attention</figcaption>\n",
    "  </div>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96638e79-6c3f-4bdb-8ecd-34a293a64358",
   "metadata": {},
   "source": [
    "### Erklärung der Grafik\n",
    "\n",
    "Die Abbildung zeigt zwei Beispiele für die Visualisierung von Multi-headed Attention in einem Transformer-Modell. Jedes Diagramm repräsentiert die Aufmerksamkeitsverteilung eines eigenen \"Kopfes\" innerhalb des Attention-Mechanismus, und zwar für einen gegebenen Satz \"Ich besuchte den Kurs Digital Leadership und lernte\".\n",
    "In beiden Diagrammen sind die vertikalen Balken proportional zur Stärke der Aufmerksamkeit, die jedes Wort vom jeweiligen Kopf erhält. Ein höherer Balken bedeutet, dass das entsprechende Wort eine stärkere Aufmerksamkeit erhält, wenn das Modell versucht, die Bedeutung des Satzes zu interpretieren oder eine Aufgabe wie die Übersetzung durchzuführen.\n",
    "\n",
    "Die Wörter am unteren Rand jedes Diagramms sind die Eingabewörter, und die kleinen \"v\" und \"k\" Symbole repräsentieren die Values und Keys im Attention-Mechanismus. Das \"q\" steht für den Query-Vektor, der in diesem Fall auf das Wort \"lernte\" zeigt, was bedeutet, dass die Visualisierung die Aufmerksamkeit aus der Perspektive dieses spezifischen Wortes darstellt.\n",
    "\n",
    "Attention-Kopf 1 fokussiert sich auf die Entitäten. Hier sehen wir, dass dieser Kopf vor allem auf die Wörter \"Kurs\", \"Digital\", und \"Leadership\" Aufmerksamkeit legt. Diese Wörter sind als Entitäten (Namen von Personen, Orten oder spezifischen Objekten) identifiziert worden, was darauf hindeutet, dass dieser Kopf darauf trainiert ist, solche Entitäten im Text zu erkennen und hervorzuheben.\n",
    "Rechts: Attention-Kopf 2 fokussiert sich auf die syntaktisch relevanten Wörter\n",
    "\n",
    "Der zweite Kopf scheint die Aufmerksamkeit auf die Wörter \"Ich\", \"besuchte\" aber auch das query Wort \"lernte\" selbst zu richten. Die beiden letzten Wörter sind Verben und \"Ich\" ist das zugehörige Pronomen. Dieser Kopf ist somit auf die Identifizierung syntaktischer Strukturen ausgerichtet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebaa9f-8183-4a31-96e1-e4dbe5813a06",
   "metadata": {},
   "source": [
    "### <a id=\"masking\"></a> Masking\n",
    "\n",
    "Das Maskieren des Inputs ist eine wichtige  Komponente der Transformerarchitektur. Beim Masking handelt es sich in Wirklichkeit um zwei Mechanismen, die zwar dieselbe Funktionsweise besitzen, aber sehr unterschiedliche Aufgaben in der Architektur übernehmen. Einerseits das Subsequent Masking, andererseits das Padding Masking. Das Padding Masking stellt jediglich sicher, dass nur Positionen mit Inhalt vom Transformer verarbeitet werden, während das Subsequent Masking dafür sorgt, dass der Decoder des Transformers autoregressiv ist. Das bedeutet, bei einer Vorhersage für eine Ausgabe an der Position $i$, soll das Modell nur Informationen aus den Eingabepositionen $1,\\ldots,i-1$ nutzen. Das Zusammenspiel der Beiden sehen Sie in der folgenden Abbildung (<a href=\"#fig:fig_masking\">Abbildung 7</a>)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e465d9",
   "metadata": {},
   "source": [
    "#### <a id=\"padding-masking\"></a> Padding Masking\n",
    "\n",
    "Das Padding Masking ist notwendig, da Transformer sequenzielle Daten so verarbeiten, als ob sie eine fixe Länge $d_{model}$ hätten. Das geschieht deshalb, weil Transformer so lernen können jede Position der Ausgabelänge $d_{model}$ parallel vorherzusagen. \n",
    "\n",
    "Um während des Trainings auch Daten mit einer Länge größer oder kleiner $d_{model}$ zu nutzen, werden längere Sequenzen abgeschnitten und kürzere mit Nullen aufgefüllt. Diese Nullen müssen dann mit Hilfe von Padding Masking für das Training irrelevant gemacht werden. Das geschieht indem man alle Positionen die eine Null enthalten für das Modell auf $-\\infty$ setzt, sodass sie beim Gradient Descent nicht berücksichtigt werden.\n",
    "\n",
    "#### <a id=\"subsequent-masking\"></a> Subsequent Masking\n",
    "\n",
    "Das Subsequent Masking benutzt die gleich Technik des Werte auf $-\\infty$ setzen. Dabei wird aber nicht das mit irrelevanten Informationen angefüllte Ende des Eingabevektors auf $-\\infty$ gesetzt, sondern es werden diejenige Werte von $Score(Q, V)$ auf $-\\infty$ gesetzt, die einen Wert $V_j$, für die Ausgabe $i$ $Attention(Q,K,V)_i$ gewichten würden, obwohl $j>i$ gilt. Also es wird beschränkt, dass $Score(Q, V)$ nur diejenigen Werte von $V$ einbeziehen darf, die bei der Vorhersage für den $i$-ten Wert schon bekannt sind.\n",
    "\n",
    "Dass das relevant ist liegt daran, dass beim Training von Transformern der komplette Input gleichzeitig verwertet wird. Ein Satz wird als Ganzes vom Transformer verarbeitet und er erzeugt eine Vorhersage, welches Token an einer bestimmten Position in diesem Satz vorkommt. Dabei erhält er in der Eingabe aber schon die Information, welches Token an dieser Position tatsächlich steht, da er den kompletten Satz als Eingabe bekommen hat. Dies muss nun also innerhalb des Modells mit einer Maskierung wieder rückgängig gemacht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9065dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wir sollten überlegen in einer zukünftigen Version eine Simulation zur Darstellung des Masking einzufügen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e3202-2841-4260-991f-e3919582cffa",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig_masking\" style=\"text-align: center;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "    <img src=\"./img/tf_masking.jpg\" style=\"width: 400px; height: 350px;\" alt=\"Abbildung 7: Zwei Varianten des Maskings im Transformer Modell.\"/>\n",
    "    <figcaption>Abbildung 7: Die zwei Varianten des Maskings im Transformer Modell</figcaption>\n",
    "  </div>\n",
    "</figure>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a0ea31",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"attention-mechanismen\"></a> Verschiedene Attention-Mechanismen\n",
    "\n",
    "In der Architektur werden verschiedene Attentionstypen unterschieden. Es gibt dabei zwei Variablen die beeinflussen, welche Art von Attention wir verwenden. Die erste Variable ist, woher die Eingaben $Q', K'$ und $V'$ kommen. Die zweite Variable ist die Maskierung, die wir auf $\\text{Score}(Q,K)$ anwenden.\n",
    "\n",
    "\n",
    "#### <a id=\"self-attention\"></a> Self-Attention\n",
    "Wir sprechen von Self-Attention, wenn gilt \n",
    "\n",
    "$$Q'=K'=V'.$$ \n",
    "\n",
    "Wenn sich $\\text{Score}(Q,K)$ also bildlich gesprochen daraus ergibt, welche Attention jede Position der Eingabe auf eine andere Position derselben Eingabe legt und diese Attention auf die Eingabe selbst angewandt wird.\n",
    "\n",
    "#### <a id=\"cross-attention\"></a> Cross-Attention\n",
    "\n",
    "Wie sprechen von Cross-Attention, wenn gilt \n",
    "\n",
    "$$K' = V'$$\n",
    "\n",
    "aber $Q'$ von diesen beiden Werten verschieden ist.\n",
    "Wenn sich $\\text{Score}(Q,K)$ also daraus ergibt, welche Attention jede Position einer Eingabe $Q'$ auf die Positionen einer zweiten Eingabe $K'$ gibt und dieser Attentionsscore auf die zweite Eingabe angewandt wird.\n",
    "\n",
    "Dies ist zum Beispiel in Transformern der Fall, wenn $Q'$ sich aus der Ausgabe des Encoder ergibt und $K' = V'$ ein Zwischenergebnis des Decoders ist.\n",
    "\n",
    "#### <a id=\"masked-attention\"></a> Masked Attention\n",
    "\n",
    "Ein Fall von Masked-Attention liegt dann vor, wenn bestimmte Werte von $\\text{Score}(Q,K)$ maskiert werden. Das ist zum Beispiel beim Subsequent Masking der Fall, hier wird $\\text{Score}(Q,K)_{i,j} = -\\infty$ gesetzt für alle Einträge $j>i$. Dadurch wird verhindert, dass die Ausgabe $\\text{Attention}(Q,K,V)_i$ sich auf die Werte $V_j, j>i $ stützt. Zum Beispiel wird während des Trainings im Decoder dadurch verhindert, dass das Model lernt Informationen aus den zukünftigen Einträgen $V_j, j>i$ zu benutzen, um $V_i$ vorherzusagen. Man sieht das gut in der Darstellung von $\\text{Score}(Q,K)$ in unserer <a href=\"#simulation_attention\">Simulation der Attention-Matrizen</a>. Dort liegen die Werte für $j>i$ meistens nahe bei $0$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77e5aaf9",
   "metadata": {},
   "source": [
    "## <a id=\"simulation\"></a> Vollständige Simulation\n",
    "\n",
    "Zuletzt findet sich hier nun noch eine Simulation des kompletten Inferenzvorgangs innerhalb eines Transformermodells. Diese Simulation zeigt alle der vorher genannten Schritte in einem Prozess und liefert eine tatsächliche Vorhersage für den hier eingegebene Text.\n",
    "Da unser Modell im Vergleich zu großen in der Wirtschaft eingesetzen Modellen nur mit sehr wenig Traninigsdaten und -zeit trainiert wurde, ist seine Vorhersageleistung sehr beschränkt und es wird keinen vernünftigen Text liefern. Die Mechanismen die dabei implementiert wurdens, sind allerdings identisch zu denen sehr großer Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cf257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-16T09:57:26.762518500Z",
     "start_time": "2024-01-16T09:57:26.729423800Z"
    }
   },
   "outputs": [],
   "source": [
    "inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_inf = widgets.Text(\n",
    "    value='Was ist die Hauptstadt von Deutschland?',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  \n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_inf = widgets.Button(description='Run interactive inference',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_inf = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_inf:\n",
    "        output_widget_inf.clear_output()  \n",
    "        inference_model(input_widget_inf.value, interactive=True) \n",
    "        inference_model.print_results(visualisation=True)\n",
    "\n",
    "button_widget_inf.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_inf, button_widget_inf, output_widget_inf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82dc7ff0",
   "metadata": {},
   "source": [
    "# <a id=\"bibliographie\"></a> Bibliographie\n",
    "[1] Vaswani, A. et al. Attention Is All You Need. Preprint at http://arxiv.org/abs/1706.03762 (2017).\n",
    "\n",
    "[2] Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Computation 9, 1735–1780 (1997).\n",
    "\n",
    "[3] Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks 5, 157–166 (1994).\n",
    "\n",
    "[4] Cho, K., van Merrienboer, B., Bahdanau, D. & Bengio, Y. On the Properties of Neural Machine Translation: Encoder–Decoder Approaches. in Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation 103–111 (Association for Computational Linguistics, 2014). doi:10.3115/v1/W14-4012.\n",
    "\n",
    "[6] Kaplan, J. et al. Scaling Laws for Neural Language Models. Preprint at http://arxiv.org/abs/2001.08361 (2020).\n",
    "\n",
    "[5] Goodfellow, I., Bengio, Y. & Courville, A. Deep learning. (The MIT Press, 2016).\n",
    "\n",
    "[7] Radford, A. et al. Language Models are Unsupervised Multitask Learners. (2019).\n",
    "\n",
    "[8] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Preprint at http://arxiv.org/abs/1810.04805 (2019).\n",
    "\n",
    "[9] Alammar, J. The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. https://jalammar.github.io/illustrated-transformer/ (2018).\n",
    "\n",
    "[10] Encoder-Decoder. Understanding The Model Architecture | by Naoki | Medium. https://naokishibuya.medium.com/transformers-encoder-decoder-434603d19e1.\n",
    "\n",
    "[11] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. (2014).\n",
    "\n",
    "[12] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[13] Gage, P. A New Algorithm for Data Compression. (1994).\n",
    "\n",
    "[14] Cho, K. et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Preprint at http://arxiv.org/abs/1406.1078 (2014).\n",
    "\n",
    "[15] Bahdanau, D., Cho, K. & Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate. Preprint at http://arxiv.org/abs/1409.0473 (2016).\n",
    "\n",
    "[16] OpenAI. GPT-4 Technical Report. Preprint at http://arxiv.org/abs/2303.08774 (2023).\n",
    "\n",
    "[17] Grefenstette, G. & Tapanainen, P. What is a word, What is a sentence? Problems of Tokenization.\n",
    "\n",
    "[18] Lin, Z. et al. A Structured Self-attentive Sentence Embedding. Preprint at http://arxiv.org/abs/1703.03130 (2017).\n",
    "\n",
    "[19] Schmidt, R. M. Recurrent Neural Networks (RNNs): A gentle Introduction and Overview. Preprint at http://arxiv.org/abs/1912.05911 (2019).\n",
    "\n",
    "[20] Ioffe, S. & Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Preprint at http://arxiv.org/abs/1502.03167 (2015).\n",
    "\n",
    "[21] Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer Normalization. Preprint at http://arxiv.org/abs/1607.06450 (2016).\n",
    "\n",
    "[22] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[23] Tunstall, L., von Werra, L., Wolf, T. Natural Language Processing Mit Transformern. https://www.oreilly.com/library/view/natural-language-processing/9781098157081/.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "clean_voila_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
