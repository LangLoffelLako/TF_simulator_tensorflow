{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ab5bac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:48.405636500Z",
     "start_time": "2023-11-13T16:26:43.882608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logging und Decorators\n",
    "import logging as log\n",
    "\n",
    "# Tensorflow Module\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Visualisierung und Eingabe\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact_manual, interactive, interact, VBox, HTML\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Backend Module\n",
    "from interactive_inference_backend import ModelLoader, StoryTokenizer, WordComplete, VisualWrapper, positional_encoding\n",
    "from interactive_inference_backend import reserved_tokens, vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cff4da4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:49.992823800Z",
     "start_time": "2023-11-13T16:26:48.405636500Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ModelLoader(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                            d_model=512,\n",
    "                            n_stacks=2,\n",
    "                            h_att=4,\n",
    "                            load_model=True,\n",
    "                            model_load_path=\"model_N2_h4_d512_t20230831-134344\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94fa9c-ba0a-4b73-92ed-2f9d2965a7ac",
   "metadata": {},
   "source": [
    "# Interaktive Erklärung der Transformerarchitektur\n",
    "\n",
    "## Inhaltsverzeichnis\n",
    "- [Einleitung](#einleitung)\n",
    "    - [Kurzübersicht](#kurzübersicht-transformer)\n",
    "    - [Ziel des Artikels](#ziel-dieses-interaktiven-artikels)\n",
    "    - [Architekturübersicht](#architekturübersicht)\n",
    "    - [Encoder-Decoder Architektur](#encoder-decoder)\n",
    "    - [Architekturblöcke](#architekturblöcke)\n",
    "- [Input](#input)\n",
    "    - [Tokenisation](#tokenisation)\n",
    "    - [Byte-Pair Encoding](#byte-pair-encoding)\n",
    "    - [Embedding](#embedding)\n",
    "    - [Positional Encoding](#positional-encoding)\n",
    "- [Trainingsmethoden](#trainingsmethoden)\n",
    "    - [Dropout](#dropout)\n",
    "    - [Normalization](#normalization)\n",
    "    - [Residual Connection](#residual-connection)\n",
    "- [Layers](#layers)\n",
    "    - [Attention](#attention)\n",
    "        - [Attention-Function](#attention-function)\n",
    "        - [Multi-Headed Attention](#multi-headed-attention)\n",
    "    - [Masking](#masking)\n",
    "        - [Padding Masking](#padding-masking)\n",
    "        - [Subsequent Masking](#subsequent-masking)\n",
    "    - [Attention-Mechanismen](#verschiedene-attention-mechanismen)\n",
    "        - [Self-Attention](#self-attention)\n",
    "        - [Cross-Attention](#cross-attention)\n",
    "        - [Masked Attention](#masked-attention)\n",
    "    - [Skalierung mit sqr(d_k)](#skalierung-mit-sqrd_k)\n",
    "- [Simulation](#vollständige-simulation)\n",
    "- [Bibliographie](#bibliography)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b9c9b",
   "metadata": {},
   "source": [
    "## Einleitung\n",
    "### Kurzübersicht Transformer\n",
    "\n",
    "Transformer-Modelle sind eine von Vaswani et al. [1] vorgeschlagene Architektur für das Modellieren von sequentiellen Daten. Im Gegensatz zu den vorher genutzten Architekturen wie Recurrent Neural Networks (RNN) [2, 3] oder Convolutional Neural Networks (CNN) [4] ermöglichen Transformer allerdings das parallele Verarbeiten der sequentiellen Daten während des Trainings und ermöglichen dadurch mit erheblich reduzierter Trainingszeit und -rechenkapazität sequentielle Daten zu verarbeiten.\n",
    "\n",
    "Um diese parallele Verarbeitung zu erreichen nutzen Transformermodelle Attention-Blöcke. Attention-Blöcke sind trainierbare Matrizen, die von einem Inputvektor auf einen Outputvektor projizieren, lso eine sequentielle Datenstruktur in Vektorform in eine ebenfalls sequentielle Datenstruktur in Vektorform verarbeiten, indem eine Matrixmultiplikation auf dein Eingabevektor angewandt wird. Wie das funktioniert wird im Kapitel [Attention](#attention) gezeigt.\n",
    "Diese Matrizen projizieren also im Idealfall von jedem Eintrag im Eingabevektor genau diejenige Information auf einen Eintrag im Ausgabevektor, die an der jeweiligen Stelle die Ausgabe beeinflussen soll. Da hierbei die Ausgabe i nicht von der Ausgabe i-1 abhängt, können wir parallel die komplette Ausgabe zu jeder Eingabe erzeugen und mit einem Gradient Descent Verfahren [5] unser Modell trainieren.\n",
    "Da wir während der Inference, also dem Erstellen von Vorhersagen mit einem trainierten Modell, aus bekannten Informationen neue Daten vorhersagen, benutzen Transformer Architekturen während des Trainings eine Maske. Diese filtert nach der Anwendung der Matrixmultiplikation alle Informationen, die nachfolgende Daten liefern würden heraus. Dadurch wird während des Trainings nur die Verbindung zwischen der aktuellen Position und den an dieser Position bekannten Daten gelernt.\n",
    "Durch wiederholen von Attention-Blöcken und einem nachfolgenden Feed-Forward Network entsteht so eine Architektur zur Verarbeitung sequenzieller Daten, die verglichen mit vorherigen Architekturen weniger Rechenkapazität benötigt und trotzdem eine wesentlich reduzierte Trainigszeit aufweist. [1, 6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0012634",
   "metadata": {},
   "source": [
    "\n",
    "### Ziel dieses interaktiven Artikels\n",
    "\n",
    "Ziel dieses interaktiven Artikels soll es sein, die Architektur die [1] beschreiben in ihren einzelnen Komponenten darzustellen. Der Fokus liegt hierbei auf den Prozessen, die während der Verarbeitung sequentieller Daten stattfinden. Diese werden grafisch durch interaktive Anwendungen dargestellt. Das Ziel ist, dass dem Nutzer der Einfluss unterschiedlicher Architekturbausteine auf verschiedene Eingaben anschaulich klar wird. Ziel ist es auch, die verschiedenen Bausteine so zu erklären, dass einem möglichen Anwender die Implementation einer Tranformer-Architektur, durch das Verständnis der einzelnen Architekturbausteine, erleichtert wird.\n",
    "\n",
    "Der Artikel soll dazu dienen eine Implementierung ohne einschlägiges Vorwissen, z.B. im Kontext von kleinen und mittelständischen Unternehmen (KMU), die sich bisher noch nicht mit der Modellierung sequentieller Daten beschäftigt haben, zu ermöglichen.\n",
    "Wissenschaftliche Litaertur wie [1] und viele der auf ihrer Architektur aufbauenden wissenschaftlichen Arbeiten [7, 8] sowie Erklärartikel oder -videos [9, 10] beschränken sich, wenn überhaupt, darauf den Attention-Mechanismus ausführlich darzustellen. Dabei werden die Designentscheidungen für trainingsrelevante Elemente wie Dropout [11], Residual Connections [12] sowie die Beschreibung bereits etablierter Methoden wie Byte-Pair Encoding [13], Embedding oder Log-Softmax [5] vernachlässigt. Diese Elemente sollen hier aber ebenfalls dargestellt werden.\n",
    "\n",
    "### Architekturübersicht\n",
    "\n",
    "In <a href=\"#fig:\">Abbildung 1</a> ist eine vollständige Darstellung aller Architekturelemente zu finden, die Teil der Transformerarchitektur sind. Dabei unterscheiden wir vier verschiedene Elemente. Prozesse und Methoden werden in Schwarz dargestellt. Die in diesen Prozessen generierte Daten werden in Blau dargestellt. Trainierbare Parameter werden in Gelb dargestellt. Hyperparameter, also festzulegende Eingabeparameter für das Modell werde in Grau dargestellt. Ebenfalls in Grau werden Eingabe- und Ausgabedaten dargestellt.\n",
    "Unsere Abbildung ist insofern komplett, als sie jeglichen Weg zeigen, den Daten durch das Modell nehmen können. Eine Darstellung, in der Form eines Encode-Decoder Network [14], wie sie [1] nutzen findet sich in Abbildung 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4d914d5",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig1\" style=\"text-align: center; height: 700px;\">\n",
    "  <img src=\"./img/tf_arch_full.jpg\" width=\"1000\" height=\"1200\" style=\"margin:auto;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>\n",
    "  Abbildung 1: Transformerarchitektur\n",
    "  Dieses Bild zeigt die gesamte Architektur eines Transformermodells. Dabei wurde vor allem Wert darauf gelegt darzustellen, welchen Weg Informationen durch ein Transformermodell nehmen können, sowie darauf die Funktionen darzustellen, die Veränderungen an diesen Informationen vornehmen. Dabei unterscheiden wir vier verschiedene Elemente. Prozesse und Methoden werden in Schwarz dargestellt. Die in diesen Prozessen generierte Daten werden in Blau dargestellt. Trainierbare Parameter werden in Gelb dargestellt. Hyperparameter, also festzulegende Eingabeparameter für das Modell werde in Grau dargestellt. Ebenfalls in Grau werden Eingabe- und Ausgabedaten dargestellt.\n",
    "  </figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a6b568b",
   "metadata": {},
   "source": [
    "\n",
    "#### Encoder-Decoder\n",
    "\n",
    "Eine Encoder-Decoder Architektur besteht aus zwei voneinander getrennten Neural Networks dem Encoder und dem Decoder, die aber zusammen trainiert werden. Diese Architektur wurde von [14] vorgeschlagen und dann vor allem im Kontext von Neural Machine Translation (NMT) verwendet. Der Vorteil von Encoder-Decoder liegt darin, dass man sowohl den Encoder als auch den Decoder ersetzen kann. Im Idealfall könnte man im Bereich von NMT pro Sprache einen Encoder und einen Decoder trainieren, um dann beliebige Übersetzungen zwischen allen Sprachen zu generieren. Da auch [15], die zuerst Attentionsmechanismen für RNNs vorgeschlagen haben, eine Encoder-Decoder Architektur verwenden, verwundert es nicht, dass auch [1] eine solche Architektur wählten. Insbesondere da [1] Transformer ebenfalls für NMT benutzen.\n",
    "Transformer zur Textvervollständigung können auch ausschließlich auf Decodern beruhen [16]. In unserem Beispiel wird der Encoder zum enkodieren der Eingabe verwendet, um dann im Decoders die Eingabe als Informationsquelle für die Ausgabe zu verwenden. In Abbildung 2 sieht man wie Encoder und Decoder zusammenarbeiten. Der Decoder benutzt Self-Attention, um seinen Input zu verarbeiten, und dann Cross-Attention, für den nächsten Schritt, um final erneut Self-Attention zu benutzen, wohingegen der Encoder ausschließlich über Self-Attention funktioniert. Näheres siehe im Kapitel [Attention](#attention).\n",
    "\n",
    "#### Architekturvarianten und verwirklichte Modelle\n",
    "\n",
    "Transformer können auch als reine Encoder oder reine Decoder-Architektur verwendet werden. Wie beschrieben war die Encoder-Decoder Architektur vor allem für NMT gedacht. In der Umsetzung haben sich allerdings reine Encoder oder Decoder durchgesetzt. Dies liegt vermutlich an der einfacheren Architekturaufbau. Zudem hat sich herausgestellt, dass diese ausreichen, um eine sehr flexible Datenverarbeitung zu ermöglichen und somit auch komplexe Aufgaben wie Übersetzungen zu ermöglichen. In der folgenden Tabelle [23] ist nun also eine Auflistung der bekanntesten Modelle nach Architekturtyp.\n",
    "\n",
    "Encoder             | Encoder-Decoder   | Decoder\n",
    "--------            | --------          | --------\n",
    "BERT, DistillBERT   | T5                | GPT\n",
    "RoBERTa             | BART              | GPT-2\n",
    "XLM, XLM-R          | M2M-100           | GPT-3\n",
    "ALBERT              | BugBird           | GPT-4\n",
    "ELECTRA             |                   | GPT-Neo, GPT-J\n",
    "DeBERTa             |                   | "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "398698e4",
   "metadata": {},
   "source": [
    "Hier können Sie nun einen Beispielsatz zuerst vom Encoder kodieren lassen, um ihn dann im nächsten Schritt vom Decoder dekodieren zu lassen und damit eine Ausgabe zu erhalten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4798807b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:50.025040100Z",
     "start_time": "2023-11-13T16:26:50.003142500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edf5e8f704d47338e33c179a69bd837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Encoder-Decoder Test', continuous_update=False, description='Ihre Eingabe:', layout…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "transformer = model.model\n",
    "\n",
    "input_widget_enc_dec = widgets.Text(\n",
    "    value='Encoder-Decoder Test',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_enc = widgets.Button(description='Enkodiere die Eingabe',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "button_widget_dec = widgets.Button(description='Dekodiere die Input',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_enc = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "output_widget_dec = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def encode():\n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    if len(tensor_input.shape) == 0:                                           # Überprüft, ob der Eingabetensor im korrekten Format ist\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # Falls nicht, wird eine Dimension hinzufügt \n",
    "    \n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')\n",
    "    \n",
    "    context = transformer.encode(input_without_eos, None)                      # Kodierung des Inputsatzes von (Transformer-Modell)\n",
    "    return context, string_value, lookup\n",
    "\n",
    "\n",
    "def decode(): \n",
    "    tensor_input = tf.convert_to_tensor(input_widget_enc_dec.value)            # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)   # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "    if len(tensor_input.shape) == 0:                                           # wie bei der Encodierung\n",
    "      tensor_input = tensor_input[tf.newaxis]                                  # wie bei der Encodierung\n",
    "\n",
    "    tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()             # wie bei der Encodierung\n",
    "    input_without_eos = tokenized_input[:, :-1]\n",
    "    \n",
    "    token_input = tokenizer.detokenize(input_without_eos)\n",
    "    string_value = token_input.numpy()[0].decode('utf-8')                      \n",
    "    context = transformer.encode(input_without_eos, None)                     \n",
    "    lookup = tokenizer.lookup(input_without_eos)\n",
    "    lookup = [item.decode('utf-8') for sublist in lookup.numpy() for item in sublist]\n",
    "\n",
    "                              \n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):                        # Schleife durch jedes Token des Satzes\n",
    "      output_array = output_array.write(i, value)                              # Speichern des Tokens im Output array\n",
    "\n",
    "    dec_input = output_array.concat()[tf.newaxis]                              # Output Array wird zu einem einzigen Tensor konkateniert \n",
    "                                                                               # und anschließend um eine zusätzliche Dimension erweitert\n",
    "\n",
    "    dec_out = transformer.decode(context, None, dec_input, None)               # Decoder des Transformer-Modells wird verwendet, um den dec_input-Tensor \n",
    "                                                                               # unter Verwendung des zuvor berechneten Kontexts zu decodieren.\n",
    "\n",
    "    return dec_out, string_value, lookup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def on_button_click_enc(b):\n",
    "  with output_widget_enc:\n",
    "    output_widget_enc.clear_output()  # clear the previous output\n",
    "    context, tokens, lookup = encode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(context) \n",
    "\n",
    "\n",
    "def on_button_click_dec(b):\n",
    "  with output_widget_dec:\n",
    "    output_widget_dec.clear_output()\n",
    "    dec_out, tokens, lookup = decode()\n",
    "    print('Wörter: ', tokens)\n",
    "    print('Tokens: ', lookup)\n",
    "    print('\\n')\n",
    "    #VisualWrapper.display_text('Beispieltext')\n",
    "    VisualWrapper.color_bar(dec_out)\n",
    "\n",
    "\n",
    "\n",
    "button_widget_enc.on_click(on_button_click_enc)\n",
    "button_widget_dec.on_click(on_button_click_dec)\n",
    "\n",
    "widgets.VBox([input_widget_enc_dec, widgets.HBox([widgets.VBox([button_widget_enc, output_widget_enc]), widgets.VBox([button_widget_dec, output_widget_dec])])])\n",
    "#display(input_widget_enc_dec, button_widget_enc, output_widget_enc, button_widget_dec, output_widget_dec)\n",
    "\n",
    "\n",
    "#print('tok_out', tokenized_input)\n",
    "#print('enc_out', context)\n",
    "#print(\"dec_out\", dec_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa505ea8-2a43-43f6-936e-0d45b4c96f75",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "Über der Grafik sind die verarbeiteten Tokens zu sehen. In der Grafik wird die Position der Tokens wird durch die y-Achse angezeigt und entspricht der Reihenfolge der Tokens im Beispielsatz.  Dabei werden Leerzeichen weggelassen. Das Model erkennt unterschiedliche Wörter mit Hilfe der \"##\" Zeichen. Durch diese Zeichen sieht das Modell, dass ein Token mit \"##\" Zeichenkette zu dem vorherigen Token gehört wie im Beispiel bei dem Wort \"Endcoder\", welches aus den Tokens 'e', '##n', '##co', '##der' besteht. Weitere Informationen in Abschnitt [Byte-Pair Encoding](#byte-pair-encoding).\n",
    "\n",
    "Schaut man sich die Wörter vor der Tokenisierung an, sieht man das alle Wörter nun kleingeschrieben werden. Somit werden groß- und kleingeschriebene Formen des Wortes zusammengefasst. Dadurch wird die Anzahl des Vokabulars reduziert. Linguistische und grammatische Informationen, die aufgrund der Zusammenführung verloren zu gehen scheinen, werden durch die Position und den Kontext des Wortes im Satz erhalten (z.B. Substantivierungen). Ebenso wird das Modell dazu verleitet, mehr Acht auf die Position des Wortes im Satz zu geben.  Der Tokenizer fügt außerdem ein Start- und Endtoken hinzu. Das Endtoken wird im Modell nicht weiterverwendet, während das Starttoken die Position 0 in der weiteren Verarbeitung und Visualisierung einnimmt.\n",
    "\n",
    "Die x-Achse repräsentiert die Tiefe der Token bzw. die Position der einzelnen Werte des Vektors, der die Tokens darstellt. In diesem Fall beträgt die Tiefe 512, was bedeutet, dass die Wörter durch 512 verschiedene Werte dargestellt werden. Diese Vektoren sind jedoch nicht zufällig, sondern weisen untereinander Abhängigkeiten auf. Das heißt, ähnliche Wörter haben z.B. ähnliche Vektoren. Darüber hinaus sind in diesen Vektoren auch die Abhängigkeiten der Wörter im Satz enthalten, was als sogenannt [Positional Encoding](#positional-encoding) bezeichnet wird.  \n",
    ".  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "028a3317",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Architekturblöcke\n",
    "\n",
    "Prinzipiell lassen sich Transformer in mehrere Blöcke einteilen.\n",
    "\n",
    "1. [Eingabeblock](#input)\n",
    "\n",
    "Die Eingabepipeline verarbeitet die Eingabedaten in eine Form die für die Matrixtransformation im Attention-Block genutzt werden kann. Diese unterscheidet sich für verschiedene Datentypen. In unserem Beispiel nutzen wir Textdaten, die wir durch Tokenization [17] und ein Embedding [18] in Tensoren verwandeln.\n",
    "\n",
    "2. [Attention-Block](#attention)\n",
    "\n",
    "Der Attention-Block verarbeitet Daten in Tensorform und liefert somit eine Abbildung von den Eingabetensoren auf die Ausgabetensoren, die jeweils von den Eingabe- und Ausgabeblöcken interpretiert wird.\n",
    "Man kann verschiedene Modulvarianten innerhalb eines Attention-Blocks unterscheiden. Einzelne Attention-Module erhalten als Eingabedaten immer drei Tensoren. Diese werden Query, Key und Value genannt. Aus diesen berechnet ein Attention-Modul eine Ausgabe.\n",
    "Typischerweise werden Attention-Module dadurch unterschieden aus welcher Quelle Query, Key und Value stammen.\n",
    "Es gibt \n",
    "\n",
    "- Self-Attention bei der Query, Key und Value alle aus einer Quelle stammen und\n",
    "- Source-Attention (Hier ist der Query aus einer anderen Quelle als Key und Value, z.B. wenn der Query die Ausgabe eines Encoders ist, während Target die Ausgabe eines Decoderblocks ist).\n",
    "\n",
    "Desweiteren kann man nach Art des Maskings unterscheiden. Masking wird verwendet, um das Attention-Modul daran zu hindern aus einem Teil der Ausgabedaten zu lernen. In Transformern gibt es folgende Arten von Masking:\n",
    "\n",
    "- Subsequent Masking, das den Decoder-Attention-Block daran hindert für die Vorhersage der Position i Daten aus den Positionen i+j zu nutzen, und\n",
    "- Padding Masking, dass Fehler verhindert, die entstehen, da die Eingabe eines Transformers, unabhängig vom Inhalt, immer die selbe Anzahl an Tokens besitzt.\n",
    "\n",
    "3. Ausgabeblock\n",
    "\n",
    "Die Ausgabepipeline interpretiert die Ausgaben des Attention-Blocks, sodass sie in eine für menschlichen Gebrauch nützlichen Form vorliegen (typischerweise z.B. Textdaten, Bilddaten, etc.). Dafür wird in Transfomern eine Log-Softmax Funktion genutzt, die auf die Tensorausgabe des letzten Attention-Blockes angewandt wird.\n",
    "\n",
    "In <a href=\"#fig:fig1\">Abbildung 1</a> entspricht der Attention-Block den Prozessen innerhalb der grauen Umrandung, während die Eingabe- und Ausgabepipeline darunter bzw. darüber zu finden sind."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f86373b1",
   "metadata": {},
   "source": [
    "\n",
    "## Input\n",
    "\n",
    "Der erste Teil eines Transformermodells besteht aus der Eingabepipeline. Diese verarbeitet die Eingabe, z.B. die Texteingabe eines Nutzers, und bereitet sie auf die Verarbeitung in den Attention-Modulen vor. Die Attention-Module arbeiten über eine Attention-Matrix, die aus der jeweiligen Eingabe eine Ausgabe berechnet. Wir müssen also aus einer Eingabe in Textform eine Vektorrepräsentation erzeugen, die alle notwendigen Informationen für das Modell enthalten, um mithilfe von Matrixmanipulationen nützliche Vorhersagen zu machen.\n",
    "\n",
    "In <a href=\"#fig:fig2\">Abbildung 2</a> sehen sie nochmal den Ausschnitt aus der obigen Grafik, der die Eingabepipeline darstellt. \n",
    "Wie zu erkennen ist, wird in der Eingabepipeline während des Training eines Transformermodells ausschließlich die Weights für das Embedding trainiert, alle anderen Funktionen sind rein deterministisch und bleiben damit vom Trainingsprozess unbeeinflusst.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4caafaa5",
   "metadata": {},
   "source": [
    "\n",
    "<figure id=\"fig:fig2\" style=\"text-align: center; height: 700px;\">\n",
    "  <img src=\"./img/tf_input_pipeline.jpg\" style=\"height: 700px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 2: Eingabepipeline eines Transformer-Netzwerks</figcaption>\n",
    "</figure>\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4578ca",
   "metadata": {},
   "source": [
    "Prinzipiell besteht die Eingabepipeline aus drei Modulen:\n",
    "\n",
    "1. Die [Tokenisation](#tokenisation), die den Text mithilfe eines Symbolalphabets in eine Zahlenkodierung umwandelt.\n",
    "2. Das [Embedding](#embedding), das diese Kodierung mithilfe eines trainierbaren Algorithmus in eine Vektordarstellung umwandelt. Das Embedding lernt die komplexe Struktur eines Textes so darzustellen, dass sie informativ für die nachfolgenden Module ist. Wie genau das passiert ist dabei aufgrund der stochastischen Natur von Deep Learning Modellen nicht offen einsehbar.\n",
    "3. Das [Positional Encoding](#positional-encoding) ein mit der Transformer-Architektur eingeführter Mechanismus. Im Gegensatz zu RNNs, die die Eingabedaten sequentiell präsentiert bekommen [19], enthalten bei Transformermodellen die Eingaben keine Information zur relativen Position der Tokens. Diese fehlenden Informationen werden in diesem Schritt manuell hinzugefügt.\n",
    "\n",
    "### Tokenisation\n",
    "\n",
    "Bei der Tokenisation wird der Satz in Textform z.B. \"Das ist ein Testsatz.\" in einen Zahlencode verwandelt. Hierfür kommen verschieden Methoden in Frage. Einer der simpelsten Methoden ist es jedem Buchstaben eine eindeutige Zahl zuzuordnen. Da hier jeder Buchstabe einzeln kodiert werden muss, führt das allerdings zu einer sehr langen Kodierung. Obwohl das also eine mögliche Kodierung wäre, gibt es bessere Verfahren.\n",
    "\n",
    "Die entscheidenden Faktoren für eine gute Kodierung sind\n",
    "- Vollständigkeit der Kodierung, \n",
    "- Länge des kodierten Vektors und \n",
    "- Größe des dafür nötigen Vokabulars.\n",
    "\n",
    "Die Kodierung mit einzelnen Buchstaben ist vollständig (man kann beliebige Zeichenkombinationen kodieren) und besitzt ein kurzes Vokabular (26 für alle Buchstaben plus Sonderzeichen), allerdings ist die Länge der kodierten Vektoren sehr groß.\n",
    "\n",
    "Benutzt man ein Vokabular aus Wörtern, wird die Länge der Kodierung verkürzt, allerdings besteht die Gefahr der Unvollständigkeit. Um das nach Möglichkeit zu verhindern, benötigt man ein Vokabular, dass den Raum der möglichen Wörter vollständig abdeckt. Das Vokabular der Kodierung müsste also sehr groß sein.\n",
    "\n",
    "Die Probleme mit obigen Methoden hat dazu geführt, dass sich gemischte Verfahren ergeben haben, die an einem möglichst großem Korpus trainiert werden. Es gibt einerseits Top-Down Verfahren, die von einem aus dem Korpus extrahierten Wort-Vokabular ausgehen und dann lernen unbekannte Worte in Teilworte zu zerlegen, die in das Vokabular mitaufgenommen werden. Andererseits gibt es Bottom-Up Verfahren, die von einem Buchstabenvokabular ausgehen und häufig wiederkehrende Kombinationen in dieses explizit aufnehmen.\n",
    "Die Transformerarchitektur nach [1] verwendet ein solches Bottom-Up Verfahren namens Byte-Pair Encoding [13], dass sich als effizient erwiesen hat und ein relativ kompaktes Vokabular ermöglicht dabei aber die Länge der Tokenisation klein hält. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4a5e1d1",
   "metadata": {},
   "source": [
    "\n",
    "### Byte-Pair Encoding\n",
    "\n",
    "Das Byte-Pair Encoding Verfahren nutzt ein Vokabular mit einer festgelegten Länge. In unserer Implementation des Tokenizer nutzten wir ein Vokabular von der Länge 8000. Das Vokabular wird dabei folgendermaßen erstellt:\n",
    "\n",
    "  1. Ein Text, der für die Erstellung des Vokabulars verwendet wird, wird in eine Sequenz von Symbole zerlegt. Wortenden werden mit einem zusätzlichen Symbol kodiert.\n",
    "  2. Alle vorhandenen Symbole werden automatisch in das Vokabular aufgenommen.\n",
    "  3. Nun wird das häufigste 2-Gramm, also zwei aufeinander folgende Symbole, gesucht, das im Text zu finden ist.\n",
    "  4. Dieses wird ins Vokabular aufgenommen und im Text durch ein einzelnes Symbol ersetzt.\n",
    "  5. Dieser Prozess wird nun wiederholt, bis die vorgegebene Länge des Vokabulars erreicht ist.\n",
    "\n",
    "Dabei können sowohl ganze Wörter ins Vokabular aufgenommen werden, wenn sie denn oft genug auftauchen (bespielweise werden die Worte \"a\", \"the\", \"and\" bei englischen Texte sicherlich mitaufgenommen werden), aber auch einzelne Wortteile wie z.B. \"en##\", \"##ment\" oder \"##ed\" werden in diesem Vokabular sicherlich vorkommen, um seltene Kombinationen wie \"enablement\" in die Wortteile \"en##\", \"able\" und \"##ment\" zerlegen zu können oder grammatikalische Formen wie \"wanted\" zu bilden. Die Zeichenfolge \"##\" beschreibt dabei, dass hier kein Wortende stehen darf.\n",
    "\n",
    "In unserem Testbeispiel ist zu sehen, wie Ihre Eingabe in Tokens getrennt und dann in eine Kodierung umgewandelt wird, je nachdem, welche Position das Token in unserem Vokabular hat.\n",
    "Wie Sie sehen, enthält das Byte-Pair Encoding Vokabular auch ein [START]- und [END]-Token, sowie Elemente vom Typ 'abc##' oder '##abc'. Die Elemente mit Doppel-'#' stellen eine Sequenz am Anfang oder Ende eines Wortes dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60193c08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:50.045785600Z",
     "start_time": "2023-11-13T16:26:50.025040100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f79cc978694d35b38a3c339b0d9661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Tokenizer test', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6d6a8ee0f643feb8450bea20328587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run tokenizer on input', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebd2d62e67b462bb17da9f666e1400e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_tok = widgets.Text(\n",
    "    value='Tokenizer test',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_tok = widgets.Button(description='Run tokenizer on input',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_tok = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "\n",
    "def tokenize(input_widget_tok):\n",
    "    tokens = tokenizer.tokenize(input_widget_tok.value)                    # Erstellung der Tokens als Index für Vokabular\n",
    "    lookup = tokenizer.lookup(tokens)                                      # Abrufen der Zeichenkette des Index im Vokabular                    \n",
    "    \n",
    "    return tokens, lookup\n",
    "    \n",
    "def on_button_click(b):\n",
    "    with output_widget_tok:\n",
    "        output_widget_tok.clear_output()                                                        \n",
    "        tokens, lookup = tokenize(input_widget_tok)\n",
    "\n",
    "        VisualWrapper.display_text('Tokens die aus der Eingabe mit Byte-Pair Encoding extrahiert werden:'.rjust(100) + ', '.join([token.decode('utf-8').rjust(10) for token in lookup.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "        VisualWrapper.display_text('Ihre Positionsnummer im Alphabet des Byte-Pair Encoding Algorithmus:'.rjust(100) + ', '.join([str(token).rjust(10) for token in tokens.numpy()[0]])\n",
    "                                   .replace(' ', '&nbsp;'))\n",
    "\n",
    "button_widget_tok.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_tok, button_widget_tok, output_widget_tok)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e5b84a",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Die Sequenzen die durch den Tokenizer entsteht variiert in ihrer Länge. Einen Einfluss darauf hat die Länge der Eingabe, aber auch durch die Eigenschaften des Byte-Pair-Encoding. Für die Parallelisierung des Trainingsprozesses ist es für Transformer aber notwendig eine gleichbleibende Eingabelänge beizubehalten.\n",
    "\n",
    "Dies wird dadurch gelöst, dass Padding Tokens zum Einsatz kommen, also Tokens, die keine Information codieren, sondern der Eingabe beigefügt werden, um die erforderliche Länge zu erreichen.\n",
    "\n",
    "Außerdem wird die Ausgabe des Tokenizers mithilfe einer trainierbaren Matrix in das notwendige Vektorformat gebracht\n",
    "\n",
    "Das Embedding sorgt dafür, dass die Ausgabe des Tokenizer in einen Vektor der Modellgröße $d_{model}$ kodiert wird. Das heißt jedes Token, das zuvor durch eine Zahl kodiert wurde wird nun durch einen Vektor der Länge $d_{model}$ kodiert. \n",
    "Das Embedding  ist Teil der vom Modell gelernten Parameter (siehe <a href=\"#fig:embedding\">Abbildung 3</a>) und somit nicht deterministisch gegeben. Welche Informationen aus der Ausgabe des Tokenizers wo gespeichert wird ist also nicht nachvollziehbar.\n",
    "Vorstellen kann man sich aber, dass in jedem der $d_{model}$ Parametern einige der Informationen gespeichert werden, die für das jeweilige Token wichtig sind. Beispielsweise die Bedeutung des Tokens, seine grammatikalische Form, in welchem Kontext es benutzt wurde, steht es am Anfang eines Satzes oder am Ende, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab2ec3f9",
   "metadata": {},
   "source": [
    "<figure id=\"fig:embedding\" style=\"text-align: center; height: 300px;\">\n",
    "  <img src=\"./img/tf_embedding.jpg\" style=\"height: 300px;\" alt=\"Eingabepipeline mit Tokenizer, Embedding und Positional Encoding.\"/>\n",
    "  <figcaption>Abbildung 3: Gewichte der Eingabepipeline</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b34b4d",
   "metadata": {},
   "source": [
    "Wie ein solches Embedding aussieht und wie es sich verändert, wenn man beispielsweise neue Teile an den Satz anfügt können Sie in der nachfolgenden Simulation ausprobieren. Der Eingabetext wird erst vom Tokenizer in Tokens umgewandelt und dann durch das Embedding in einen Tensor.\n",
    "\n",
    "An jeder Position (vertikal dargestellt) ist dann das Embedding des Tokens an dieser Stelle zu sehen (horizontal dargestellt). Die farbliche Kodierung stellt dabei das Zahlenspektrum dar, indem sich die Vektoreinträge bewegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "758a1d67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:50.088784200Z",
     "start_time": "2023-11-13T16:26:50.045785600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42be7f46baf49189eb49b76a4c223af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<p style=\"font-size:18px; color:blue;\">Hier kannst du einen Text einbetten lassen. Wenn du die Ein…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b239330e62b4af8a9b092cd6a143a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Einbettung Test', continuous_update=False, description='Ihre Eingabe:', layout=Layout(margin='0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4ba186c3f243c8a80bebaaaa0badd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Einbettung erstellen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd4e08fc38a452896bb90caf9451b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EmbeddingExample():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "        self.input_widget = widgets.Text(\n",
    "            value = 'Einbettung Test',\n",
    "            description = 'Ihre Eingabe:',\n",
    "            continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "            layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "        )\n",
    "\n",
    "        self.button_widget = widgets.Button(description='Einbettung erstellen',\n",
    "                                    layout = widgets.Layout(width='auto'))\n",
    "\n",
    "        self.output_widget = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "        self.old_context = None\n",
    "\n",
    "    def create_tokenized_embeddings(self):\n",
    "        tokens = self.tokenizer.tokenize(self.input_widget.value)                                 # Tokenisierung der Eingabe\n",
    "        tokens_all = tokens[tf.newaxis, :, :]                                                     # Hinzufügen einer weiteren Dimension\n",
    "        input_without_eos = tokens[tf.newaxis, :, :-1]                                            # Auswahl der Tokens bis zum [END] Token\n",
    "        token_input = self.tokenizer.detokenize(tokens_all)                                       # Nur zur Ausgabe Zwecken\n",
    "        string_value = token_input.numpy()[0][0].decode('utf-8')                                  # Nur zur Ausgabe Zwecken\n",
    "        lookup = tokenizer.lookup(input_without_eos)                                              # Nur zur Ausgabe Zwecken\n",
    "        lookup = [item.decode('utf-8') for sublist in lookup.numpy()[0] for item in sublist]      # Nur zur Ausgabe Zwecken\n",
    "        print(\"Wörter: \", string_value)\n",
    "        print(\"Tokens: \", lookup)\n",
    "        context = model.model.enc_embed(input_without_eos)                                        # Erstellung des Kontext Embedding \n",
    "        VisualWrapper.display_text('So sieht die Einbettung der Eingabe aus.')\n",
    "        VisualWrapper.color_bar(context.to_tensor())\n",
    "        if self.old_context is not None:\n",
    "             padded_context, padded_old_context = self.pad_tensors(context, self.old_context)     # Erstellung des Padding Vektors der Eingaben\n",
    "             VisualWrapper.display_text('So unterscheiden sich die alte und die neue Einbettung voneinander.')\n",
    "             context_diff = padded_context - padded_old_context                                   # Berechnung der Unterschiede beider Vektoren\n",
    "             VisualWrapper.color_bar(context_diff)\n",
    "\n",
    "        self.old_context = context\n",
    "\n",
    "    \n",
    "    def on_button_click(self, b):\n",
    "        with self.output_widget:\n",
    "            self.output_widget.clear_output()  # clear the previous output\n",
    "            VisualWrapper.reset_visualiser()\n",
    "            self.create_tokenized_embeddings()\n",
    "    \n",
    "    def pad_tensors(self, ragged_tensor1, ragged_tensor2):\n",
    "        \"\"\"Funktion um die Tensoren der Eingabe auf die gleiche Länge zu transformieren\"\"\"\n",
    "        tensor1 = ragged_tensor1.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "        tensor2 = ragged_tensor2.to_tensor()                                                     # Umwandlung in normalen Tensor\n",
    "\n",
    "        shape1 = tf.shape(tensor1)\n",
    "        shape2 = tf.shape(tensor2)\n",
    "\n",
    "        target_shape = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            target_shape.append(tf.maximum(shape1[i], shape2[i]))                                # Die maximale Größe der Dimension wird an die Zielform angehängt.\n",
    "\n",
    "        target_shape = tf.stack(target_shape)                                                    # Umwandlung der Zielform in einen Tensor\n",
    "\n",
    "\n",
    "        paddings1 = []\n",
    "        paddings2 = []\n",
    "\n",
    "        for i in range(shape1.shape[0]):                                                         # Iterieren über die Dimensionen der Tensoren\n",
    "            paddings1.append([0, target_shape[i] - shape1[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "            paddings2.append([0, target_shape[i] - shape2[i]])                                   # Auffüllung der Tensor auf maximale Länge \n",
    "\n",
    "        paddings1 = tf.stack(paddings1)                                                          # Konvertieren der Paddings in Tensoren\n",
    "        paddings2 = tf.stack(paddings2)                                                          # Konvertieren der Paddings in Tensoren\n",
    "\n",
    "        tensor1_padded = tf.pad(tensor1, paddings1)                                              # Tensoren an die Zielform anpassen\n",
    "        tensor2_padded = tf.pad(tensor2, paddings2)                                              # Tensoren an die Zielform anpassen\n",
    "\n",
    "        return tensor1_padded, tensor2_padded\n",
    "\n",
    "emb_ex = EmbeddingExample()\n",
    "\n",
    "VisualWrapper.display_text('Hier kannst du einen Text einbetten lassen. Wenn du die Eingabe veränderst wird außerdem gezeigt, wie sich die Einbettung geändert hat.')\n",
    "\n",
    "emb_ex.button_widget.on_click(emb_ex.on_button_click)\n",
    "display(emb_ex.input_widget, emb_ex.button_widget, emb_ex.output_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef7ac5-da74-40c0-be78-f0f866ca9fd9",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesen Beispiel wie ein Embedding grafisch aussieht. Das Embedding wird in der Funktion *create_tokenized_embeddings()* erstellt. Dazu wird zu erst der Eingabetext vom Tokenizer in Tokens unterteilt (Zeile 20). Die Tokens können Sie über der Grafik sehen. In Zeile 29 werden diese dann vom Transformer Modell in die Embeddings umgewandelt. \n",
    "\n",
    "Verwendet man zum Beispiel das Eingabebeispiel \"Einbettung Test\" und als zweites \"Einbettung Test neu\", sieht man im zweiten Bild wie die zusätzlichen Positionen dazu kommen. Die Visualisierung von \"Einbettung Test\" zeigt auf der y-Achse sieben Tokens. Bei dem Satz \"Einbettung Test neu\" kommen dann 3 Tokens dazu, diese sind \"#n\", \"##e\", \"##u \". Ebenso kann man ein Muster bei dem Wertebereich erkennen. Der Bereich der Tiefe zwischen 0bis 256 bewegt sich hauptsächlich im Wertebereich -1 bis 2 und der Bereich von 257 bis 512 im Wertebereich 0 bis -3. Dies geht auf die Sinus-Funktion des Positional Encodings zurück, welche auf das Embedding angewendet wird. Diese hat für diese Bereiche stark unterschiedliche Werte. \n",
    "\n",
    "Die Werte in den ersten 256 Positionen können bestimmte Merkmale oder Eigenschaften der Wörter repräsentieren, während der Bereich der Positionen von 266 bis 512 andere Merkmale oder Eigenschaften widerspiegelt. Diese getrennte Darstellung ermöglicht dem Modell, komplexe Beziehungen und Muster in den Daten zu erfassen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e70583",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Da im Embedding keine Informationen über die relative Position der verschiedenen Worte kodiert wird, muss diese manuell hinzugefügt werden. Hierfür benutzt die Transformerarchitektur für jede Position des Embedding (also jedes enkodierte Wort) eine Sinuskurve mit anderer Frequenz und Phase. [1] Der sich ergebende Wert wird dem Embedding an der jeweiligen Stelle hinzugefügt.\n",
    "\n",
    "Dadurch lassen sich die verschiedenen Worte sehr gut voneinander trennen. Die Idee dahinter ist, dass sich:\n",
    "1. die grobe Position eines Worte anhand der langfrequenten Sinuskurven bestimmen lässt, da sie sich über die gesamte Länge der Eingabe nur allmählich verändern und die Werte des Embedding durch diese insgesamt in eine bestimmte Richtung verschoben werden, z.B. die Worte im hinteren Teil der Eingabe größere Werte besitzen als die im vorderen Teil der Eingabe,\n",
    "2. die genaue Position durch die hochfrequenten Sinuskurven bestimmen lässt, da diese sich schon für benachbarte Vektoren klar unterscheiden und somit klar wird, welches Wort an welcher Stelle im Embedding kodiert wurde.\n",
    "\n",
    "$$\n",
    "PE(pos, i) =\n",
    "\\begin{cases}\n",
    "  \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{modell}}}}\\right) & \\text{falls } i \\text{ gerade ist} \\\\\n",
    "  \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{modell}}}}\\right) & \\text{falls } i \\text{ ungerade ist}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $pos$ die Position des Tokens in der Sequenz ist.\n",
    "- $i$ der Index der Dimension in der Positionsverschlüsselung ist.\n",
    "- $d_{model}$ die Dimensionalität des Modells ist.\n",
    "\n",
    "In der unten stehenden Simulation ist zu sehen, wie das positional Encoding beipielhaft für ein 2048 x 512 langes und tiefes Embedding aussieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d875186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:50.290492700Z",
     "start_time": "2023-11-13T16:26:50.079780900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502a75e94a584678b3309568ef7deb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1025, description='length', max=2048, min=2), IntSlider(value=257, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(length=(2,2048,1), depth=(2,512,1))\n",
    "def print_pos_enc(length, depth):\n",
    "    VisualWrapper.color_bar(positional_encoding(length, depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb626ffc-d9b6-4181-92d3-1bf3618660b4",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "An diesem Beispiel sieht man den Effekt des Positional Encodings auf verschiedene lange bzw. tiefe Einbettungen. Wenn man mit den zwei Reglern spielt, sieht man, verschieden große Zooms auf den Effekt. Erhöht man den \"length\"-Regler sieht man die Auswirkungen, welche die Länge des Satzes betreffen. Die Werte die durch das Positional Encoding zu dem Vektor hinzugefügt werden sind dabei jedoch immer dieselben. Zum Bespiel wird an Stelle 100 immer derselbe Wert hinzugefügt, egal ob die maximale Länge des Satzes (\"length\") sich verändert. Dasselbe gilt auch für die Tiefe jedes einzelnen Vektors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389fa0d1",
   "metadata": {},
   "source": [
    "\n",
    "## Trainingsmethoden\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout [11] ist eine Methode, die während des Trainingsprozesses eines neuronalen Netzes genutzt wird, um zu verhindern, dass die gelernte Ausgabe eines Modells sich zu sehr auf einen einzelnen Prädikator stützt.\n",
    "Dafür wird zwischen zwei Schritten desselben Modells, eine Dropout-Layer eingefügt. Diese setzt zufällig einige der vom ersten Modellteil generierten Ausgabe auf einen vordefinierten Wert (meistens -inf), um den nachfolgenden Schichten diese Information vorzuenthalten. Da diese Operation zufällig erfolgt, müssen die nachfolgenden Teile des Modells lernen ihre Ausgabe auch ohne diese Information zu erstellen. Somit lernt das Model seine Vorhersage auf eine möglichst breite Kombination an Merkmalen aufzubauen und man verhindert, dass Vorhersagen nur aufgrund eines einzigen Merkmals der vorherigen Ausgabe gemacht werden.\n",
    "\n",
    "Ein gutes Beispiel ist das Ende eines Satzes vorherzusagen. In europäischen Sprachen wird ein Satz fast immer mit einem Punkt beendet, also ist es eine gute Strategie zu lernen, dass ein Satz durch einen Punkt beendet wird. Doch ein Modell, dass einen Punkt als einziges Merkmal eines Satzendes nutzt ist wenig robust. Wenn man an falscher Stelle einen Punkt setzt oder ihn an einem Satzende durch ein anderes Zeichen ersetzt werden die Vorhersagen des Models schlecht sein. Dabei gibt es auch andere Hinweise auf ein Satzende, z.B. das Vorkommen eines Verbs in der deutschen Sprache oder von Ort und Zeitangaben im Englischen.\n",
    "\n",
    "Um dem Modell keine Informationen vorzuenthalten, wenn es tatsächlich eingesetzt wird, ist das Dropout immer nur während des Trainings aktiv und wird danach abgeschalten, sodass während der Inferenzphase keine Informationen gelöscht werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b5c7833",
   "metadata": {},
   "source": [
    "Wie sich eine Ausgabe eines Transformermodells unterscheiden kann, wenn man Dropout anwendet können Sie in der nachfolgenden Simulation sehen.\n",
    "\n",
    "In der ersten Grafik sieht man welche Werte in einem uniformen Vektor vom Dropout verändert werden. In den beiden darauffolgenden Grafiken wird das Dropout auf den im Textfeld eingegebenen Beispieltext angewandt, nachdem er durch den Tokenizer und ein Embedding in Vektorform gebracht wurde. Die erste Grafik zeigt den vollständigen Vektor und die zweite Grafik den Vektor, der vom Dropout verändert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b358029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:50.970429400Z",
     "start_time": "2023-11-13T16:26:50.289446900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ec05195a2845b88e8eff4d4ec8034d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=30, continuous_update=False, description='Länge des Tensors:', max=2048, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455d69b761034b9f87a9350b181e8223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=512, continuous_update=False, description='Tiefe des Tensors:', max=512, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e3c9cd912c4b88a8f4594c5b050edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.1, continuous_update=False, description='Dropoutrate:', max=0.9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a027bb73f0b246f59ab10ecf259bb6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Tes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72af34f64fb0406ab25ee9cf314bc492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_drop = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input_widget_drop = widgets.Text(value = 'Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test Test',\n",
    "                                 description = 'Ihre Eingabe:',\n",
    "                                 continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                                 layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    "                                 )\n",
    "\n",
    "length_widget_drop = widgets.IntSlider(value=30,\n",
    "                           min=2,\n",
    "                           max=2048,\n",
    "                           description='Länge des Tensors:',\n",
    "                           continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                           )\n",
    "depth_widget_drop = widgets.IntSlider(value=512,\n",
    "                          min=2,\n",
    "                          max=512,\n",
    "                          description='Tiefe des Tensors:',\n",
    "                          continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                          )\n",
    "dropout_widget = widgets.FloatSlider(value=0.1,\n",
    "                              min=0,\n",
    "                              max=0.9,\n",
    "                              step=0.1,\n",
    "                              description='Dropoutrate:',\n",
    "                              continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "def dropout_function(length, depth, dropout, input):\n",
    "\n",
    "    # Erstellung der Dropout Layer\n",
    "    dropout_layer = layers.Dropout(dropout)                                                         # Anwendung des Dropout auf die Layer\n",
    "    one_tensor = tf.ones([length, depth])                                                           # Erstellung eines Arrays aus 1-en\n",
    "    dropout_tensor = dropout_layer(one_tensor, training=True)                                       # Anwendung des Dropout auf die Layer\n",
    "\n",
    "    # Erstellung der Kontext Layer\n",
    "    tokens = tokenizer_drop.tokenize(input)                                                         # Tokenisierung des Inputs\n",
    "    input_without_eos = tokens[tf.newaxis, :, 1:-1]                                                 # Auswahl der Tokens bis zum [END] Token\n",
    "    context = model.model.enc_embed(input_without_eos)                                              # Erstellung des Embedding durch das Modell\n",
    "    context_drop = dropout_layer(context, training=True)                                            # Anwendung des Dropout auf das Embedding\n",
    "\n",
    "    return dropout_tensor, context_drop, context\n",
    "\n",
    "def out(length, depth, dropout, input):\n",
    "    VisualWrapper.reset_visualiser()                                                   \n",
    "    dropout_tensor, context_drop, context = dropout_function(length, depth, dropout, input)\n",
    "    VisualWrapper.color_bar(dropout_tensor)\n",
    "                               \n",
    "    VisualWrapper.color_bar(context.to_tensor())                     \n",
    "    VisualWrapper.color_bar(context_drop.to_tensor())\n",
    "    \n",
    "\n",
    "output_widget_dropout = widgets.interactive_output(out,\n",
    "                                                   {'length': length_widget_drop, 'depth': depth_widget_drop, 'dropout': dropout_widget, 'input': input_widget_drop}, \n",
    "                                                   )\n",
    "\n",
    "display(length_widget_drop, depth_widget_drop, dropout_widget, input_widget_drop, output_widget_dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d176e24-7e39-4e92-95d5-3d24eef076ec",
   "metadata": {},
   "source": [
    "#### Erklärung des Beispiels\n",
    "In diesem Beispiel wird der Effekt von Dropout auf die Layer und das Embedding dargestellt. Dafür werden in der Funktion *dropout_function()* jeweils Dropout auf die Layer und auf das Embedding angewendet. Dafür wird in Zeile 36 und 42 jeweils die jeweilige Transformation auf den beiden Objekten angewendet. Damit wird also ein gewisser Teil, welche mit dem Parameter \"Dropoutrate\" bestimmt wird, der Werte Layer bzw. des Embeddings den weiteren Verarbeitungsschritten vorenthalten. Dieser Parameter ist ein prozentualer Wert, d.h. bei einem Wert von 0.2 werden 20% der Werte vorenthalten. \n",
    "\n",
    "Für das Beispiel können dieses Mal die Länge des Tensors (Eingabe) und die Tiefe des Tensors bestimmt werden. Ebenso kann die Dropoutrate verändert werden. In der ersten Grafik sieht man welche Werte in einem uniformen Vektor vom Dropout verändert werden. Das bedeutet, welche der Werte in der Tiefe des Tensors ausgeblendet werden. Der Wert ist entspricht dabei einer Prozentzahl bei 0.4 werden somit 40 Prozent der Werte ausgeblendet bzw. auf den festgesetzten Wert gesetzt.\n",
    "In den beiden darauffolgenden Grafiken wird das Dropout auf den im Textfeld eingegebenen Beispieltext angewandt, nachdem er durch den Tokenizer und die Einbettung in Vektorform gebracht wurde. Die erste Grafik zeigt den vollständigen Vektor und die zweite Grafik den Vektor, der vom Dropout verändert wurde. Hier sieht man die ausgelassenen Positionen sehr gut. Mit dem Erhöhen der Dropout Rate, werden diese mehr. Außerdem kann man erkennen, dass sich, wenn man den Dropout erhöht, der Wertebereich ebenfalls ausweitet. Erklärbar ist dies dadurch, dass Merkmale, vor allem prägnante Merkmale, sich durch das weitere Training verstärken bzw. abschwächen. Die Dropoutrate kann jedoch nicht beliebig erhöht werden bei gleichzeitigem Erhalt der Güte der Klassifikation.\n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2d169f9",
   "metadata": {},
   "source": [
    "### Normalisierung\n",
    "\n",
    "Normalization ist eine Technik, die von [20] eingeführt wurde. In Deep Neural Networks, die mit nicht-linearen Aktivierungsfunktionen wie der Sigmoid-Funktion\n",
    "\n",
    "$$g(x) = 1/(1+exp(-x))$$\n",
    "\n",
    "trainiert werden gilt, dass $g’(x) -> 0 für |x|-> inf$. \n",
    "\n",
    "Das führt dazu, dass diese Modelle in einen Bereich geraten können, in der der Gradient für das Stochastic Gradient Descent (SGD) minimal wird und man vom Vanishing Gradient Problem spricht.\n",
    "\n",
    "In neuronalen Netzen ist hierbei das Problem, dass eine Layer $z = g(Wx + b)$ mit der Sigmoid-Funktion g versucht den Output des gesamten vorherigen Netzes x zu gewichten. Dabei hängen sowohl W als auch b von x ab. Da sich während des Trainings alle Layers des Netzes fortwährend aktualisieren, ändert sich auch der Input x fortwährend. \n",
    "Dieser Effekt wird von [20] Internal Covariate Shift genannt. \n",
    "\n",
    "Je tiefer das neuronale Netz, umso größer sind diese Veränderungen, da es mehr Schichten gibt, die sich verändern können. Die Tiefe einer neuronalen Netzes erhöht also die Wahrscheinlichkeit das ein Vanishing Gradient Problem auftritt und ein effektives Training frühzeitig aufhört.\n",
    "\n",
    "Transformer wie sie in [1] beschrieben sind nutzen hierfür Layer Normalization, einen Normalisierungsalgorithmus den [21] entwickelt hat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39a9d2b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:51.050157800Z",
     "start_time": "2023-11-13T16:26:50.970429400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b736c94303c42edb064e80d282e65ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Button(description='Click to proceed', style=ButtonStyle())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VisualWrapper.reset_visualiser()\n",
    "tokenizer_norm = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "\n",
    "input=\"Test\"\n",
    "\n",
    "tensor_input = tf.convert_to_tensor(input)\n",
    "if len(tensor_input.shape) == 0:\n",
    "    tensor_input = tensor_input[tf.newaxis]\n",
    "\n",
    "tokenized_input = tokenizer_norm.tokenize(tensor_input).to_tensor()                             # Anwendung eines Tokenizers mit Normalisierung\n",
    "input_without_eos = tokenized_input[:, :-1]\n",
    "context = model.model.encode(input_without_eos, None)\n",
    "\n",
    "VisualWrapper.visualize_data(id='layer')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57791aa1",
   "metadata": {},
   "source": [
    "\n",
    "### Residual Connection\n",
    "\n",
    "Die Idee für das Nutzen von Residual Connections kommt von [22]. Die Autoren stellten fest, dass bei neuronalen Netzen sowohl die Genauigkeit während des Trainings als auch die Genauigkeit auf dem Testdatensatz mit zunehmender Tiefe schlechter wird.\n",
    "\n",
    "Da durch Normalization bereits sichergestellt ist, dass das Vanishing Gradient Problem nicht auftritt, scheitert die Optimierung der neuronalen Netze aus anderen Gründen.\n",
    "Einer der Gründe hierfür liegt vermutlich darin, dass die tieferen Schichten eines Modells zu Beginn des Trainings sehr viel stärker zur Ausgabe beitragen, als die vorhergehenden Schichten. Sie werden somit zuerst trainiert. Die weniger tiefen Schichten werden erst ausreichend trainiert, wenn in den tiefen Schichten keine Optimierung mehr möglich ist. \n",
    "\n",
    "Eine Lösung hierfür sind Residul Connections.\n",
    "Sie ersetzen eine Schicht F(x) durch ihre Residual Connection\n",
    "\n",
    "\\\n",
    "$$H(x) = F(x) + x.$$\n",
    "\n",
    "<br>\n",
    "In das Ergebnis von H(x) geht also sowohl der Output, als auch der Input von F direkt mit ein. Wendet man dieses Prinzip auf die Schichten tiefer neuronaler Netze an, sorgt das dafür, dass gleich zu Beginn, der Output der wenig tiefen Schichten relevant in den Output des gesamten Netzes einfließt, denn es gilt für das gesamte Netz N:\n",
    "\n",
    "\\\n",
    "$$N(x) = H_n(H_n-1(x)) + H_n-1(x) = H_n(H_n-1(x)) + H_n-1(H_n-2(x)) + … + H_2(H_1(x)) + H_1(x)$$\n",
    "\n",
    "<br>\n",
    "Wie man in <a href=\"#fig:fig1\">Abbildung 1</a> sehen kann, haben alle Attentionsmodule, sowie alle Feed-Forward Schichten in einem Transformermodell eine residuale Verbindung.\n",
    "\n",
    "In der nachfolgenden Simulation können Sie sehen, wie sich die Ausgabe einer neuronalen Schicht verändert, wenn man ihr eine Residual Connection beifügt. Der Effekt auf den gesamten Trainingsprozess lässt sich dabei natürlich nur schwer darstellen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa40a6df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Layers\n",
    "\n",
    "### Attention\n",
    "\n",
    "Die Neuerung von Transformern im Vergleich zu vorangegangenen Lösungen für Neural Machine Trainslation (NMT) ist es, allein auf Attention als Mechanismus für das Verarbeiten von Sprache zu setzen. Attention wurde auch vorher schon von [15] zur Verbesserung von RNNs zur Übersetzung von Texten verwendet.\n",
    "\n",
    "Der Attention-Mechanismus, wie ihn [1] beschreiben, orientiert sich dabei an der Idee einer Suchanfrage des Ausgabetextes and sich selbst, bzw. den Eingabetext. Die Attention-Layer bekommt dabei zwei oder eigentlich drei Eingaben: \n",
    "\n",
    "1. den Query (Q), \n",
    "2. den Key (K),\n",
    "3. den Value (V).\n",
    "\n",
    "In der Praxis erhalten aber Key und Value in Transformern immer dieselbe Eingabe und häufig sind Query, Key und Value sogar alle identisch. Aus welcher Quelle Query, Key und Value kommen unterscheidet verschiedene Formen von Attention. So nennen wir Self-Attention denjenigen Fall indem Q=K=V gilt und Cross-Attention denjenigen Fall indem der Query aus der Ausgabe des Encoder besteht und Key und Value beide aus der Ausgabe eines Decoder-Blocks stammen.\n",
    "\n",
    "Um zu erklären, wie Attention funktioniert, sollten wir aber zunächst davon ausgehen, dass Query-, Key- und Value-Eingabe verschieden sind. Ich schreibe bewusst von der Eingabe, da in jeder Attention-Layer zunächst Query-, Key- und Value-Eingabe Q', K', V' mit Hilfe von gewichteten Matrix W^Q, W^K und W^V in Query, Key und Value \n",
    "\n",
    "$$Q=Q′×W^Q$$\n",
    "$$K=K′×W^K$$\n",
    "$$V=V′×W^V$$ \n",
    "\n",
    "umgewandelt werden. Diese gewichteten Matrizen W^Q, W^K, W^V sind die trainierbaren Weights der Attention-Layer. Alle nachfolgenden Prozesse sind deterministischer Natur. Das bedeutet, dass diese Matrizen festlegen, welches Ergebnis die Attention-Layer liefert.\n",
    "Eine grafisch aufbereitete Erklärung dessen, was hier beschrieben wird findet sich übrigens bei [9].\n",
    "\n",
    "#### Attention Function\n",
    "\n",
    "Die Attentionsfunktion bekommt die Eingaben Q, K, V aus der vorher beschriebenen Gewichtung und lautet:\n",
    "<br>\n",
    "<br>\n",
    "$$Attention(Q, K, V)=softmax(QK^T/sqr(d_k))V$$\n",
    "<br>\n",
    "Es wird also zuerst das Kreuzprodukt aus Q und K gebildet. Dieses Produkt wird mit sqr(*d_k*) skaliert, weshalb ist in folgendem [Kaptiel](#skalierung-mit-sqrd_k) nachzulesen, und auf dieses Ergebnis wird dann die Softmax-Funktion sigma(x) angewandt.\n",
    "Diese Funktion lässt sich am besten positionsweise beschreiben\n",
    "<br>\n",
    "<br>\n",
    "$$sigma(x)_i = exp(x_i)/sum_j=1^n(exp(x_j)) für i=1,...,n.$$\n",
    "<br>\n",
    "Nennen wir also\n",
    "<br>\n",
    "$$softmax(QK^T/sqr(d_k)) = Score(Q,K),$$\n",
    "<br>\n",
    "dann ist \n",
    "<br>\n",
    "$$Attention(Q, K, V) = Score(Q,K)*V $$\n",
    "<br>\n",
    "eine Funktion, die $V$ mit einem Vektor multipliziert, wobei für den Vektor gilt $|Score(Q,K)| = 1$.\n",
    "\n",
    "$Score(Q,K$) ist also ein Vektor exakt der Länge von $V$, der angibt, mit welchem Anteil jeder Eintrag von $V$ in den Ausgabevektor $Attention(Q,K,V)$ eingehen soll. Dabei summiert sich $Score(Q,K)$ zu 1, ist also tatsächlich eine Gewichtung der Einträge von $V$.\n",
    "\n",
    "Da mit $Score(Q,K)$ nun also eine Gewichtung besteht, wie stark die Ausgabe $Attention(Q,K,V)_i$ von $V_j$ abhängt, kann man dieses Verhältnis als Matrix grafisch darstellen. Dies geschieht in der Simulation unten, bei der für den vorgegebenen Satz eine Attentionsgewichtung aus einer der Attentionsschichten dargestellt wird. Der Eintrag in der $i-ten$ Zeile und $j-ten$ Spalte gibt dabei den einfluss von $V_j$ auf $Attention(Q,K,V)_i$ an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3afaf5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:51.105968600Z",
     "start_time": "2023-11-13T16:26:51.054172600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75fb0e9fbfb487a902e8b4491d57456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.', conti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5347dd9ccfa245e28ece72a499d4d249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Attention berechnen', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27ae6f1c94a4647a9ca1c72745edbcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_attn = StoryTokenizer(reserved_tokens, vocab_path)\n",
    "attn_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_attn = widgets.Text(\n",
    "    value='A longer test sentence is more interesting, as it allows to see dependencies more clearly.',\n",
    "    description='Ihre Eingabe:',\n",
    "    continuous_update=False,  # updates value only when you finish typing or hit \"Enter\"\n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_attn = widgets.Button(description='Attention berechnen',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_attn = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def create_tokenized_embeddings():\n",
    "        tensor_input = tf.convert_to_tensor(input_widget_attn.value)                # Umwandelung des Textinputs in ein TensorFlow-Tensor \n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)    # Erstellung eines leeren TensorArrays für die spätere Ausgabe\n",
    "        if len(tensor_input.shape) == 0:                                            # Überprüft, ob der Eingabetensor im korrekten Format ist                                     \n",
    "            tensor_input = tensor_input[tf.newaxis]                                 # Falls nicht, wird eine Dimension hinzufügt \n",
    "\n",
    "    \n",
    "        tokenized_input = tokenizer.tokenize(tensor_input).to_tensor()              # Umwandlung des Textinputs in Tokens und anschließend in einen Tensor\n",
    "        input_without_eos = tokenized_input[:, :-1]\n",
    "        context = transformer.encode(input_without_eos, None)                       # Erstellung der Kontext-Vektoren vom Transformer-Modell\n",
    "\n",
    "        # Write the input tokens (excluding the last one) to the output array\n",
    "        for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "            output_array = output_array.write(i, value)\n",
    "\n",
    "        dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "        dec_out = transformer.decode(context, None, dec_input, None)\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_attn:\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 6\n",
    "        #output_widget_attn.clear_output()  # clear the previous output\n",
    "        create_tokenized_embeddings()\n",
    "        VisualWrapper.visualize_data(id='attention')\n",
    "        #VisualWrapper.n_vis_layers_per_class['MultiHeadedAttention'] = 1\n",
    "            \n",
    "\n",
    "button_widget_attn.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_attn, button_widget_attn, output_widget_attn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "162b450c",
   "metadata": {},
   "source": [
    "\n",
    "#### Multi-headed Attention\n",
    "\n",
    "In der Praxis hat sich bewährt parallel mehrere dieser Attention-Mechanismen durchzuführen. Dies erklärt sich dadurch, dass die Zusammenhänge zwischen verschiedenen Teilen eines Textes einer komplexen Struktur unterliegen. Nutzt man zu deren Darstellung einen einzigen Attention-Mechanismus konkurrieren viele verschiedene Zusammenhänge um Einfluss. Erlaubt man jedoch mehrere Attention-Mechanismen, spezialisieren sich diese auf bestimmte Zusammenhänge, so die Hoffnung.\n",
    "Dazu werden zu Beginn h gewichteten Matrizen \n",
    "\n",
    "$$W_i^Q, W_i^K, W_i^V i= 1,...,h$$ \n",
    "\n",
    "eingeführt. Diese erzeugen also h verschiedene Matrixtripel \n",
    "\n",
    "$$Q_i, K_i, V_i i = 1,...,h$$\n",
    "\n",
    "und somit ergeben sich h verschiedene Ausgaben \n",
    "\n",
    "$$H_i = Attention(Q_i, K_i, V_i) i=1,...,h.$$\n",
    "\n",
    "Diese werden nun zu einer einzigen Ausgabe zusammengeführt, indem wir \n",
    "\n",
    "$$MultiHeadAttention(Q_in,K_in,V_in) = Concat(H_1,..., H_h)W^O$$\n",
    "\n",
    "berechnen. Dabei ist $Concat(A_1,...,A_n)$ das hintereinanderschreiben mehrerer Matrizen und $W^O$ eine weitere trainierbare Matrix, die die verschiedenen Ausgaben $H_1,..., H,h$ gewichtet.\n",
    "Deshalb sehen wir in der obigen Ausgabe auch x verschiedene Matrizen, die $Score(Q_i,K_i)$ darstellen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fec2a5af",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "Das Maskieren des Inputs ist eine wichtige  Komponente der Transformerarchitektur. Beim Masking handelt es sich in Wirklichkeit um zwei Mechanismen, die zwar dieselbe Funktionsweise besitzen, aber sehr unterschiedliche Aufgaben in der Architektur übernehmen. Einerseits das Subsequent Masking, andererseits das Padding Masking. Das Padding Masking stellt jediglich sicher, dass nur Positionen mit Inhalt vom Transformer verarbeitet werden, während das Subsequent Masking dafür sorgt, dass der Decoder des Transformers autoregressiv ist. Das bedeutet, bei einer Vorhersage für eine Ausgabe an der Position $i$, soll das Modell nur Informationen aus den Eingabepositionen $1,…,i-1$ nutzen. Das Zusammenspiel der Beiden sehen Sie in der folgenden Abbildung (<a href=\"#fig:fig_masking\">Abbildung 4</a>)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e465d9",
   "metadata": {},
   "source": [
    "#### Padding Masking\n",
    "\n",
    "Das Padding Masking ist notwendig, da Transformer sequentielle Daten so verarbeiten, als ob sie eine fixe Länge $d_{model}$ hätten. Das geschieht deshalb, weil Transformer so lernen können jede Position der Ausgabelänge $d_{model}$ parallel vorherzusagen. \n",
    "\n",
    "Um während des Trainings auch Daten mit einer Länge größer oder kleiner $d_{model}$ zu nutzen, werden längere Sequenzen abgeschnitten und kürzere mit Nullen aufgefüllt. Diese Nullen müssen dann mit Hilfe von Padding Masking für das Training irrelevant gemacht werden. Das geschieht indem man alle Positionen die eine Null enthalten für das Modell auf $-inf$ setzt, sodass sie beim Gradient Descent nicht berücksichtigt werden.\n",
    "\n",
    "#### Subsequent Masking\n",
    "\n",
    "Das Subsequent Masking benutzt die gleich Technik des Werte auf $-inf$ setzen. Dabei wird aber nicht das mit irrelevanten Informationen angefüllte Ende des Eingabevektors auf Null gesetzt, sondern es werden diejenige Werte von $Score(Q, V)$ auf $-inf$ gesetzt, die einen Wert $V_j$, für die Ausgabe $i$ $Attention(Q,K,V)_i$ gewichten würden, obwohl $j>i$ gilt. Also es wird beschränkt, dass $Score(Q, V)$ nur diejenigen Werte von $V$ einbeziehen darf, die bei der Vorhersage für den i-ten Wert schon bekannt sind.\n",
    "\n",
    "Dass das relevant ist liegt daran, dass beim Training von Transformern der komplette Input gleichzeitig verwertet wird. Ein Satz wird als Ganzes vom Transformer verarbeitet und er erzeugt eine Vorhersage, welches Token an einer bestimmten Position in diesem Satz vorkommt. Dabei erhält er in der Eingabe aber schon die Information, welches Token an dieser Position tatsächlich steht, da er den kompletten Satz als Eingabe bekommen hat. Dies muss nun also innerhalb des Modells mit einer Maskierung wieder rückgängig gemacht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e3202-2841-4260-991f-e3919582cffa",
   "metadata": {},
   "source": [
    "<figure id=\"fig:fig_masking\" style=\"text-align: center; height: 700px;\">\n",
    "  <img src=\"./img/tf_masking.jpg\" alt=\"Abbildung 4: Zwei Varianten des Maskings im Transformer Modell.\"/>\n",
    "  <figcaption>Abbildung 4: Die zwei Varianten des Maskings im Transformer Modell</figcaption>\n",
    "</figure>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a0ea31",
   "metadata": {},
   "source": [
    "\n",
    "### Verschiedene Attention-Mechanismen\n",
    "\n",
    "In der Architektur werden verschiedene Attentionstypen unterschieden. Es gibt dabei zwei Variablen die beeinflussen, welche Art von Attention wir verwenden. Die erste Variable ist, woher die Eingaben $Q', K' und V'$ kommen. Die zweite Variable ist die Maskierung, die wir auf $Score(Q,K)$ anwenden.\n",
    "\n",
    "#### Self-Attention\n",
    "\n",
    "Wir sprechen von Self-Attention, wenn gilt \n",
    "\n",
    "$$Q'=K'=V'.$$ \n",
    "\n",
    "Wenn sich $Score(Q,K)$ also bildlich gesprochen daraus ergibt, welche Attention jede Position der Eingabe auf eine andere Position derselben Eingabe legt und diese Attention auf die Eingabe selbst angewandt wird.\n",
    "\n",
    "#### Cross-Attention\n",
    "\n",
    "Wie sprechen von Cross-Attention, wenn gilt \n",
    "\n",
    "$$K' = V'$$\n",
    "\n",
    "aber $Q'$ von diesen beiden Werten verschieden ist.\n",
    "Wenn sich $Score(Q,K)$ also daraus ergibt, welche Attention jede Position einer Eingabe $Q'$ auf die Positionen einer zweiten Eingabe $K'$ gibt und dieser Attentionsscore auf die zweite Eingabe angewandt wird.\n",
    "\n",
    "Dies ist zum Beispiel in Transformern der Fall, wenn $Q'$ sich aus der Ausgabe des Encoder ergibt und $K' = V'$ ein Zwischenergebnis des Decoders ist.\n",
    "\n",
    "#### Masked Attention\n",
    "\n",
    "Ein Fall von Masked-Attention liegt dann vor, wenn bestimmte Werte von $Score(Q,K)$ maskiert werden. Das ist zum Beispiel beim Subsequent Masking der Fall, hier wird $Score(Q,K)i,j = -inf$ gesetzt für alle Einträge $j>i$. Dadurch wird verhindert, dass die Ausgabe $Attention(Q,K,V)_i$ sich auf die Werte $V_j, j>i $ stützt. Zum Beispiel wird während des Trainings im Decoder dadurch verhindert, dass das Model lernt Informationen aus den zukünftigen Einträgen $V_j, j>i$ zu benutzen, um $V_i$ vorherzusagen. Man sieht gut in der Darstellung von $Score(Q,K)$, dass die Werte für $j>i$ dadurch meistens nahe bei $0$ liegen.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b92c7bb2",
   "metadata": {},
   "source": [
    "### Skalierung mit sqr(d_k)\n",
    "\n",
    "Dasselbe Problem des Vanishing Gradient könnte innerhalb der Attention-Funktion auftauchen, da hier $softmax(QK^T)V$ berechnet wird, das Skalarprodukt $QK^T$ mit $d_{model}$ skaliert und die Softmaxfunktion\n",
    "Das Problem des Vanishing Gradients kann auch innerhalb der Attention Function auftreten, da hier softmax(QK^T)V berechnet wird und das Skalarprodukt QK^T mit *d_model* skaliert und die Softmaxfunktion\n",
    "    \n",
    "$$sigma(z)_i = exp(z_i)/sum_j=1^N(exp(z_j)) für j=1,...,N $$\n",
    "\n",
    "somit leicht in einen saturierten Bereich mit extrem kleinen Gradienten gerät. Deshalb wird in der Umsetzung der Architektur $QK^T$ mit $sqr(d_{model})$ skaliert: \n",
    "$$softmax(QK^T/sqr(d_{model}))$$, \n",
    "um die Skalierung mit $d_{model}$ zu minimieren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77e5aaf9",
   "metadata": {},
   "source": [
    "## Vollständige Simulation\n",
    "\n",
    "Zuletzt findet sich hier nun noch eine Simulation des kompletten Inferenzvorgangs innerhalb eines Transformermodells. In dieser Simulation kann man noch einmal alle vorher beschriebenen Schritte nachvollziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3cf257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T16:26:51.105968600Z",
     "start_time": "2023-11-13T16:26:51.083068600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe6dafd590646518fd6c4d7a19c710c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Test sentence', continuous_update=False, description='Your input:', layout=Layout(margin='0px 0px …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac509e0d2acd43538b2bd7b7522af501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Run interactive inference', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8f7ddb08614687a53f9b527c5a7c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model.model, max_length=32)\n",
    "\n",
    "input_widget_inf = widgets.Text(\n",
    "    value='Test sentence',\n",
    "    description='Your input:',\n",
    "    continuous_update=False,  \n",
    "    layout = widgets.Layout(width='auto', margin='0px 0px 10px 0px')\n",
    ")\n",
    "\n",
    "button_widget_inf = widgets.Button(description='Run interactive inference',\n",
    "                               layout = widgets.Layout(width='auto'))\n",
    "\n",
    "output_widget_inf = widgets.Output(layout = widgets.Layout(width='auto'))\n",
    "\n",
    "def on_button_click(b):\n",
    "    with output_widget_inf:\n",
    "        output_widget_inf.clear_output()  \n",
    "        inference_model(input_widget_inf.value, interactive=True) \n",
    "        inference_model.print_results(visualisation=True)\n",
    "\n",
    "button_widget_inf.on_click(on_button_click)\n",
    "\n",
    "display(input_widget_inf, button_widget_inf, output_widget_inf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82dc7ff0",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "[1] Vaswani, A. et al. Attention Is All You Need. Preprint at http://arxiv.org/abs/1706.03762 (2017).\n",
    "\n",
    "[2] Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Computation 9, 1735–1780 (1997).\n",
    "\n",
    "[3] Bengio, Y., Simard, P. & Frasconi, P. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks 5, 157–166 (1994).\n",
    "\n",
    "[4] Cho, K., van Merrienboer, B., Bahdanau, D. & Bengio, Y. On the Properties of Neural Machine Translation: Encoder–Decoder Approaches. in Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation 103–111 (Association for Computational Linguistics, 2014). doi:10.3115/v1/W14-4012.\n",
    "\n",
    "[6] Kaplan, J. et al. Scaling Laws for Neural Language Models. Preprint at http://arxiv.org/abs/2001.08361 (2020).\n",
    "\n",
    "[5] Goodfellow, I., Bengio, Y. & Courville, A. Deep learning. (The MIT Press, 2016).\n",
    "\n",
    "[7] Radford, A. et al. Language Models are Unsupervised Multitask Learners. (2019).\n",
    "\n",
    "[8] Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Preprint at http://arxiv.org/abs/1810.04805 (2019).\n",
    "\n",
    "[9] Alammar, J. The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. https://jalammar.github.io/illustrated-transformer/ (2018).\n",
    "\n",
    "[10] Encoder-Decoder. Understanding The Model Architecture | by Naoki | Medium. https://naokishibuya.medium.com/transformers-encoder-decoder-434603d19e1.\n",
    "\n",
    "[11] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. (2014).\n",
    "\n",
    "[12] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[13] Gage, P. A New Algorithm for Data Compression. (1994).\n",
    "\n",
    "[14] Cho, K. et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. Preprint at http://arxiv.org/abs/1406.1078 (2014).\n",
    "\n",
    "[15] Bahdanau, D., Cho, K. & Bengio, Y. Neural Machine Translation by Jointly Learning to Align and Translate. Preprint at http://arxiv.org/abs/1409.0473 (2016).\n",
    "\n",
    "[16] OpenAI. GPT-4 Technical Report. Preprint at http://arxiv.org/abs/2303.08774 (2023).\n",
    "\n",
    "[17] Grefenstette, G. & Tapanainen, P. What is a word, What is a sentence? Problems of Tokenization.\n",
    "\n",
    "[18] Lin, Z. et al. A Structured Self-attentive Sentence Embedding. Preprint at http://arxiv.org/abs/1703.03130 (2017).\n",
    "\n",
    "[19] Schmidt, R. M. Recurrent Neural Networks (RNNs): A gentle Introduction and Overview. Preprint at http://arxiv.org/abs/1912.05911 (2019).\n",
    "\n",
    "[20] Ioffe, S. & Szegedy, C. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Preprint at http://arxiv.org/abs/1502.03167 (2015).\n",
    "\n",
    "[21] Ba, J. L., Kiros, J. R. & Hinton, G. E. Layer Normalization. Preprint at http://arxiv.org/abs/1607.06450 (2016).\n",
    "\n",
    "[22] He, K., Zhang, X., Ren, S. & Sun, J. Deep Residual Learning for Image Recognition. Preprint at http://arxiv.org/abs/1512.03385 (2015).\n",
    "\n",
    "[23] Tunstall, L., von Werra, L., Wolf, T. Natural Language Processing Mit Transformern. https://www.oreilly.com/library/view/natural-language-processing/9781098157081/.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
