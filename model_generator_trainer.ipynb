{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "import functools\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "\n",
    "# general modules\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# tensorflow modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "# necessary for visualization and user input\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set True, if code is run as jupyter notebook\n",
    "is_interactive_notebook = True\n",
    "\n",
    "# paths\n",
    "dataset_path = 'datasets\\\\corpus.txt'\n",
    "vocab_path = 'datasets\\\\vocab.txt'\n",
    "\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"\"\"Produce N identical layers\"\"\"\n",
    "    return [copy.deepcopy(module) for _ in range(N)]\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"\"\"Mask out subsequent positions.\"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerWrapper(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A wrapper for Keras layers, which allows to visualize data at each layer.\n",
    "\n",
    "    Attributes:\n",
    "        should_visualize (bool): Class attribute controlling whether visualization should occur.\n",
    "        layer (Layer): The Keras layer to be wrapped.\n",
    "        inputs (List[Tensor]): Inputs to the layer during calls.\n",
    "        outputs (List[Tensor]): Outputs of the layer during calls.\n",
    "        counter (int): Counter of layer calls.\n",
    "        visualize_on_calls (List[int]): List of call counts at which to visualize.\n",
    "        visualizations (List[Tuple[str, str]]): List of visualization modes and what to visualize.\n",
    "        visual_setter (bool): If True, this instance can change the should_visualize class variable.\n",
    "    \"\"\"\n",
    "    should_visualize = True  # class variable\n",
    "\n",
    "    def __init__(self, layer, visualize_on_calls=None, visualizations=None, visual_setter=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the LayerCallWrapper\n",
    "        Args:\n",
    "            layer (Layer): The Keras layer to be wrapped.\n",
    "            visualize_on_calls (List[int], optional): List of call counts at which to visualize. Defaults to empty list.\n",
    "            visualizations (List[Tuple[str, str]], optional): List of visualization modes and what to visualize. Defaults to an empty list.\n",
    "            visual_setter (bool, optional): If True, this instance can change the should_visualize class variable. Defaults to False.\n",
    "            **kwargs: Additional keyword arguments.u\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer = layer\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.counter = 0\n",
    "        self.visualize_on_calls = visualize_on_calls if visualize_on_calls else []\n",
    "        self.visualizations = visualizations if visualizations else []\n",
    "        self.visual_setter = visual_setter\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\"\n",
    "        Overloads the attributte access in order to access the wrapped layers attribute if not found in the wraper\n",
    "        \"\"\"\n",
    "        if 'layer' in self.__dict__:\n",
    "            return getattr(self.layer, attr)\n",
    "        else:\n",
    "            raise AttributeError(f\"{self.__class__.__name__} object has no attribute {attr}\")\n",
    "\n",
    "        \n",
    "    def call(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Overloads the call to the layer, allowing to capture inputs and outputs, and visualize if needed.\n",
    "\n",
    "        Args:\n",
    "            *args: Variable length argument list.\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output of the layer call.\n",
    "        \"\"\"\n",
    "        self.inputs.append([arg for arg in args])\n",
    "        output = self.layer(*args, **kwargs)\n",
    "        self.outputs.append(output.numpy())\n",
    "\n",
    "        # check for visualisation param of the instance and visualize or change class settings\n",
    "        if self.counter in self.visualize_on_calls:\n",
    "            if self.should_visualize:\n",
    "                self.visualize(self.visualizations)\n",
    "            if self.visual_setter:\n",
    "                LayerWrapper.should_visualize = True\n",
    "        else:\n",
    "            if self.visual_setter:\n",
    "                LayerWrapper.should_visualize = False\n",
    "\n",
    "        self.counter += 1\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def wait_for_user_input():\n",
    "        # waits for user input, if not jupyter notebook\n",
    "        # causes problems in jupyter\n",
    "        if not is_interactive_notebook:\n",
    "            proceed = input('Continue')\n",
    "\n",
    "    def visualize(self, visualizations):\n",
    "        for mode, what_to_output in visualizations:\n",
    "            if what_to_output == 'x':\n",
    "                data = self.inputs[-1]\n",
    "            elif what_to_output == 'y':\n",
    "                data = self.outputs[-1]\n",
    "            elif what_to_output == 'y-x':\n",
    "                data = [output - input for input, output in zip(self.inputs[-1], self.outputs[-1])]\n",
    "\n",
    "            if mode == 'mode1':\n",
    "                self.visualization_func_1(data)\n",
    "            elif mode == 'mode2':\n",
    "                self.visualization_func2(data)\n",
    "\n",
    "        self.wait_for_user_input()\n",
    "\n",
    "    def visualization_func_1(self, data):\n",
    "        # Assuming data[0] is a numpy array.\n",
    "        # If it's a ListWrapper or another list-like object, convert it to a numpy array.\n",
    "        array_data = np.array(data[0])\n",
    "        # If the array is 1D, reshape it into a 2D array with one column\n",
    "        if array_data.ndim == 1:\n",
    "            array_data = np.reshape(array_data, (-1, 1))\n",
    "        # Set the size of the plot (you can adjust the dimensions as needed)\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        # Use imshow to create a color-coded visualization and display it\n",
    "        plt.imshow(array_data, cmap='jet', aspect='auto')\n",
    "        plt.colorbar(label='Tensor Value')\n",
    "        plt.show()\n",
    "        \n",
    "    def visualization_func2(self, data):\n",
    "        # Your visualization code here\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes are built using the Keras Functional API, which provides more flexibility than the Sequential API for defining complex models. Each class is a subclass of tf.keras.layers.Layer, so they can be composed to build more complex layers or models. The call method of each class defines the computation that the layer performs.\n",
    "\n",
    "These classes are designed to be components of a larger transformer model. The model itself is typically composed of an encoder and a decoder, each of which is made up of a stack of identical layers. The layers themselves contain sublayers that perform operations such as self-attention, source attention (in the case of the decoder), and position-wise feed-forward networks. These operations are encapsulated within classes like `EncoderStack`, `DecoderStack`, `EncoderLayer`, `DecoderLayer`, and `PositionwiseFeedForward`. The layer norm and dropout are applied in `ResidualSublayer` for regularizing and speeding up the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Decoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `EncoderDecoder`:\n",
    "    - `__init__(self, encoder, decoder, enc_embed, dec_embed, generator)`: This initializes the EncoderDecoder instance. It takes in five arguments:\n",
    "        - `encoder`: The encoder layer to be used.\n",
    "        - `decoder`: The decoder layer to be used.\n",
    "        - `enc_embed`: The embedding layer for the encoder.\n",
    "        - `dec_embed`: The embedding layer for the decoder.\n",
    "        - `generator`: The final layer that generates the output tokens.\n",
    "    - `encode(self, inputs, pad_mask)`: This method is used to encode the inputs using the encoder layer. It takes in two arguments:\n",
    "        - `inputs`: The input tokens to be encoded.\n",
    "        - `pad_mask`: The mask indicating which tokens are padding.\n",
    "    - `decode(self, enc_input, pad_mask, inputs, subseq_mask)`: This method is used to decode the encoded inputs using the decoder layer. It takes in four arguments:\n",
    "        - `enc_input`: The encoded input from the encoder.\n",
    "        - `pad_mask`: The mask indicating which tokens are padding in the encoded input.\n",
    "        - `inputs`: The target tokens to be decoded.\n",
    "        - `subseq_mask`: The mask indicating which tokens in the target sequence should not be attended to.\n",
    "    - `call(self, enc_input, dec_input, pad_mask, subseq_mask)`: This method is used to perform the complete transformation from input tokens to output tokens. It takes in four arguments that are the same as those described in the `encode` and `decode` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(layers.Layer):\n",
    "    def __init__(self, encoder, decoder, enc_embed, dec_embed, generator):\n",
    "        super().__init__()\n",
    "        # modules\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.enc_embed = enc_embed\n",
    "        self.dec_embed = dec_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def encode(self, inputs, pad_mask):\n",
    "        return self.encoder(self.enc_embed(inputs), pad_mask)\n",
    "    \n",
    "    def decode(self, enc_input, pad_mask, inputs, subseq_mask):\n",
    "        return self.decoder(self.dec_embed(inputs), enc_input, pad_mask, subseq_mask)\n",
    "\n",
    "    def call(self, enc_input, dec_input, pad_mask, subseq_mask):\n",
    "        return self.decode(self.encode(enc_input, pad_mask), \n",
    "                           pad_mask,\n",
    "                           dec_input, \n",
    "                           subseq_mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. `LayerNorm`:\n",
    "    - `__init__(self, features, eps=1e-6)`: This initializes the LayerNorm instance. It takes in two arguments:\n",
    "        - `features`: The number of features in the input to be normalized.\n",
    "        - `eps`: A small number to add to the denominator for numerical stability.\n",
    "    - `call(self, x)`: This method is used to apply layer normalization to the input. It takes in one argument:\n",
    "        - `x`: The input to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(layers.Layer):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6) -> None:\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = self.add_weight(shape=(features,), initializer='ones')\n",
    "        self.b_2 = self.add_weight(shape=(features,), initializer='zeros')\n",
    "        self.eps = eps\n",
    "\n",
    "    def call(self, x):\n",
    "        mean, var = tf.nn.moments(x, axes=-1, keepdims=True)\n",
    "        std = tf.math.sqrt(var + self.eps)\n",
    "        return self.a_2 * (x - mean) / std + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. `ResidualSublayer`:\n",
    "    - `__init__(self, size, dropout)`: This initializes the ResidualSublayer instance. It takes in two arguments:\n",
    "        - `size`: The number of features in the input.\n",
    "        - `dropout`: The dropout rate to be applied after the sublayer.\n",
    "    - `call(self, x, sublayer)`: This method is used to apply a sublayer and a residual connection to the input. It takes in two arguments:\n",
    "        - `x`: The input to be transformed.\n",
    "        - `sublayer`: The sublayer to be applied to the input. This is expected to be a function or callable object that takes in the input and returns a tensor of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSublayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm. Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout) -> None:\n",
    "        super(ResidualSublayer, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Stack Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `EncoderStack`:\n",
    "    - `__init__(self, layer, N)`: This initializes the EncoderStack instance. It takes in two arguments and initializes two instance variables:\n",
    "        - `layer`: The type of layer to be used in the encoder stack. This should be a callable object that takes in the input and a mask and returns a tensor.\n",
    "        - `N`: The number of layers in the encoder stack.\n",
    "        - `self.layers` is a list of `N` layer clones of the type `layer`.\n",
    "        - `self.norm` is the norm layer, that is applied to the output of the `EncoderStack`.\n",
    "    - `call(self, x, mask)`: This method is used to pass the input through each layer in the encoder stack in turn. It takes in two arguments:\n",
    "        - `x`: The input to be processed by the encoder stack.\n",
    "        - `mask`: The mask indicating which tokens should not be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(layers.Layer):\n",
    "    \"\"\"\n",
    "    Core encoder is a stack of N=6 Layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(EncoderStack, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        \"\"\"\n",
    "        Pass the input (and mask) through each layer in turn\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. `EncoderLayer`:\n",
    "    - `__init__(self, size, self_attn, feed_forward, dropout)`: This initializes the EncoderLayer instance. It takes in four arguments:\n",
    "        - `size`: The number of features in the input.\n",
    "        - `self_attn`: The self-attention mechanism to be used in the encoder layer. This should be a callable object that takes in the input and a mask and returns a tensor.\n",
    "        - `feed_forward`: The feed-forward network to be used in the encoder layer. This should be a callable object that takes in the input and returns a tensor.\n",
    "        - `dropout`: The dropout rate to be applied after each sublayer.\n",
    "    - `call(self, x, mask)`: This method is used to pass the input through the self-attention mechanism and the feed-forward network. It takes in two arguments:\n",
    "        - `x`: The input to be processed by the encoder layer.\n",
    "        - `mask`: The mask indicating which tokens should not be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder is made up of a self-attention and a feed forward layer \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(ResidualSublayer(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Stack Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. `DecoderStack`:\n",
    "    - `__init__(self, layer, N)`: This initializes the DecoderStack instance. It takes in two arguments and initializes two instance variables:\n",
    "        - `layer`: The type of layer to be used in the decoder stack. This should be a callable object that takes in the input, the memory from the encoder, a source mask, and a target mask, and returns a tensor.\n",
    "        - `N`: The number of layers in the decoder stack.\n",
    "        - `self.layers` is a list of `N` layer clones of the type `layer`.\n",
    "        - `self.norm` is the norm layer, that is applied to the output of the `EncoderStack`.\n",
    "    - `call(self, x, memory, src_mask, tgt_mask)`: This method is used to pass the input through each layer in the decoder stack in turn. It takes in four arguments:\n",
    "        - `x`: The input to be processed by the decoder stack.\n",
    "        - `memory`: The output of the encoder, which serves as the memory for the decoder.\n",
    "        - `src_mask`: The mask indicating which tokens in the source sequence should not be attended to.\n",
    "        - `tgt_mask`: The mask indicating which tokens in the target sequence should not be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(layers.Layer):\n",
    "    \"\"\"\n",
    "    Generic N layer decoder with masking\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def call(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. `DecoderLayer`:\n",
    "    - `__init__(self, size, self_attn, src_attn, feed_forward, dropout)`: This initializes the DecoderLayer instance. It takes in five arguments:\n",
    "        - `size`: The number of features in the input.\n",
    "        - `self_attn`: The self-attention mechanism to be used in the decoder layer. This should be a callable object that takes in the input and a mask and returns a tensor.\n",
    "        - `src_attn`: The source attention mechanism to be used in the decoder layer. This should be a callable object that takes in the input, the memory from the encoder, and a mask, and returns a tensor.\n",
    "        - `feed_forward`: The feed-forward network to be used in the decoder layer. This should be a callable object that takes in the input and returns a tensor.\n",
    "        - `dropout`: The dropout rate to be applied after each sublayer.\n",
    "    - `call(self, x, memory, src_mask, tgt_mask)`: This method is used to pass the input through the self-attention mechanism, the source attention mechanism, and the feed-forward network. It takes in four arguments:\n",
    "        - `x`: The input to be processed by the decoder layer.\n",
    "        - `memory`: The output of the encoder, which serves as the memory for the decoder.\n",
    "        - `src_mask`: The mask indicating which tokens in the source sequence should not be attended to.\n",
    "        - `tgt_mask`: The mask indicating which tokens in the target sequence should not be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Decoder is made of self-attn, source-attn and feedforward layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(ResidualSublayer(size, dropout), 3)\n",
    "\n",
    "    def call(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sublayers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. `PositionwiseFeedForward`:\n",
    "    - `__init__(self, d_model, d_ff, dropout=0.1, *args, **kwargs)`: This initializes the PositionwiseFeedForward instance. It takes in three arguments and an optional set of arguments:\n",
    "        - `d_model`: The number of features in the input.\n",
    "        - `d_ff`: The number of features in the hidden layer of the feed-forward network.\n",
    "        - `dropout`: The dropout rate to be applied after the first layer of the feed-forward network.\n",
    "        - `*args, **kwargs`: Additional arguments that might be necessary for the parent class initialization.\n",
    "    - `call(self, x)`: This method is used to pass the input through the feed-forward network. It takes in one argument:\n",
    "        - `x`: The input to be processed by the feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(layers.Layer):\n",
    "    \"\"\"Implements FFN equation\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, *args, **kwargs):\n",
    "        super(PositionwiseFeedForward, self).__init__(*args, **kwargs)\n",
    "        self.w_1 = layers.Dense(d_ff)\n",
    "        self.w_2 = layers.Dense(d_model)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.w_2(self.dropout(tf.nn.relu(self.w_1(x))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. `Generator`:\n",
    "    - `__init__(self, vocab)`: This method initializes the Generator instance. It accepts one argument:\n",
    "        - `vocab`: The size of the vocabulary which will be the number of output units in the dense layer.\n",
    "    - `call(self, x)`: This method is used to pass the input through the generator. It takes in one argument:\n",
    "        - `x`: The input tensor to be processed by the generator. The method returns the log softmax of the output of the dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(layers.Layer):\n",
    "    \"\"\"\n",
    "    Define standard linear + softmax generation step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        super(Generator,self).__init__()\n",
    "        self.proj = layers.Dense(vocab)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.nn.log_softmax(self.proj(x), axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "10. `attention(query, key, value, mask=None, dropout=None)`:\n",
    "    - This is a function that computes the 'Scaled Dot Product Attention'. The arguments are as follows:\n",
    "        - `query`, `key`, `value`: These are the main inputs to the attention function.\n",
    "        - `mask`: Optional mask for the attention scores.\n",
    "        - `dropout`: Optional dropout rate to be applied to the attention scores.\n",
    "    - The function first scales the dot product of the query and key, applies the mask if provided, applies softmax to compute attention scores, applies dropout if provided, and then uses the attention scores to compute a weighted sum of the value inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "\n",
    "    d_k = query.shape[-1]\n",
    "    scores = tf.matmul(query, tf.transpose(key, perm=[0, 1, 3, 2])) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = tf.cast(mask, dtype=tf.bool)\n",
    "        scores = tf.where(mask, scores, tf.fill(tf.shape(scores), -1e9))\n",
    "    p_attn = tf.nn.softmax(scores, axis=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return tf.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. `MultiHeadedAttention`:\n",
    "    - `__init__(self, h, d_model, dropout=0.1)`: This initializes the MultiHeadedAttention instance. It takes in three arguments:\n",
    "        - `h`: The number of attention heads.\n",
    "        - `d_model`: The number of features in the input.\n",
    "        - `dropout`: The dropout rate to be applied after the softmax in the attention computation.\n",
    "    - `call(self, query, key, value, mask=None)`: This method is used to compute the multi-headed attention over the inputs. It takes in four arguments:\n",
    "        - `query`, `key`, `value`: These are the main inputs to the attention computation.\n",
    "        - `mask`: Optional mask for the attention scores.\n",
    "    - The method first computes the linear projections of the inputs, applies the attention function to the projected inputs, concatenates the outputs of the attention function across the attention heads, and then applies a final linear transformation to the concatenated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.query, self.key, self.value, self.linear = clones(layers.Dense(d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads\n",
    "            mask = tf.expand_dims(mask, 1)\n",
    "        nbatches = query.shape[0]\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            tf.transpose(tf.reshape(lin(x), [nbatches, -1 , self.h, self.d_k]), perm=[0, 2, 1, 3]) \n",
    "            for lin, x in zip(\n",
    "                [self.query, self.key, self.value], \n",
    "                (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = tf.reshape(tf.transpose(x ,perm=[0, 2, 1, 3]), (nbatches, -1, self.h * self.d_k))\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embedding Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. `positional_encoding(length, depth)`:\n",
    "    - This is a function that computes the positional encoding for a sequence of a given length and depth. The arguments are as follows:\n",
    "        - `length`: The length of the sequence for which positional encoding is to be computed.\n",
    "        - `depth`: The number of features in the input sequence.\n",
    "    - The function first computes the rates at which the angles should change across the positions and depths, then computes the angles at each position and depth, and finally applies sine to the angles at the even indices and cosine to the angles at the odd indices. The positional encoding for a position is thus a vector of these sine and cosine values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]   # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)               # (1, depth)\n",
    "    angle_rads  = positions * angle_rates           # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1\n",
    "        )\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. `PositionalEmbedding`:\n",
    "    - `__init__(self, vocab_size, d_model)`: This method initializes the PositionalEmbedding instance. It takes in two arguments:\n",
    "        - `vocab_size`: The size of the vocabulary, which will be the input dimension of the embedding layer.\n",
    "        - `d_model`: The number of features to be output by the embedding layer and the depth for the positional encoding.\n",
    "    - `call(self, x)`: This method is used to compute the positionally encoded embeddings of the inputs. It takes in one argument:\n",
    "        - `x`: The input tensor for which the embeddings are to be computed.\n",
    "    - The method first computes the embeddings of the inputs, scales the embeddings by the square root of `d_model`, and then adds the positional encoding to these scaled embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positional_encoding\n",
    "        x *=tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        #print(tf.shape(x))\n",
    "        #print(tf.shape(self.pos_encoding[tf.newaxis, :length, :]))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. `make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1)`:\n",
    "    - The `make_model` function constructs a Transformer model from given hyperparameters. It takes seven arguments:\n",
    "        - `src_vocab`: The size of the source vocabulary.\n",
    "        - `tgt_vocab`: The size of the target vocabulary.\n",
    "        - `N`(default=6): The number of layers in the Transformer's Encoder and Decoder stacks.\n",
    "        - `d_model`(default=512): The dimension of the model. It's the number of features in input and output.\n",
    "        - `d_ff`(default=2048): The number of features in the hidden layer of the feed-forward network.\n",
    "        - `h`(default=8): The number of attention heads in the MultiHeadedAttention mechanism.\n",
    "        - `dropout`(default=0.1): The dropout rate to be applied in several parts of the model.\n",
    "    - Inside this function, instances of `MultiHeadedAttention` and `PositionwiseFeedForward` are created. These instances are then deep-copied and used to construct the Encoder and Decoder stacks, additionally the PositionalEmbeddings, and the Generator are instantiated. All these parts are then assembled into a `EncoderDecoder` instance, which includes the complete Transformer model. If a module is wrapped with a `LayerWrapper` this is in order to visualize the output of this layer on sucessive calls. Look for the specific meaning of the `LayerWrapper` parameters in the definition of the class.\n",
    "    - Finally, the function returns the constructed model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    model = LayerWrapper(\n",
    "                EncoderDecoder(\n",
    "                EncoderStack(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "                DecoderStack(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "                PositionalEmbedding(src_vocab, d_model),\n",
    "                PositionalEmbedding(tgt_vocab, d_model),\n",
    "                LayerWrapper(Generator(tgt_vocab), visualize_on_calls=[1], visualizations=[('mode1', 'x')])\n",
    "            ),\n",
    "            visualize_on_calls=[1], visual_setter=True)\n",
    "\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    # model.build([(None, None), (None, None)])  # Explicit build call to initialize variables\n",
    "    # for w in model.trainable_variables:\n",
    "    #     if len(w.shape) > 1:\n",
    "    #         tf.keras.initializers.GlorotUniform()(w)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "\n",
    "    test_model.trainable = False\n",
    "    src = tf.constant([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], dtype=tf.int64)\n",
    "    src_mask = tf.ones((1, 1, 10), dtype=tf.float32)\n",
    "\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    ys = tf.zeros((1, 1), dtype=tf.int64)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(memory, src_mask, ys, subsequent_mask(ys.shape[1]))\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        next_word = tf.argmax(prob, axis=-1)[0]\n",
    "        ys = tf.concat([ys, tf.reshape(next_word, (1, 1))], axis=1)\n",
    "\n",
    "    print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "def run_tests():\n",
    "    for _ in range(10):\n",
    "        inference_test()\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_text_file):\n",
    "    return tf.data.TextLineDataset(filenames=dataset_text_file)\n",
    "\n",
    "def create_vocab(dataset):\n",
    "    bert_vocab_args=dict(\n",
    "        vocab_size = 8000,\n",
    "        reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"],\n",
    "        bert_tokenizer_params = dict(lower_case=True),\n",
    "        learn_params = {},\n",
    "    )\n",
    "\n",
    "    story_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "        dataset.batch(1000).prefetch(2),\n",
    "        **bert_vocab_args\n",
    "    )\n",
    "    return story_vocab\n",
    "\n",
    "def create_vocab_from_textdata(text_file=dataset_path):\n",
    "    dataset = load_dataset(text_file)\n",
    "    vocab = create_vocab(dataset)\n",
    "    return vocab\n",
    "\n",
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as file:\n",
    "        for token in vocab:\n",
    "            print(token, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2): # 2 = <blank>\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad)[:, np.newaxis, :]\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = tf.reduce_sum(tf.cast(self.tgt_y != pad, tf.int64))\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad)[:, np.newaxis, :]\n",
    "        tgt_mask = tf.logical_and(tgt_mask, subsequent_mask(tgt.shape[-1]))\n",
    "        return tgt_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_end(ragged):\n",
    "    START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "    END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "    count = ragged.bounding_shape()[0]\n",
    "    starts = tf.fill([count, 1], START)\n",
    "    ends = tf.fill([count, 1], END)\n",
    "    return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    bad_tokens = list(filter(lambda token: token != \"[UNK]\", reserved_tokens))\n",
    "    bad_tokens_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_tokens_re)\n",
    "    ragged_result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    result = tf.strings.reduce_join(ragged_result, separator=' ', axis=-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tf_text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:\n",
    "\n",
    "        # tokenize signature for a batch of strings\n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "        \n",
    "        # detokenize and lookup signature for:\n",
    "        # * Tensor with shape [tokens] and [batch, tokens]\n",
    "        # * RaggedTensor with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        \n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        \n",
    "\n",
    "        # get_* methods take no argument\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        encoded = self.tokenizer.tokenize(strings)\n",
    "        merged_enc = encoded.merge_dims(-2, -1)\n",
    "        merg_enc_start_end = add_start_end(merged_enc)\n",
    "        return merg_enc_start_end\n",
    "    \n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "    \n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "    \n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "    \n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "    \n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0 # Steps in the current epoch\n",
    "    accum_step: int = 0 # Number of gradient accumulation steps\n",
    "    samples: int = 0 # total number of examples used\n",
    "    tokens: int = 0 # total number of tokens processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_simu_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
