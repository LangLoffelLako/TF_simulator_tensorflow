{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I finally understood, that during traingin each next token is calculated simultaneously for the whole sentence, such that no sequential processing is needed. That is of course redundant for inference. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luis\\miniconda3\\envs\\tf_simu_tensor\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\Luis\\miniconda3\\envs\\tf_simu_tensor\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\Luis\\miniconda3\\envs\\tf_simu_tensor\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "c:\\Users\\Luis\\miniconda3\\envs\\tf_simu_tensor\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# logging and decorators\n",
    "import logging as log\n",
    "import functools\n",
    "import time\n",
    "\n",
    "# system tools\n",
    "import pathlib\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# general modules\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# tensorflow modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "# necessary for visualization and user input\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging settings\n",
    "log.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "        # log.INFO for normal run\n",
    "    level=log.INFO,\n",
    "        # log.DEBUG for diagnostics\n",
    "    # level=log.DEBUG,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "log_enabled = True\n",
    "\n",
    "# Set True, if code is run as jupyter notebook\n",
    "is_interactive_notebook = True\n",
    "\n",
    "# paths\n",
    "corpus_file_path = 'datasets\\\\corpus'\n",
    "bco_file_path = \"datasets\\\\bookscorpusopen\\\\epubtxt\"\n",
    "tight_fit_512_dataset_path = 'datasets\\\\tight_fit'\n",
    "vocab_path = 'datasets\\\\vocab.txt'\n",
    "\n",
    "# tokenizer\n",
    "tokenizer_name = 'story_corpus_tokenizer'\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dec(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            if log_enabled and not kwargs.get('training') is None:\n",
    "                start_time = time.time()\n",
    "                class_name = func.__qualname__.split('.')[0]\n",
    "                log.info(f'{class_name}.{func.__name__} called')\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "        finally:\n",
    "            if log_enabled and not kwargs.get('training') is None:\n",
    "                duration = time.time() - start_time\n",
    "                log.info(f'{class_name}.{func.__name__} executed')\n",
    "    return wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "def clones(module, N):\n",
    "    \"\"\"Produce N identical layers\"\"\"\n",
    "    return [copy.deepcopy(module) for _ in range(N)]\n",
    "\n",
    "def clones_alt(layer_class, N, **kwargs):\n",
    "    \"\"\"Produce N identical layers\"\"\"\n",
    "    return [layer_class(**kwargs) for layer_number in range(N)]\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"\"\"Mask out subsequent positions.\"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualWrapper():\n",
    "    should_visualize = False\n",
    "    instances = []\n",
    "\n",
    "    def __init__(self, vis_on_count=None, enabler=False):\n",
    "        self.counter = 0\n",
    "        self.vis_on_count = vis_on_count if vis_on_count else []\n",
    "        self.enabler = enabler\n",
    "        VisualWrapper.instances.append(self)\n",
    "\n",
    "    def visualize_data(self, data_x, mode, training, text=None, data_y=None, vis_diff=False):\n",
    "        # TODO: data_y and vis_diff are there to be used for calculating the \n",
    "        if training is False:\n",
    "            # check for visualisation param of the instance and visualize or change class settings\n",
    "            if self.counter in self.vis_on_count:  \n",
    "                if self.should_visualize:   \n",
    "                    tf.print(text)  \n",
    "                    self.choose_func(mode)(data_x)\n",
    "                if self.enabler:\n",
    "                    VisualWrapper.should_visualize = True\n",
    "            else:\n",
    "                if self.enabler:\n",
    "                    VisualWrapper.should_visualize = False\n",
    "            self.counter += 1\n",
    "\n",
    "    # @log_dec\n",
    "    def choose_func(self, mode):\n",
    "        if mode == 'color_bar':\n",
    "            return lambda x: self.color_bar(x)\n",
    "        elif mode == 'print':\n",
    "            return lambda x: self.print_data(x)\n",
    "        elif mode == 'reduce_dim':\n",
    "            return lambda x: self.reduce_dim(x)\n",
    "        else:\n",
    "            return do_nothing\n",
    "\n",
    "    def color_bar(self, tensor):\n",
    "        x_label = 'Positions'\n",
    "        y_label = 'Embbeddings'\n",
    "        # Assuming data[0] is a numpy array.\n",
    "        # If it's a ListWrapper or another list-like object, convert it to a numpy array.\n",
    "        tensor = np.array(tensor[0])\n",
    "        # If the array is 1D, reshape it into a 2D array with one column\n",
    "        if tensor.ndim == 1:\n",
    "            tensor = np.reshape(tensor, (-1, 1))\n",
    "        # Set the size of the plot (you can adjust the dimensions as needed)\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        # Use imshow to create a color-coded visualization\n",
    "        plt.imshow(tensor, cmap='jet', aspect='auto')\n",
    "        plt.colorbar(label='Tensor Value')\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    def print_data(self,data):\n",
    "        tf.print(data)\n",
    "\n",
    "    def reduce_dim(self, tensor):\n",
    "\n",
    "        array = np.squeeze(tensor, axis=0)\n",
    "\n",
    "        # array = tensor.numpy().reshape(tensor.shape[0], -1)\n",
    "\n",
    "        scaled_array = array / np.min(np.abs(array))\n",
    "\n",
    "        # tf.print('scaled_numpy_array', scaled_array)\n",
    "\n",
    "        # TODO: PCA must be trained\n",
    "        pca = PCA(n_components=3)\n",
    "        tsne = TSNE(n_components=3, random_state=42)\n",
    "        reducer = umap.UMAP(n_components=3, random_state=42)\n",
    "        reduced_array = pca.fit_transform(scaled_array)\n",
    "\n",
    "        # Create a new figure and axes\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        \n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Create a scatter plot of the UMAP embeddings\n",
    "        # Color each point by the sample's label\n",
    "        # scatter = ax.scatter(reduced_array[:, 0], reduced_array[:, 1], reduced_array[:, 2], s=50)\n",
    "        # Optional: include a colorbar if you have labels\n",
    "        # plt.colorbar(scatter)\n",
    "\n",
    "        # Create a quiver plot to visualize each point as a vector from the origin\n",
    "        ax.quiver(0, 0, 0, reduced_array[:, 0], reduced_array[:, 1], reduced_array[:, 2], arrow_length_ratio=0.1)\n",
    "       # ax.quiver(0, 0, reduced_array[:, 0], reduced_array[:, 1], arrow_length_ratio=0.1)\n",
    "\n",
    "        # Add some helpful labels\n",
    "        ax.set_xlabel('Component 1')\n",
    "        ax.set_ylabel('Component 2')\n",
    "        ax.set_zlabel('Component 3')\n",
    "        ax.set_title('Embeddings')\n",
    "\n",
    "        boundaries = np.max(reduced_array)\n",
    "\n",
    "        # Set the x and y axis limits\n",
    "        ax.set_xlim([-boundaries, boundaries])\n",
    "        ax.set_ylim([-boundaries, boundaries])\n",
    "        ax.set_zlim([-boundaries, boundaries])\n",
    "\n",
    "\n",
    "        # Set aspect of the plot to equal to ensure that the vectors are displayed correctly\n",
    "        # ax.set_aspect('equal')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def reset_counter(cls):\n",
    "        for instance in cls.instances:\n",
    "            instance.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerWrapper(layers.Layer):\n",
    "    \"\"\"\n",
    "    A wrapper for Keras layers, which allows to visualize data at each layer.\n",
    "\n",
    "    Attributes:\n",
    "        should_visualize (bool): Class attribute controlling whether visualization should occur.\n",
    "        layer (Layer): The Keras layer to be wrapped.\n",
    "        inputs (List[Tensor]): Inputs to the layer during calls.\n",
    "        outputs (List[Tensor]): Outputs of the layer during calls.\n",
    "        counter (int): Counter of layer calls.\n",
    "        visualize_on_calls (List[int]): List of call counts at which to visualize.\n",
    "        visualizations (List[Tuple[str, str]]): List of visualization modes and what to visualize.\n",
    "        visual_setter (bool): If True, this instance can change the should_visualize class variable.\n",
    "    \"\"\"\n",
    "    should_visualize = True  # class variable\n",
    "\n",
    "    def __init__(self, layer, visualize_on_calls=None, visualizations=None, visual_setter=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the LayerCallWrapper\n",
    "        Args:\n",
    "            layer (Layer): The Keras layer to be wrapped.\n",
    "            visualize_on_calls (List[int], optional): List of call counts at which to visualize. Defaults to empty list.\n",
    "            visualizations (List[Tuple[str, str]], optional): List of visualization modes and what to visualize. Defaults to an empty list.\n",
    "            visual_setter (bool, optional): If True, this instance can change the should_visualize class variable. Defaults to False.\n",
    "            **kwargs: Additional keyword arguments.u\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer = layer\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        self.counter = 0\n",
    "        self.visualize_on_calls = visualize_on_calls if visualize_on_calls else []\n",
    "        self.visualizations = visualizations if visualizations else []\n",
    "        self.visual_setter = visual_setter\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\"\n",
    "        Overloads the attributte access in order to access the wrapped layers attribute if not found in the wraper\n",
    "        \"\"\"\n",
    "        if 'layer' in self.__dict__:\n",
    "            return getattr(self.layer, attr)\n",
    "        else:\n",
    "            raise AttributeError(f\"{self.__class__.__name__} object has no attribute {attr}\")\n",
    " \n",
    "    def call(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Overloads the call to the layer, allowing to capture inputs and outputs, and visualize if needed.\n",
    "\n",
    "        Args:\n",
    "            *args: Variable length argument list.\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output of the layer call.\n",
    "        \"\"\"\n",
    "\n",
    "        self.inputs.append([arg for arg in args])\n",
    "        output = self.layer(*args, **kwargs)\n",
    "\n",
    "        tf.print(f'shape of {self.__class__.__name__}.{self.layer.__class__.__name__} out', tf.shape(output))\n",
    "        tf.print(f'dtype of {self.__class__.__name__}.{self.layer.__class__.__name__} out', output.dtype)\n",
    "        \n",
    "        if kwargs.get('training') is False:\n",
    "            self.outputs.append(output)\n",
    "\n",
    "            # check for visualisation param of the instance and visualize or change class settings\n",
    "            if self.counter in self.visualize_on_calls:\n",
    "                if self.should_visualize:\n",
    "                    self.visualize(self.visualizations)\n",
    "                if self.visual_setter:\n",
    "                    LayerWrapper.should_visualize = True\n",
    "            else:\n",
    "                if self.visual_setter:\n",
    "                    LayerWrapper.should_visualize = False\n",
    "\n",
    "            self.counter += 1\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def wait_for_user_input():\n",
    "        # waits for user input, if not jupyter notebook\n",
    "        # causes problems in jupyter\n",
    "        if not is_interactive_notebook:\n",
    "            proceed = input('Continue')\n",
    "\n",
    "    # @log_dec\n",
    "    def visualize(self, visualizations):\n",
    "        for mode, what_to_output in visualizations:\n",
    "            if what_to_output == 'x':\n",
    "                data = self.inputs[-1]\n",
    "            elif what_to_output == 'y':\n",
    "                data = self.outputs[-1]\n",
    "            elif what_to_output == 'y-x':\n",
    "                data = [output - input for input, output in zip(self.inputs[-1], self.outputs[-1])]\n",
    "\n",
    "            continue\n",
    "\n",
    "            if mode == 'mode1':\n",
    "                self.visualization_func_1(data)\n",
    "            elif mode == 'mode2':\n",
    "                self.visualization_func2(data)\n",
    "\n",
    "        self.wait_for_user_input()\n",
    "\n",
    "    def visualization_func_1(self, data):\n",
    "        # Assuming data[0] is a numpy array.\n",
    "        # If it's a ListWrapper or another list-like object, convert it to a numpy array.\n",
    "        array_data = np.array(data[0])\n",
    "        # If the array is 1D, reshape it into a 2D array with one column\n",
    "        if array_data.ndim == 1:\n",
    "            array_data = np.reshape(array_data, (-1, 1))\n",
    "        # Set the size of the plot (you can adjust the dimensions as needed)\n",
    "        plt.figure(figsize=(10, 2))\n",
    "        # Use imshow to create a color-coded visualization and display it\n",
    "        plt.imshow(array_data, cmap='jet', aspect='auto')\n",
    "        plt.colorbar(label='Tensor Value')\n",
    "        plt.show()\n",
    "        \n",
    "    def visualization_func2(self, data):\n",
    "        # Your visualization code here\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes are built using the Keras Functional API, which provides more flexibility than the Sequential API for defining complex models. Each class is a subclass of tf.keras.layers.Layer, so they can be composed to build more complex layers or models. The call method of each class defines the computation that the layer performs.\n",
    "\n",
    "These classes are designed to be components of a larger transformer model. The model itself is typically composed of an encoder and a decoder, each of which is made up of a stack of identical layers. The layers themselves contain sublayers that perform operations such as self-attention, source attention (in the case of the decoder), and position-wise feed-forward networks. These operations are encapsulated within classes like `EncoderStack`, `DecoderStack`, `EncoderLayer`, `DecoderLayer`, and `PositionwiseFeedForward`. The layer norm and dropout are applied in `ResidualSublayer` for regularizing and speeding up the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Decoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `EncoderDecoder`:\n",
    "    - `__init__(self, encoder, decoder, enc_embed, dec_embed, generator)`: This initializes the EncoderDecoder instance. It takes in five arguments:\n",
    "        - `encoder`: The encoder layer to be used.\n",
    "        - `decoder`: The decoder layer to be used.\n",
    "        - `enc_embed`: The embedding layer for the encoder.\n",
    "        - `dec_embed`: The embedding layer for the decoder.\n",
    "        - `generator`: The final layer that generates the output tokens.\n",
    "    - `encode(self, inputs, pad_mask)`: This method is used to encode the inputs using the encoder layer. It takes in two arguments:\n",
    "        - `inputs`: The input tokens to be encoded.\n",
    "        - `pad_mask`: The mask indicating which tokens are padding.\n",
    "    - `decode(self, enc_input, pad_mask, inputs, subseq_mask)`: This method is used to decode the encoded inputs using the decoder layer. It takes in four arguments:\n",
    "        - `enc_input`: The encoded input from the encoder.\n",
    "        - `pad_mask`: The mask indicating which tokens are padding in the encoded input.\n",
    "        - `inputs`: The target tokens to be decoded.\n",
    "        - `subseq_mask`: The mask indicating which tokens in the target sequence should not be attended to.\n",
    "    - `call(self, enc_input, dec_input, pad_mask, subseq_mask)`: This method is used to perform the complete transformation from input tokens to output tokens. It takes in four arguments that are the same as those described in the `encode` and `decode` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @LayerWrapperDecorator(visualize_on_calls=[1], visual_setter=True)\n",
    "class EncoderDecoder(tf.keras.Model, VisualWrapper):\n",
    "    def __init__(self, encoder_stack, decoder_stack, enc_embed, dec_embed, generator):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0], enabler=False)\n",
    "        # modules\n",
    "        self.encoder_stack = encoder_stack\n",
    "        self.decoder_stack = decoder_stack\n",
    "        self.enc_embed = enc_embed\n",
    "        self.dec_embed = dec_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    # @log_dec\n",
    "    def encode(self, inputs, pad_mask, training=None):\n",
    "        return self.encoder_stack(self.enc_embed(inputs), pad_mask, training=training)\n",
    "    \n",
    "    # @log_dec\n",
    "    def decode(self, enc_input, pad_mask, inputs, subseq_mask, training=None):\n",
    "        return self.decoder_stack(self.dec_embed(inputs), enc_input, pad_mask, subseq_mask, training=training)\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        enc_input, dec_input, pad_mask, subseq_mask = inputs\n",
    "\n",
    "        if not training: # Additional training = False check, such that calculations for execution are not conducted unless not training\n",
    "            input_emb_enc = self.enc_embed(enc_input)\n",
    "            input_emb_dec = self.dec_embed(dec_input)\n",
    "            self.visualize_data(input_emb_enc- input_emb_dec, mode='color_bar', training=training, text='The difference between the enc_emb and the dec_emb')\n",
    "\n",
    "        return self.decode(self.encode(enc_input, pad_mask, training), \n",
    "                           pad_mask,\n",
    "                           dec_input, \n",
    "                           subseq_mask, training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. `LayerNorm`:\n",
    "    - `__init__(self, features, eps=1e-6)`: This initializes the LayerNorm instance. It takes in two arguments:\n",
    "        - `features`: The number of features in the input to be normalized.\n",
    "        - `eps`: A small number to add to the denominator for numerical stability.\n",
    "    - `call(self, x)`: This method is used to apply layer normalization to the input. It takes in one argument:\n",
    "        - `x`: The input to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(layers.Layer, VisualWrapper):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6) -> None:\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.a_2 = self.add_weight(shape=(features,), initializer='ones', name=self.name + \"a_2\")\n",
    "        self.b_2 = self.add_weight(shape=(features,), initializer='zeros', name=self.name + \"b_2\")\n",
    "        self.eps = eps\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, x):\n",
    "        mean, var = tf.nn.moments(x, axes=-1, keepdims=True)\n",
    "        std = tf.math.sqrt(var + self.eps)\n",
    "        return self.a_2 * (x - mean) / std + self.b_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. `ResidualSublayer`:\n",
    "    - `__init__(self, size, dropout)`: This initializes the ResidualSublayer instance. It takes in two arguments:\n",
    "        - `size`: The number of features in the input.\n",
    "        - `dropout`: The dropout rate to be applied after the sublayer.\n",
    "    - `call(self, x, sublayer)`: This method is used to apply a sublayer and a residual connection to the input. It takes in two arguments:\n",
    "        - `x`: The input to be transformed.\n",
    "        - `sublayer`: The sublayer to be applied to the input. This is expected to be a function or callable object that takes in the input and returns a tensor of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualSublayer(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm. Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout) -> None:\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, x, sublayer, training=None):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        sublayer_out = sublayer(self.norm(x))\n",
    "        return x + self.dropout(sublayer_out, training=training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Stack Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. `EncoderStack`:\n",
    "    - `__init__(self, layer, N)`: This initializes the EncoderStack instance. It takes in two arguments and initializes two instance variables:\n",
    "        - `layer`: The type of layer to be used in the encoder stack. This should be a callable object that takes in the input and a mask and returns a tensor.\n",
    "        - `N`: The number of layers in the encoder stack.\n",
    "        - `self.layers` is a list of `N` layer clones of the type `layer`.\n",
    "        - `self.norm` is the norm layer, that is applied to the output of the `EncoderStack`.\n",
    "    - `call(self, x, mask)`: This method is used to pass the input through each layer in the encoder stack in turn. It takes in two arguments:\n",
    "        - `x`: The input to be processed by the encoder stack.\n",
    "        - `mask`: The mask indicating which tokens should not be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    Core encoder is a stack of N=6 Layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N, size, **kwargs):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.layers = clones_alt(layer, N, size=size, **kwargs)\n",
    "        self.norm = LayerNorm(size)\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, x, mask, training=None):\n",
    "        \"\"\"\n",
    "        Pass the input (and mask) through each layer in turn\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask, training=training)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. `EncoderLayer`:\n",
    "    - `__init__(self, size, self_attn, feed_forward, dropout)`: This initializes the EncoderLayer instance. It takes in four arguments:\n",
    "        - `size`: The number of features in the input.\n",
    "        - `self_attn`: The self-attention mechanism to be used in the encoder layer. This should be a callable object that takes in the input and a mask and returns a tensor.\n",
    "        - `feed_forward`: The feed-forward network to be used in the encoder layer. This should be a callable object that takes in the input and returns a tensor.\n",
    "        - `dropout`: The dropout rate to be applied after each sublayer.\n",
    "    - `call(self, x, mask)`: This method is used to pass the input through the self-attention mechanism and the feed-forward network. It takes in two arguments:\n",
    "        - `x`: The input to be processed by the encoder layer.\n",
    "        - `mask`: The mask indicating which tokens should not be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    Encoder is made up of a self-attention and a feed forward layer \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones_alt(ResidualSublayer, N=2, size=size, dropout=dropout)\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, x, mask, training=None):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask, training=training), training=training)\n",
    "        return self.sublayer[1](x, lambda x: self.feed_forward(x, training=training), training=training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Stack Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. `DecoderStack`:\n",
    "    - `__init__(self, layer, N)`: This initializes the DecoderStack instance. It takes in two arguments and initializes two instance variables:\n",
    "        - `layer`: The type of layer to be used in the decoder stack. This should be a callable object that takes in the input, the memory from the encoder, a source mask, and a target mask, and returns a tensor.\n",
    "        - `N`: The number of layers in the decoder stack.\n",
    "        - `self.layers` is a list of `N` layer clones of the type `layer`.\n",
    "        - `self.norm` is the norm layer, that is applied to the output of the `EncoderStack`.\n",
    "    - `call(self, x, memory, src_mask, tgt_mask)`: This method is used to pass the input through each layer in the decoder stack in turn. It takes in four arguments:\n",
    "        - `x`: The input to be processed by the decoder stack.\n",
    "        - `memory`: The output of the encoder, which serves as the memory for the decoder.\n",
    "        - `src_mask`: The mask indicating which tokens in the source sequence should not be attended to.\n",
    "        - `tgt_mask`: The mask indicating which tokens in the target sequence should not be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    Generic N layer decoder with masking\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, N, size, **kwargs):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.layers = clones_alt(layer, N, size=size, **kwargs)\n",
    "        self.norm = LayerNorm(size)\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, x, memory, src_mask, tgt_mask, training=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask, training=training)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. `DecoderLayer`:\n",
    "    - `__init__(self, size, self_attn, src_attn, feed_forward, dropout)`: This initializes the DecoderLayer instance. It takes in five arguments:\n",
    "        - `size`: The number of features in the input.\n",
    "        - `self_attn`: The self-attention mechanism to be used in the decoder layer. This should be a callable object that takes in the input and a mask and returns a tensor.\n",
    "        - `src_attn`: The source attention mechanism to be used in the decoder layer. This should be a callable object that takes in the input, the memory from the encoder, and a mask, and returns a tensor.\n",
    "        - `feed_forward`: The feed-forward network to be used in the decoder layer. This should be a callable object that takes in the input and returns a tensor.\n",
    "        - `dropout`: The dropout rate to be applied after each sublayer.\n",
    "    - `call(self, x, memory, src_mask, tgt_mask)`: This method is used to pass the input through the self-attention mechanism, the source attention mechanism, and the feed-forward network. It takes in four arguments:\n",
    "        - `x`: The input to be processed by the decoder layer.\n",
    "        - `memory`: The output of the encoder, which serves as the memory for the decoder.\n",
    "        - `src_mask`: The mask indicating which tokens in the source sequence should not be attended to.\n",
    "        - `tgt_mask`: The mask indicating which tokens in the target sequence should not be attended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    Decoder is made of self-attn, source-attn and feedforward layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones_alt(ResidualSublayer, N=3, size=size, dropout=dropout)\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, x, memory, src_mask, tgt_mask, training=None):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sublayers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. `PositionwiseFeedForward`:\n",
    "    - `__init__(self, d_model, d_ff, dropout=0.1, *args, **kwargs)`: This initializes the PositionwiseFeedForward instance. It takes in three arguments and an optional set of arguments:\n",
    "        - `d_model`: The number of features in the input.\n",
    "        - `d_ff`: The number of features in the hidden layer of the feed-forward network.\n",
    "        - `dropout`: The dropout rate to be applied after the first layer of the feed-forward network.\n",
    "        - `*args, **kwargs`: Additional arguments that might be necessary for the parent class initialization.\n",
    "    - `call(self, x)`: This method is used to pass the input through the feed-forward network. It takes in one argument:\n",
    "        - `x`: The input to be processed by the feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(layers.Layer, VisualWrapper):\n",
    "    \"\"\"Implements FFN equation\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.w_1 = layers.Dense(d_ff)\n",
    "        self.w_2 = layers.Dense(d_model)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, x, training=None):\n",
    "        return self.w_2(self.dropout(tf.nn.relu(self.w_1(x)), training=training))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. `Generator`:\n",
    "    - `__init__(self, vocab)`: This method initializes the Generator instance. It accepts one argument:\n",
    "        - `vocab`: The size of the vocabulary which will be the number of output units in the dense layer.\n",
    "    - `call(self, x)`: This method is used to pass the input through the generator. It takes in one argument:\n",
    "        - `x`: The input tensor to be processed by the generator. The method returns the log softmax of the output of the dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @LayerWrapperDecorator(visualize_on_calls=[1], visualizations=[('mode1', 'x')])\n",
    "class Generator(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    Define standard linear + softmax generation step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.proj = layers.Dense(vocab)\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, x, training=None):\n",
    "        result = tf.nn.log_softmax(self.proj(x), axis=-1)\n",
    "        self.visualize_data(result, \n",
    "                            'color_bar', \n",
    "                            text=f\"This is the data from {self.__class__.__name__}\", \n",
    "                            training=training)\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "10. `attention(query, key, value, mask=None, dropout=None)`:\n",
    "    - This is a function that computes the 'Scaled Dot Product Attention'. The arguments are as follows:\n",
    "        - `query`, `key`, `value`: These are the main inputs to the attention function.\n",
    "        - `mask`: Optional mask for the attention scores.\n",
    "        - `dropout`: Optional dropout rate to be applied to the attention scores.\n",
    "    - The function first scales the dot product of the query and key, applies the mask if provided, applies softmax to compute attention scores, applies dropout if provided, and then uses the attention scores to compute a weighted sum of the value inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @log_dec\n",
    "def attention(query, key, value, mask=None, dropout=None, training=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "\n",
    "    d_k = query.shape[-1]\n",
    "    scores = tf.matmul(query, tf.transpose(key, perm=[0, 1, 3, 2])) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = tf.cast(mask, dtype=tf.bool)\n",
    "        scores = tf.where(mask, scores, tf.fill(tf.shape(scores), -1e9))\n",
    "    p_attn = tf.nn.softmax(scores, axis=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn, training=training)\n",
    "    return tf.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. `MultiHeadedAttention`:\n",
    "    - `__init__(self, h, d_model, dropout=0.1)`: This initializes the MultiHeadedAttention instance. It takes in three arguments:\n",
    "        - `h`: The number of attention heads.\n",
    "        - `d_model`: The number of features in the input.\n",
    "        - `dropout`: The dropout rate to be applied after the softmax in the attention computation.\n",
    "    - `call(self, query, key, value, mask=None)`: This method is used to compute the multi-headed attention over the inputs. It takes in four arguments:\n",
    "        - `query`, `key`, `value`: These are the main inputs to the attention computation.\n",
    "        - `mask`: Optional mask for the attention scores.\n",
    "    - The method first computes the linear projections of the inputs, applies the attention function to the projected inputs, concatenates the outputs of the attention function across the attention heads, and then applies a final linear transformation to the concatenated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(layers.Layer, VisualWrapper):\n",
    "    \n",
    "    def __init__(self, h, d_model, dropout=0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.query, self.key, self.value, self.linear = clones_alt(layers.Dense, N=4, units=d_model)\n",
    "        self.attn = None\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, query, key, value, mask=None, training=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads\n",
    "            mask = tf.expand_dims(mask, 1)\n",
    "\n",
    "        nbatches = tf.shape(query)[0]\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            tf.transpose(tf.reshape(lin(x), [nbatches, -1 , self.h, self.d_k]), perm=[0, 2, 1, 3]) \n",
    "            for lin, x in zip(\n",
    "                [self.query, self.key, self.value], \n",
    "                (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout, training=training)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = tf.reshape(tf.transpose(x ,perm=[0, 2, 1, 3]), (nbatches, -1, self.h * self.d_k))\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embedding Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. `positional_encoding(length, depth)`:\n",
    "    - This is a function that computes the positional encoding for a sequence of a given length and depth. The arguments are as follows:\n",
    "        - `length`: The length of the sequence for which positional encoding is to be computed.\n",
    "        - `depth`: The number of features in the input sequence.\n",
    "    - The function first computes the rates at which the angles should change across the positions and depths, then computes the angles at each position and depth, and finally applies sine to the angles at the even indices and cosine to the angles at the odd indices. The positional encoding for a position is thus a vector of these sine and cosine values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @log_dec\n",
    "def positional_encoding(length, depth):\n",
    "    depth = depth / 2\n",
    "\n",
    "    positions = np.arange(length)[:, np.newaxis]   # (seq, 1)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth  # (1, depth)\n",
    "\n",
    "    angle_rates = 1 / (10000**depths)               # (1, depth)\n",
    "    angle_rads  = positions * angle_rates           # (pos, depth)\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1\n",
    "        )\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. `PositionalEmbedding`:\n",
    "    - `__init__(self, vocab_size, d_model)`: This method initializes the PositionalEmbedding instance. It takes in two arguments:\n",
    "        - `vocab_size`: The size of the vocabulary, which will be the input dimension of the embedding layer.\n",
    "        - `d_model`: The number of features to be output by the embedding layer and the depth for the positional encoding.\n",
    "    - `call(self, x)`: This method is used to compute the positionally encoded embeddings of the inputs. It takes in one argument:\n",
    "        - `x`: The input tensor for which the embeddings are to be computed.\n",
    "    - The method first computes the embeddings of the inputs, scales the embeddings by the square root of `d_model`, and then adds the positional encoding to these scaled embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer, VisualWrapper):\n",
    "    def __init__(self, vocab_size, d_model, dropout):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0,1,2])\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    # @log_dec\n",
    "    # @tf.function\n",
    "    def call(self, x, training=None):\n",
    "\n",
    "        length = tf.shape(x)[1]\n",
    "        x_emb = self.embedding(x)\n",
    "        # This factor sets the relative scale of the embedding and positional_encoding\n",
    "        x_emb_scale = x_emb * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        y = self.dropout(x_emb_scale + self.pos_encoding[tf.newaxis, :length, :])\n",
    "\n",
    "        self.visualize_data(x_emb, mode='color_bar', text=f\"This is the embedding of the input to {self.__class__.__name__}.\", training=training)\n",
    "        self.visualize_data(y, mode='color_bar', text=f\"This is the embedding of the input to {self.__class__.__name__} with added positional encoding.\", training=training)\n",
    "        self.visualize_data(x_emb-y, mode='color_bar', text=f\"Here you can see the difference between both.\", training=training)\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. `make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1)`:\n",
    "    - The `make_model` function constructs a Transformer model from given hyperparameters. It takes seven arguments:\n",
    "        - `src_vocab`: The size of the source vocabulary.\n",
    "        - `tgt_vocab`: The size of the target vocabulary.\n",
    "        - `N`(default=6): The number of layers in the Transformer's Encoder and Decoder stacks.\n",
    "        - `d_model`(default=512): The dimension of the model. It's the number of features in input and output.\n",
    "        - `d_ff`(default=2048): The number of features in the hidden layer of the feed-forward network.\n",
    "        - `h`(default=8): The number of attention heads in the MultiHeadedAttention mechanism.\n",
    "        - `dropout`(default=0.1): The dropout rate to be applied in several parts of the model.\n",
    "    - Inside this function, instances of `MultiHeadedAttention` and `PositionwiseFeedForward` are created. These instances are then deep-copied and used to construct the Encoder and Decoder stacks, additionally the PositionalEmbeddings, and the Generator are instantiated. All these parts are then assembled into a `EncoderDecoder` instance, which includes the complete Transformer model. If a module is wrapped with a `LayerWrapper` this is in order to visualize the output of this layer on sucessive calls. Look for the specific meaning of the `LayerWrapper` parameters in the definition of the class.\n",
    "    - Finally, the function returns the constructed model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1) -> tf.keras.Model:\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    \n",
    "    model = EncoderDecoder(\n",
    "                EncoderStack(\n",
    "                    EncoderLayer,\n",
    "                    N=N, \n",
    "                    size=d_model, \n",
    "                    dropout=dropout, \n",
    "                    self_attn=MultiHeadedAttention(h, d_model), \n",
    "                    feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout)),\n",
    "                DecoderStack(\n",
    "                    DecoderLayer, \n",
    "                    N=N, \n",
    "                    size=d_model, \n",
    "                    dropout=dropout,\n",
    "                    self_attn=MultiHeadedAttention(h, d_model), \n",
    "                    src_attn=MultiHeadedAttention(h, d_model), \n",
    "                    feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout)),\n",
    "                PositionalEmbedding(\n",
    "                    src_vocab, \n",
    "                    d_model,\n",
    "                    dropout),\n",
    "                PositionalEmbedding(\n",
    "                    tgt_vocab, \n",
    "                    d_model,\n",
    "                    dropout),\n",
    "                Generator(tgt_vocab)\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryTokenizer(tf.Module, VisualWrapper):\n",
    "    def __init__(self, reserved_tokens, vocab_path):    \n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.tokenizer = tf_text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
    "        self.vocab = tf.Variable(vocab)        \n",
    "\n",
    "    def tokenize(self, strings, training=None):\n",
    "\n",
    "        encoded = self.tokenizer.tokenize(strings)\n",
    "        merged_enc = encoded.merge_dims(-2, -1)\n",
    "        out = self.add_start_end(merged_enc)\n",
    "\n",
    "        self.visualize_data(self.lookup(out),\n",
    "                            mode='print', \n",
    "                            text=f\"This is the data from {self.__class__.__name__}\", \n",
    "                            training=training)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def detokenize(self, tokenized, training=None):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return self.cleanup_text(self._reserved_tokens, words)\n",
    "    \n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_start_end(ragged):\n",
    "        START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "        END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "        count = ragged.bounding_shape()[0]\n",
    "        starts = tf.fill([count, 1], START)\n",
    "        ends = tf.fill([count, 1], END)\n",
    "        return tf.concat([starts, ragged, ends], axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_text(reserved_tokens, token_txt):\n",
    "        bad_tokens = list(filter(lambda token: token != \"[UNK]\", reserved_tokens))\n",
    "        bad_tokens_re = \"|\".join(bad_tokens)\n",
    "\n",
    "        bad_cells = tf.strings.regex_full_match(token_txt, bad_tokens_re)\n",
    "        ragged_result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "        result = tf.strings.reduce_join(ragged_result, separator=' ', axis=-1)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "    \n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "    \n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_text_file):\n",
    "    return tf.data.TextLineDataset(filenames=dataset_text_file)\n",
    "\n",
    "def create_vocab(dataset):\n",
    "    bert_vocab_args=dict(\n",
    "        vocab_size = 8000,\n",
    "        reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"],\n",
    "        bert_tokenizer_params = dict(lower_case=True),\n",
    "        learn_params = {},\n",
    "    )\n",
    "\n",
    "    story_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "        dataset.batch(1000).prefetch(2),\n",
    "        **bert_vocab_args\n",
    "    )\n",
    "    return story_vocab\n",
    "\n",
    "def create_vocab_from_textdata(text_file=corpus_file_path):\n",
    "    dataset = load_dataset(text_file)\n",
    "    vocab = create_vocab(dataset)\n",
    "    return vocab\n",
    "\n",
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w') as file:\n",
    "        for token in vocab:\n",
    "            print(token, file=file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator():\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokenizer, \n",
    "                 buffer_size=20000, \n",
    "                 batch_size=64, \n",
    "                 max_padding=128, \n",
    "                 pad_id=0):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_padding = max_padding\n",
    "        self.pad_id = pad_id\n",
    "        self.dataset = None\n",
    "\n",
    "    def txt_files_to_lines_gen(self, file_path):\n",
    "        for file in os.listdir(file_path):\n",
    "            with open(os.path.join(file_path, file), 'r') as f:\n",
    "                for line in f:\n",
    "                    yield line.strip()\n",
    "\n",
    "    def lines_to_fit_sentences(self, sentences, length):\n",
    "        length = length / 1.5 # estimate of token/word ratio (in real the value is about 1.4)\n",
    "\n",
    "        current_combined_sentence = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()  # Remove leading/trailing whitespace\n",
    "            sentence_words = sentence.split()\n",
    "\n",
    "            # Check if combining the current sentence with the previous one exceeds the word limit\n",
    "            if len(current_combined_sentence.split()) + len(sentence_words) > length:\n",
    "                yield current_combined_sentence\n",
    "                current_combined_sentence = sentence  # Start a new combined sentence\n",
    "            else:\n",
    "                current_combined_sentence += \" \" + sentence  # Concatenate the sentences\n",
    "    \n",
    "    def generate_datasets(self, file_path):\n",
    "        \n",
    "        # Create a Dataset from the text file\n",
    "        lines_gen = self.txt_files_to_lines_gen(file_path)\n",
    "        fit_sentence_gen = self.lines_to_fit_sentences(lines_gen, 512)\n",
    "        \n",
    "        dataset = tf.data.Dataset.from_generator(lambda: fit_sentence_gen, \n",
    "                                                 output_signature=tf.TensorSpec(shape=(), \n",
    "                                                                                dtype=tf.string))\n",
    "\n",
    "        # Tokenize the whole dataset with the pre-trained tokenizer\n",
    "        tokenized_dataset = (dataset\n",
    "                            .shuffle(self.buffer_size)\n",
    "                            .batch(self.batch_size)\n",
    "                            .map(lambda x: self.prepare_datapoint(x))\n",
    "                            .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "        \n",
    "        self.dataset = tokenized_dataset\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    def prepare_datapoint(self, data_point):\n",
    "        \n",
    "        src_tokens = self.tokenizer.tokenize(data_point)\n",
    "        tgt_tokens = src_tokens[:, :-1]\n",
    "        label_tokens = src_tokens[:, 1:]\n",
    "\n",
    "        src = src_tokens.to_tensor(shape=[1, self.max_padding], \n",
    "                                   default_value=self.pad_id)\n",
    "        tgt = tgt_tokens.to_tensor(shape=[1, self.max_padding], \n",
    "                                   default_value=self.pad_id)\n",
    "        label = label_tokens.to_tensor(shape=[1, self.max_padding], \n",
    "                                       default_value=self.pad_id)\n",
    "        \n",
    "        src_mask = (src != self.pad_id)[:, np.newaxis, :]\n",
    "        tgt_mask = self.make_std_mask(tgt)\n",
    "\n",
    "        return (src, tgt, src_mask, tgt_mask), label\n",
    "  \n",
    "    def make_std_mask(self, tgt):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != self.pad_id)[:, np.newaxis, :]\n",
    "        tgt_mask = tf.logical_and(tgt_mask, subsequent_mask(tgt.shape[-1]))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_dec\n",
    "def load_tokenizer(tokenizer_name):\n",
    "    return tf.saved_model.load(tokenizer_name)\n",
    "\n",
    "@log_dec\n",
    "def create_datasets(file_path, tokenizer, buffer_size=20000, batch_size=64, max_padding=128, pad_id=0):\n",
    "    # Create a Dataset from the text file\n",
    "    lines_dataset = tf.data.TextLineDataset(file_path)\n",
    "\n",
    "    # Tokenize the whole dataset with the pre-trained tokenizer\n",
    "    tokenized_dataset = (lines_dataset\n",
    "                         .shuffle(buffer_size)\n",
    "                         .batch(batch_size)\n",
    "                         .map(lambda x: prepare_datapoint(x,tokenizer, max_padding=max_padding, pad_id=0))\n",
    "                         .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "    # Determine the number of lines for training (80% for this example)\n",
    "    num_lines = sum(1 for _ in tokenized_dataset)\n",
    "    num_train = int(0.8 * num_lines)\n",
    "\n",
    "    # Split the data into train and validation datasets\n",
    "    train_dataset = tokenized_dataset.take(num_train)\n",
    "    valid_dataset = tokenized_dataset.skip(num_train)\n",
    "\n",
    "    return train_dataset, valid_dataset\n",
    "\n",
    "@log_dec\n",
    "def prepare_datapoint(data_point, tokenizer, max_padding, pad_id=0):\n",
    "    \n",
    "    src_tokens = tokenizer.tokenize(data_point)\n",
    "    tgt_tokens = src_tokens[:, :-1]\n",
    "    label_tokens = src_tokens[:, 1:]\n",
    "\n",
    "    src = src_tokens.to_tensor(shape=[1, max_padding], default_value=pad_id)\n",
    "    tgt = tgt_tokens.to_tensor(shape=[1, max_padding], default_value=pad_id)\n",
    "    label = label_tokens.to_tensor(shape=[1, max_padding], default_value=pad_id)\n",
    "    \n",
    "    src_mask = (src != pad_id)[:, np.newaxis, :]\n",
    "    tgt_mask = make_std_mask(tgt, pad_id)\n",
    "\n",
    "    return (src, tgt, src_mask, tgt_mask), label\n",
    "\n",
    "@log_dec\n",
    "def make_std_mask(tgt, pad_id):\n",
    "    \"Create a mask to hide padding and future words.\"\n",
    "    tgt_mask = (tgt != pad_id)[:, np.newaxis, :]\n",
    "    tgt_mask = tf.logical_and(tgt_mask, subsequent_mask(tgt.shape[-1]))\n",
    "    return tgt_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(layers.Layer, VisualWrapper):\n",
    "    \"\"\"\n",
    "    This class represents a loss function layer that applies label smoothing to prevent overconfidence \n",
    "    in the model's predictions. This is done by replacing the 0s and 1s in the labels with smoothed values, \n",
    "    such that the model learns to be less confident and thus, more robust.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary, which also represents the number of classes.\n",
    "        padding_idx (int): The index representing padding elements.\n",
    "        smoothing (float): The smoothing factor to be applied. The values should be between 0 and 1. \n",
    "                           Default value is 0.\n",
    "        reduction (tf.keras.losses.Reduction): The type of reduction to apply to the output loss. \n",
    "                                               Default is tf.keras.losses.Reduction.SUM.\n",
    "\n",
    "    Methods:\n",
    "        call(x, target): Calculates and returns the loss given the model's output `x` and the target labels.\n",
    "\n",
    "    Example:\n",
    "        >>> loss_func = LabelSmoothingLoss(vocab_size=5000, padding_idx=0, smoothing=0.1)\n",
    "        >>> x = tf.random.uniform((10, 5000))  # model's output\n",
    "        >>> target = tf.random.uniform((10, 1), maxval=5000, dtype=tf.int32)  # target labels\n",
    "        >>> loss = loss_func(x, target)  # calculate loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, padding_idx, smoothing=0.0):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.vocab_size = vocab_size\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        # self.loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.loss_func = tf.keras.losses.KLDivergence(reduction='none')\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, x, target):\n",
    "        # create padding mask\n",
    "        mask = self.padding_mask(target, self.padding_idx)\n",
    "\n",
    "        # tf.print('loss mask:', mask)\n",
    "\n",
    "        # Apply label confidence\n",
    "        true_dist = target * self.confidence\n",
    "\n",
    "        # Apply label smoothing\n",
    "        smoothing_value = self.smoothing / tf.cast(self.vocab_size - 2, tf.float32)\n",
    "        true_dist = tf.where(tf.equal(true_dist, 0), smoothing_value, true_dist)\n",
    "\n",
    "        # tf.print('prediction before loss:', x)\n",
    "        # tf.print('one hot smoothed', true_dist)\n",
    "        # tf.print('smoothed one hot high values at:', tf.where(true_dist > self.smoothing))\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = self.kl_div_loss(x, true_dist)\n",
    "\n",
    "        # tf.print('loss tensor:', loss)\n",
    "\n",
    "        loss = tf.cast(self.apply_mask(loss, mask), x.dtype)\n",
    "\n",
    "        # tf.print('loss after masking:', loss)\n",
    "        # tf.print('loss big at:', tf.where(loss > 0.1))\n",
    "        # tf.print('reduced sum loss:', tf.reduce_sum(loss))\n",
    "\n",
    "        loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def padding_mask(t, padding_idx):\n",
    "        return tf.cast(tf.equal(t[:, :, padding_idx], 0), tf.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_mask(t, mask):\n",
    "        return t * (tf.reshape(mask, [-1, 1]) * tf.ones_like(t))\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_div_loss(input, target):\n",
    "        return target * (tf.math.log(target)-input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCompute(tf.keras.losses.Loss, VisualWrapper):\n",
    "    '''TODO: Correct Loss Computation'''\n",
    "    def __init__(self, generator, loss_function, vocab_size, name='loss_compute'):\n",
    "        super().__init__(name=name)\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "        self.generator = generator\n",
    "        self.loss_function = loss_function\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    # @log_dec\n",
    "    def call(self, y_true, y_pred): \n",
    "        # tf.print('initial y_pred', y_pred)\n",
    "        # tf.print('initial y_true', y_true)\n",
    "        y_pred = self.generator(y_pred)\n",
    "        y_true_one_hot = tf.cast(tf.one_hot(y_true, depth=self.vocab_size), tf.float32)\n",
    "        # Compute loss\n",
    "        # tf.print('y_true_one_hot', tf.where(y_true_one_hot != 0))\n",
    "        # tf.print(tf.argmax(y_true_one_hot, axis=-1))\n",
    "        loss = self.loss_function(y_pred, y_true_one_hot)\n",
    "        # Calculate mean loss per batch\n",
    "        norm = tf.cast(tf.shape(y_true)[0], dtype=tf.float32)\n",
    "        \n",
    "        sloss = loss / norm\n",
    "\n",
    "        # Return scaled loss (mean loss per batch) and total loss (for the whole batch)\n",
    "        return loss\n",
    "        # return sloss * norm, sloss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSchedule(tf.keras.optimizers.schedules.LearningRateSchedule, VisualWrapper):\n",
    "    def __init__(self, d_model=512, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        VisualWrapper.__init__(self, vis_on_count=[0])\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    # @log_dec\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg_1 = tf.math.rsqrt(step)\n",
    "        arg_2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg_1, arg_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @log_dec\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "\n",
    "  match = label == pred\n",
    "  mask = label != 0\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(tokenizer, training_data, validation_data, config):\n",
    "\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    n_epochs = config[\"n_epochs\"]\n",
    "    base_lr = config[\"base_lr\"]\n",
    "    max_padding = config[\"max_padding\"]\n",
    "    padding_idx = config[\"padding_idx\"]\n",
    "    warmup_steps = config[\"warmup_steps\"]\n",
    "    d_model = config[\"d_model\"]\n",
    "    N = config[\"N\"]\n",
    "    h = config[\"h\"]\n",
    "    fit_verbose = config[\"fit_verbose\"]\n",
    "    load_latest = config[\"load_latest\"]\n",
    "    save_model = config[\"save_model\"]\n",
    "\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "    model = make_model(vocab_size, vocab_size, d_model=d_model, N=N, h=h)\n",
    "\n",
    "    # out = model.decode(model.encode(first_batch.src, first_batch.src_mask), \n",
    "    #                       first_batch.src_mask,\n",
    "    #                       first_batch.tgt, \n",
    "    #                       first_batch.tgt_mask)\n",
    "\n",
    "    # out = model(first_batch.src, first_batch.tgt, first_batch.src_mask, first_batch.tgt_mask)\n",
    "\n",
    "    model.compile(\n",
    "        loss = LossCompute(model.generator, \n",
    "                           LabelSmoothingLoss(vocab_size, padding_idx=padding_idx, smoothing=0.1), \n",
    "                           vocab_size=vocab_size), \n",
    "        optimizer = tf.keras.optimizers.Adam(TransformerSchedule(d_model=d_model, \n",
    "                                                                 warmup_steps=warmup_steps), # type: ignore\n",
    "                                                                 beta_1=0.9, \n",
    "                                                                 beta_2=0.98, \n",
    "                                                                 epsilon=1e-9), \n",
    "        metrics = [masked_accuracy],\n",
    "    )\n",
    "\n",
    "    model = load_latest_weights(model, d_model, load_latest=load_latest)\n",
    "\n",
    "    if save_model:\n",
    "\n",
    "        current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        directory = f\"model_N{N}_h{h}_d{d_model}_t{current_time}\"\n",
    "        ckp_name = \"model_{epoch:03d}.h5\"\n",
    "        final_name = \"final_model.h5\"\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        checkpoint_path = os.path.join(directory, ckp_name)\n",
    "        final_path = os.path.join(directory, final_name)\n",
    "        \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_path, \n",
    "            save_freq='epoch', \n",
    "            save_weights_only=True, \n",
    "            verbose=1)\n",
    "    else:\n",
    "        checkpoint = []\n",
    "\n",
    "    if n_epochs > 0:\n",
    "        # TODO: Return to fullsized dataset\n",
    "        model.fit(training_data,\n",
    "            epochs = n_epochs,\n",
    "            batch_size = batch_size,\n",
    "            validation_data = validation_data,\n",
    "            callbacks = [checkpoint],\n",
    "            verbose = fit_verbose)\n",
    "    \n",
    "    if save_model:\n",
    "        model.save_weights(final_path, overwrite=True)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    VisualWrapper.reset_counter()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_latest_weights(model, d_model, load_latest=False, model_folder=None):\n",
    "    # TODO: Ensure architecture sizes match.\n",
    "    if model_folder is not None:\n",
    "        # Load weights from the specified model folder\n",
    "        directories = [model_folder]\n",
    "    elif load_latest:\n",
    "        # Get all the directories and sort them in descending order\n",
    "        directories = sorted(glob.glob('model_N*_h*'), key=os.path.getmtime, reverse=True)\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "    # Load weights from the latest trained model\n",
    "    latest_weights = None\n",
    "    if directories:\n",
    "        latest_dir = directories[0]\n",
    "        # Get all the h5 files inside the directory and sort them\n",
    "        h5_files = sorted(glob.glob(os.path.join(latest_dir, '*.h5')))\n",
    "\n",
    "        if h5_files:\n",
    "            # Pick the last epoch file (or final_model file if it exists)\n",
    "            latest_epoch_file = h5_files[-1] if 'final_model.h5' not in h5_files[-1] else h5_files[-2]\n",
    "            latest_weights = os.path.join(latest_epoch_file)\n",
    "\n",
    "    # Load weights if we found a previously trained model\n",
    "    if latest_weights is not None:\n",
    "        print(f'Loading weights from {latest_weights}')\n",
    "        \n",
    "        # Create a dummy input matching the input shape of the model\n",
    "        dummy_input = tf.random.uniform(shape=[1,d_model]), tf.random.uniform(shape=[1,d_model]), None, None\n",
    "        # Call the model on the dummy input\n",
    "        _ = model.generator(model(dummy_input))\n",
    "\n",
    "        model.load_weights(latest_weights)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"batch_size\": 64,\n",
    "        \"n_epochs\": 1,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 512,\n",
    "        \"padding_idx\": 0,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"N\": 6,\n",
    "        \"d_model\": 32,\n",
    "        \"h\": 8,\n",
    "        \"fit_verbose\": 1,\n",
    "        \"load_latest\": False,\n",
    "        \"save_model\": True,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_data_generator = DatasetGenerator(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                                 batch_size=config[\"batch_size\"], \n",
    "                                 max_padding=config[\"max_padding\"], \n",
    "                                 pad_id=config[\"padding_idx\"])\n",
    "\n",
    "corpus_dataset = corpus_data_generator.generate_datasets(corpus_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_data_generator = DatasetGenerator(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                                 batch_size=config[\"batch_size\"], \n",
    "                                 max_padding=config[\"max_padding\"], \n",
    "                                 pad_id=config[\"padding_idx\"])\n",
    "\n",
    "tight_dataset = corpus_data_generator.generate_datasets(tight_fit_512_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bco_generator = DatasetGenerator(StoryTokenizer(reserved_tokens, vocab_path),\n",
    "                                 batch_size=config[\"batch_size\"], \n",
    "                                 max_padding=config[\"max_padding\"], \n",
    "                                 pad_id=config[\"padding_idx\"])\n",
    "\n",
    "bco_dataset = bco_generator.generate_datasets(bco_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bco_dataset.save(\"datasets\\\\bookscorpusopen\\\\tfds_shards\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    335/Unknown - 62s 125ms/step - loss: 5.7661 - masked_accuracy: 8.5909e-04"
     ]
    }
   ],
   "source": [
    "model = run_model(StoryTokenizer(reserved_tokens, vocab_path), \n",
    "                  bco_dataset.take(30000), \n",
    "                  corpus_dataset,\n",
    "                  config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordComplete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordComplete(tf.Module, VisualWrapper):\n",
    "  def __init__(self, tokenizer, transformer, max_length=512, dtype=tf.Tensor, decode_result=True):\n",
    "    super().__init__()\n",
    "    VisualWrapper.__init__(self, vis_on_count=None)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.transformer = transformer\n",
    "    self.max_length = max_length\n",
    "    self.dtype = dtype\n",
    "    self.decode_result = decode_result\n",
    "\n",
    "  def __call__(self, input, decode=True, encoding='utf-8'):\n",
    "    \n",
    "    # TODO: Bug with empty strings as input\n",
    "    tensor_input = tf.convert_to_tensor(input)\n",
    "\n",
    "    if len(tensor_input.shape) == 0:\n",
    "      tensor_input = tensor_input[tf.newaxis]\n",
    "\n",
    "\n",
    "    tokenized_input = self.tokenizer.tokenize(tensor_input, training=False).to_tensor()\n",
    "\n",
    "    enc_input = tokenized_input\n",
    "    context = self.transformer.encode(enc_input, None, training=False)\n",
    "\n",
    "    end = enc_input[-1][-1]\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "\n",
    "    for i, value in enumerate(tokenized_input[0][:-1]):\n",
    "      output_array = output_array.write(i, value)\n",
    "    \n",
    "    out_init_len = output_array.size()\n",
    "\n",
    "    # tf.print('real tokens: ', enc_input[0][:-1])\n",
    "    # tf.print('shape of real tokens: ', tf.shape(enc_input[0][:-1]))\n",
    "\n",
    "    for i in tf.range(out_init_len, self.max_length):\n",
    "      dec_input = output_array.concat()[tf.newaxis]\n",
    "\n",
    "      # tf.print('dec_in shape: ', tf.shape(dec_input))\n",
    "      # tf.print(\"dec_in :\", dec_input)\n",
    "\n",
    "      decode = self.transformer.decode(context, None, dec_input, None, training=False)\n",
    "\n",
    "\n",
    "      predictions = self.transformer.generator(decode, training=False)\n",
    "\n",
    "      # tf.print('Vorhersage des Modells', tf.argmax(predictions, axis=-1))\n",
    "      # tf.print('Shape of Prediction: ', tf.shape(predictions))\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "\n",
    "      output_array = output_array.write(i, predicted_id[0][0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = output_array.concat()[tf.newaxis]\n",
    "\n",
    "    # The output shape is `(1, tokens)`.\n",
    "    text = self.tokenizer.detokenize(output)  # Shape: `()`.\n",
    "\n",
    "    tokens = self.tokenizer.lookup(output)\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    # self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    # attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    if self.decode_result:\n",
    "      text = text.numpy()[0].decode(encoding)\n",
    "\n",
    "    VisualWrapper.reset_counter()\n",
    "\n",
    "    return text, tokens # , attention_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] can you give me to the same , i ' s not a few days . i ' s not a few days . i ' s not a few days\n"
     ]
    }
   ],
   "source": [
    "inference_model = WordComplete(StoryTokenizer(reserved_tokens, vocab_path), model, max_length=32)\n",
    "\n",
    "string = \"Can you give me\"\n",
    "\n",
    "text, tokens = inference_model(string)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_list = []\n",
    "for instance in VisualWrapper.instances:\n",
    "    if instance.counter != 0:\n",
    "        counter_list.append(instance.counter)\n",
    "print(counter_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_simu_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
