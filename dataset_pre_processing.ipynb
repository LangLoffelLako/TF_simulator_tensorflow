{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Pre-Processing Notebook\n",
    "The purpose of this notebook is to preprocess each file of the datasets we collected.\n",
    "We want all the dataset as a single csv-file with stories as entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "from time import time\n",
    "import logging as log\n",
    "import functools\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=log.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "log_enabled = True\n",
    "show_notebook_results = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dec(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            if log_enabled:\n",
    "                start_time = time()\n",
    "                log.info('{} started'.format(func.__name__))\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "        finally:\n",
    "            if log_enabled:\n",
    "                duration = time() - start_time\n",
    "                log.info('{} finished'.format(func.__name__))\n",
    "    return wrapper\n",
    "\n",
    "def run_notebook(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if show_notebook_results:\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            return\n",
    "    return wrapper\n",
    "\n",
    "def save_and_load_to_path(path):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            save_data = True\n",
    "            try:\n",
    "                try:\n",
    "                    with open(path, 'r') as file:\n",
    "                        save_data = False\n",
    "                        return json.load(file)\n",
    "                except:\n",
    "                    return func(*args, **kwargs)\n",
    "            except Exception as ex:\n",
    "                raise ex\n",
    "            finally:\n",
    "                if save_data:\n",
    "                    with open(path, 'w') as file:\n",
    "                        json.dump(func(*args, **kwargs), file)\n",
    "            return wrapper\n",
    "        return decorator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@log_dec\n",
    "def read_in_text(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "@log_dec\n",
    "def read_in_csv(file_path, data_range=None):\n",
    "    _line_delim = '\\n'\n",
    "    _clm_delim = ', '\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = csv.reader(file, delimiter=',')\n",
    "        next(data) # deletes the header row of the csv-file\n",
    "        if not data_range is None:\n",
    "            merged_data = _line_delim.join(_clm_delim.join(row[i] for i in data_range) for row in data)\n",
    "        else:\n",
    "            merged_data = _line_delim.join(_clm_delim.join(row) for row in data)\n",
    "    return merged_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Children stories text corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This corpus has each sentence in a new row.\n",
    "* There is no \\<eos\\>. Stories are mostly separated by \"\\n\\n Title \\n\\n\".\n",
    "* We will use \"\\n\\n\" as an indicator for a new story.\n",
    "* We will use \"\\n\" as indicator for \\<eos\\>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_notebook\n",
    "@log_dec\n",
    "def tokenize_children_stories():\n",
    "    data = read_in_text('datasets_raw_data\\\\children_stories_text_corpus\\\\cleaned_merged_fairy_tales_without_eos.txt')\n",
    "    data_split = list(map(lambda text: sent_tokenize(text), data.split(sep='\\n\\n')))\n",
    "    return data_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poe Short Stories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This corpus is composed of a csv-file. The structure is: title, text, wikipedia_title, publication_date, first_published_in, classification, notes, normalized_date.\n",
    "* We can thus easily export the text and apply story separation.\n",
    "* We need to use \". \" as \\<eos\\>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_notebook\n",
    "@log_dec\n",
    "def tokenize_poe_short_stories():\n",
    "    data = read_in_csv('datasets_raw_data\\\\poe_short_stories\\\\preprocessed_data.csv', data_range=[1])\n",
    "    data_split = list(map(lambda text: sent_tokenize(text), data.split(sep='\\n')))\n",
    "    return data_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Short Stories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each line of [reddit_short_stories.txt](https://github.com/tdude92/reddit-short-stories/blob/main/reddit_short_stories.txt) is one full short story.\n",
    "* Each short story begins with an \"\\<sos>\" token and ends with an \"\\<eos>\" token (eg. \"\\<sos> once upon a time, the end \\<eos>\").\n",
    "* Newline characters in a story are replaced with the \"\\<nl>\" token (eg. \"\\<sos> line 1 \\<nl> line 2 \\<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_notebook\n",
    "@log_dec\n",
    "def tokenize_reddit_short_stories():\n",
    "    data = read_in_text('datasets_raw_data\\\\popular_reddit_short_stories\\\\reddit_short_stories.txt')\n",
    "    data_strip = re.sub(r'\\<sos\\>|\\<eos\\>|\\<nl\\>', '', data)\n",
    "    data_split = list(map(lambda text: sent_tokenize(text), data_strip.split(sep='\\n\\n')))\n",
    "    return data_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scifi Stories Text Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really big corpus, don't use for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sherlock Holmes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each story is stored in a separate file.\n",
    "* At the end of each story is this block:\n",
    "    *      ----------\n",
    "        This text is provided to you \"as-is\" without any warranty. No\n",
    "        warranties of any kind, expressed or implied, are made to you as to\n",
    "        the text or any medium it may be on, including but not limited to\n",
    "        warranties of merchantablity or fitness for a particular purpose.\n",
    "\n",
    "        This text was formatted from various free ASCII and HTML variants.\n",
    "        See http://sherlock-holm.es for an electronic form of this text and\n",
    "        additional information about it.\n",
    "\n",
    "        This text comes from the collection's version 3.1.\n",
    "* Sentences are not well separated into lines. No obvious solution at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_dec\n",
    "def tokenize_single_file_sherlock_holmes(file):\n",
    "    data =              read_in_text(file)\n",
    "    data_strip =        re.sub(r'\\n|\\s{2,}', ' ', data)\n",
    "    data_strip_end =    re.split(r'---', data_strip)[0]\n",
    "    data_split =        sent_tokenize(data_strip_end)\n",
    "    return data_split\n",
    "\n",
    "@run_notebook\n",
    "@log_dec\n",
    "def tokenize_sherlock_holmes():\n",
    "    data = [tokenize_single_file_sherlock_holmes('datasets_raw_data\\\\sherlock_holmes' + '\\\\' + file)\n",
    "            for file in os.listdir('datasets_raw_data\\\\sherlock_holmes')]\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Works of Charles Dickens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This dataset is really messy.\n",
    "* Leave for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to combine our different datasets such that we have one large corpus of appropriate sized batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_dec\n",
    "def tokenize_all():\n",
    "    story_delim = '\\n\\n'\n",
    "    sentence_delim = '\\n'\n",
    "    func_list = [\n",
    "        tokenize_children_stories(),\n",
    "        tokenize_poe_short_stories(),\n",
    "        tokenize_reddit_short_stories(),\n",
    "        tokenize_sherlock_holmes(),\n",
    "    ]\n",
    "    corpus = [story for collection in func_list for story in collection]\n",
    "    with open('datasets\\\\corpus.txt', 'w') as file:\n",
    "        #json.dump(corpus, file)\n",
    "        text_data = story_delim.join(list(map(lambda text_list: sentence_delim.join(text_list),corpus)))\n",
    "        file.write(text_data)\n",
    "    return text_data\n",
    "\n",
    "tokenize_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_simu_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
