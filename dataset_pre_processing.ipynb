{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Pre-Processing Notebook\n",
    "The purpose of this notebook is to preprocess each file of the datasets we collected.\n",
    "We want all the dataset as a single csv-file with stories as entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "from time import time\n",
    "import logging as log\n",
    "import functools\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=log.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "log_enabled = True\n",
    "show_notebook_results = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decorators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `log_dec(func)`:\n",
    "\n",
    "    This is a decorator function that logs the start and end time of the function it decorates if logging is enabled. \n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    `func`: The function to be logged.\n",
    "\n",
    "    How it works: \n",
    "\n",
    "    - The `wrapper` function is defined to wrap around the `func`.\n",
    "    - If `log_enabled` is `True`, the start time of the function is logged.\n",
    "    - The `func` is then executed with its arguments (`*args` and `**kwargs`).\n",
    "    - If there's any exception, it's raised; otherwise, the function returns the output of `func`.\n",
    "    - Finally, if `log_enabled` is `True`, the duration of the function execution is calculated and logged.\n",
    "\n",
    "2. `run_notebook(func)`:\n",
    "\n",
    "    This is a decorator function that controls the display of the function it decorates based on the `show_notebook_results` flag.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    `func`: The function whose results are to be controlled.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - The `wrapper` function is defined to wrap around `func`.\n",
    "    - If `show_notebook_results` is `True`, the function is executed and its result is returned.\n",
    "    - If `show_notebook_results` is `False`, the function does not execute and no result is returned.\n",
    "\n",
    "3. `save_and_load_to_path(path)`:\n",
    "\n",
    "    This is a decorator factory that generates a decorator for saving the output of a function to a JSON file and loading it back the next time the function is called. \n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    `path`: The path where the JSON file is saved.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - A decorator `decorator(func)` is defined that takes a function `func` to be decorated.\n",
    "    - Inside this decorator, a `wrapper` function is defined to wrap around `func`.\n",
    "    - Initially, `save_data` is set to `True`.\n",
    "    - It then tries to open and load the JSON file at the given `path`. If it succeeds, it sets `save_data` to `False` and returns the loaded data.\n",
    "    - If it fails to load the data, it runs `func` and returns its output.\n",
    "    - If there's any exception, it's raised.\n",
    "    - Finally, if `save_data` is still `True` (meaning the function was run and its output wasn't already saved), it saves the output of `func` to the JSON file at `path`.\n",
    "    \n",
    "    Note: These decorators are higher-order function that use `functools.wraps()` to preserve the metadata of `func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dec(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            if log_enabled:\n",
    "                start_time = time()\n",
    "                log.info('{} started'.format(func.__name__))\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "        finally:\n",
    "            if log_enabled:\n",
    "                duration = time() - start_time\n",
    "                log.info('{} finished'.format(func.__name__))\n",
    "    return wrapper\n",
    "\n",
    "def run_notebook(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        if show_notebook_results:\n",
    "            return func(*args, **kwargs)\n",
    "        else:\n",
    "            return\n",
    "    return wrapper\n",
    "\n",
    "def save_and_load_to_path(path):\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            save_data = True\n",
    "            try:\n",
    "                try:\n",
    "                    with open(path, 'r') as file:\n",
    "                        save_data = False\n",
    "                        return json.load(file)\n",
    "                except:\n",
    "                    return func(*args, **kwargs)\n",
    "            except Exception as ex:\n",
    "                raise ex\n",
    "            finally:\n",
    "                if save_data:\n",
    "                    with open(path, 'w') as file:\n",
    "                        json.dump(func(*args, **kwargs), file)\n",
    "            return wrapper\n",
    "        return decorator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `read_in_text(file_path)`:\n",
    "\n",
    "    This function reads in a text file and returns its contents as a string. \n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    `file_path`: A string specifying the path to the text file.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - The function opens the file at `file_path` in read mode.\n",
    "    - It then reads the entire content of the file into a string.\n",
    "    - Finally, it returns this string.\n",
    "\n",
    "    Note: This function is decorated with `@log_dec`, which will log the start and end time of its execution if logging is enabled.\n",
    "\n",
    "2. `read_in_csv(file_path, data_range=None)`:\n",
    "\n",
    "    This function reads in a CSV file and returns its content as a string.\n",
    "    \n",
    "    Parameters:\n",
    "\n",
    "    `file_path`: A string specifying the path to the CSV file.\n",
    "    \n",
    "    `data_range` (optional): A range of column indices. If provided, only these columns will be included in the output string. Default is `None`, in which case all columns are included.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - The function opens the file at `file_path` in read mode.\n",
    "    - It then reads the content of the file into a CSV reader object, with a comma as the delimiter.\n",
    "    - The header row of the CSV file is skipped.\n",
    "    - If `data_range` is not `None`, it creates a string `merged_data` that contains only the columns specified by `data_range` from each row of the CSV file, with each column separated by a comma and each row separated by a newline.\n",
    "    - If `data_range` is `None`, it creates a string `merged_data` that contains all columns from each row of the CSV file, with each column separated by a comma and each row separated by a newline.\n",
    "    - Finally, it returns `merged_data`.\n",
    "\n",
    "    Note: This function is decorated with `@log_dec`, which will log the start and end time of its execution if logging is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@log_dec\n",
    "def read_in_text(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "@log_dec\n",
    "def read_in_csv(file_path, data_range=None):\n",
    "    _line_delim = '\\n'\n",
    "    _clm_delim = ', '\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = csv.reader(file, delimiter=',')\n",
    "        next(data) # deletes the header row of the csv-file\n",
    "        if not data_range is None:\n",
    "            merged_data = _line_delim.join(_clm_delim.join(row[i] for i in data_range) for row in data)\n",
    "        else:\n",
    "            merged_data = _line_delim.join(_clm_delim.join(row) for row in data)\n",
    "    return merged_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `tokenize_children_stories()`:\n",
    "\n",
    "    This function reads in a text file containing children's stories, splits the text into paragraphs, and then tokenizes each paragraph into sentences. It returns a list of lists, where each inner list contains the sentences of a paragraph.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - The function reads in a text file using the `read_in_text()` function. The file path is 'datasets_raw_data\\\\children_stories_text_corpus\\\\cleaned_merged_fairy_tales_without_eos.txt'.\n",
    "    - It then splits the read text into paragraphs at double newline ('\\n\\n') characters.\n",
    "    - Next, it tokenizes each paragraph into sentences using the `sent_tokenize()` function from the NLTK library. This results in a list of lists, where each inner list contains the sentences of a paragraph.\n",
    "    - Finally, it returns this list of lists.\n",
    "\n",
    "2. `tokenize_poe_short_stories()`:\n",
    "\n",
    "    This function reads in a CSV file containing short stories by Edgar Allan Poe, splits the text into paragraphs, and then tokenizes each paragraph into sentences. It returns a list of lists, where each inner list contains the sentences of a paragraph.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - The function reads in a CSV file using the `read_in_csv()` function. The file path is 'datasets_raw_data\\\\poe_short_stories\\\\preprocessed_data.csv', and the `data_range` parameter is set to `[1]`, meaning that only the second column of the CSV file will be read in.\n",
    "    - It then splits the read text into paragraphs at newline ('\\n') characters.\n",
    "    - Next, it tokenizes each paragraph into sentences using the `sent_tokenize()` function from the NLTK library. This results in a list of lists, where each inner list contains the sentences of a paragraph.\n",
    "    - Finally, it returns this list of lists.\n",
    "\n",
    "    Here's the documentation for these functions:\n",
    "\n",
    "3. `tokenize_reddit_short_stories()`:\n",
    "\n",
    "    This function reads a text file containing popular Reddit short stories, cleans the data by removing specific tags (`<sos>`, `<eos>`, `<nl>`), and then tokenizes the data into sentences.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It reads the text file using `read_in_text()` function from the specified path.\n",
    "    - It removes the specific tags from the data using `re.sub()`.\n",
    "    - It splits the cleaned data into paragraphs at double newline (`\\n\\n`) characters.\n",
    "    - It tokenizes each paragraph into sentences using `sent_tokenize()` function, resulting in a list of lists, where each inner list contains the sentences of a paragraph.\n",
    "    - Finally, it returns this list of lists.\n",
    "\n",
    "    Note: This function is decorated with `@run_notebook` and `@log_dec`, implying that its execution can be toggled and its execution time will be logged if logging is enabled.\n",
    "\n",
    "4. `tokenize_single_file_sherlock_holmes(file)`:\n",
    "\n",
    "    This function reads a single text file of Sherlock Holmes stories, cleans the data by removing newline characters and extra white spaces, then tokenizes the data into sentences.\n",
    "\n",
    "    Parameters:\n",
    "    \n",
    "    `file`: The path of the text file to be read and tokenized.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It reads the text file using `read_in_text()` function from the specified path.\n",
    "    - It removes newline characters and extra white spaces from the data using `re.sub()`.\n",
    "    - It splits the cleaned data at `---` characters and takes the first part.\n",
    "    - It tokenizes the cleaned data into sentences using `sent_tokenize()` function.\n",
    "    - Finally, it returns the list of tokenized sentences.\n",
    "\n",
    "    Note: This function is decorated with `@log_dec`, meaning that its execution time will be logged if logging is enabled.\n",
    "\n",
    "5. `tokenize_sherlock_holmes()`:\n",
    "\n",
    "    This function reads and tokenizes all the text files in the Sherlock Holmes dataset directory.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It iterates over all the files in the 'datasets_raw_data\\\\sherlock_holmes' directory.\n",
    "    - For each file, it applies the `tokenize_single_file_sherlock_holmes()` function, which returns a list of tokenized sentences.\n",
    "    - Finally, it returns a list of these lists.\n",
    "\n",
    "    Note: This function is decorated with `@run_notebook` and `@log_dec`, meaning that its execution can be toggled and its execution time will be logged if logging is enabled.\n",
    "\n",
    "6. `tokenize_all()`:\n",
    "\n",
    "    This function tokenizes all the stories from different sources (children stories, Poe short stories, Reddit short stories, Sherlock Holmes stories), and writes the tokenized stories into a text file.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "    - It calls the functions `tokenize_children_stories()`, `tokenize_poe_short_stories()`, `tokenize_reddit_short_stories()`, and `tokenize_sherlock_holmes()`.\n",
    "    - It concatenates all the tokenized stories into one list.\n",
    "    - It writes the concatenated list into a text file at 'datasets\\\\corpus.txt'.\n",
    "    - Finally, it returns the text data that was written into the file.\n",
    "\n",
    "    Note: This function is decorated with `@log_dec`, meaning that its execution time will be logged if logging is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_notebook\n",
    "@log_dec\n",
    "def tokenize_children_stories():\n",
    "    data = read_in_text('datasets_raw_data\\\\children_stories_text_corpus\\\\cleaned_merged_fairy_tales_without_eos.txt')\n",
    "    data_split = list(map(lambda text: sent_tokenize(text), data.split(sep='\\n\\n')))\n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_notebook\n",
    "@log_dec\n",
    "def tokenize_poe_short_stories():\n",
    "    data = read_in_csv('datasets_raw_data\\\\poe_short_stories\\\\preprocessed_data.csv', data_range=[1])\n",
    "    data_split = list(map(lambda text: sent_tokenize(text), data.split(sep='\\n')))\n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_notebook\n",
    "@log_dec\n",
    "def tokenize_reddit_short_stories():\n",
    "    data = read_in_text('datasets_raw_data\\\\popular_reddit_short_stories\\\\reddit_short_stories.txt')\n",
    "    data_strip = re.sub(r'\\<sos\\>|\\<eos\\>|\\<nl\\>', '', data)\n",
    "    data_split = list(map(lambda text: sent_tokenize(text), data_strip.split(sep='\\n\\n')))\n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_dec\n",
    "def tokenize_single_file_sherlock_holmes(file):\n",
    "    data =              read_in_text(file)\n",
    "    data_strip =        re.sub(r'\\n|\\s{2,}', ' ', data)\n",
    "    data_strip_end =    re.split(r'---', data_strip)[0]\n",
    "    data_split =        sent_tokenize(data_strip_end)\n",
    "    return data_split\n",
    "\n",
    "@run_notebook\n",
    "@log_dec\n",
    "def tokenize_sherlock_holmes():\n",
    "    data = [tokenize_single_file_sherlock_holmes('datasets_raw_data\\\\sherlock_holmes' + '\\\\' + file)\n",
    "            for file in os.listdir('datasets_raw_data\\\\sherlock_holmes')]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_dec\n",
    "def tokenize_all():\n",
    "    story_delim = '\\n\\n'\n",
    "    sentence_delim = '\\n'\n",
    "    func_list = [\n",
    "        tokenize_children_stories(),\n",
    "        tokenize_poe_short_stories(),\n",
    "        tokenize_reddit_short_stories(),\n",
    "        tokenize_sherlock_holmes(),\n",
    "    ]\n",
    "    corpus = [story for collection in func_list for story in collection]\n",
    "    with open('datasets\\\\corpus.txt', 'w') as file:\n",
    "        #json.dump(corpus, file)\n",
    "        text_data = story_delim.join(list(map(lambda text_list: sentence_delim.join(text_list),corpus)))\n",
    "        file.write(text_data)\n",
    "    return text_data\n",
    "\n",
    "tokenize_all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scifi Stories Text Corpus\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really big corpus, don't use for now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Works of Charles Dickens\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This dataset is really messy.\n",
    "* Leave for later."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bookcorpusopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_line(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        sentences = file.readlines()\n",
    "    return sentences\n",
    "\n",
    "def fit_text_data_to_length(sentences, length):\n",
    "\n",
    "    combined_sentences = []\n",
    "    current_combined_sentence = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()  # Remove leading/trailing whitespace\n",
    "        sentence_words = sentence.split()\n",
    "\n",
    "        # Check if combining the current sentence with the previous one exceeds the word limit\n",
    "        if len(current_combined_sentence.split()) + len(sentence_words) > length:\n",
    "            combined_sentences.append(current_combined_sentence)  # Save the previous combined sentence\n",
    "            current_combined_sentence = sentence  # Start a new combined sentence\n",
    "        else:\n",
    "            current_combined_sentence += \" \" + sentence  # Concatenate the sentences\n",
    "\n",
    "    # Add the last combined sentence\n",
    "    if current_combined_sentence:\n",
    "        combined_sentences.append(current_combined_sentence)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(combined_sentences)\n",
    "\n",
    "    return dataset, combined_sentences\n",
    "\n",
    "def save_combined_sentences(file_path, combined_sentences):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for sentence in combined_sentences:\n",
    "            file.write(sentence + '\\n')\n",
    "\n",
    "_, sentences = fit_text_data_to_length(read_txt_line('datasets\\\\corpus.txt'), 512)\n",
    "\n",
    "save_combined_sentences('datasets\\\\tight_fit_dataset_512.txt', sentences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_simu_tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
